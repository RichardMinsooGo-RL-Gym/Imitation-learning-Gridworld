{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8j13ZWe8ogd"
      },
      "source": [
        "# Relative Entropy Inverse Reinforcement Learning (RelEnt IRL)\n",
        "\n",
        "Relative Entropy Inverse Reinforcement Learning is a primitive model-free IRL method - in which you don't need to know the system dynamics -, because you can run this method, **even when the transition probability is unknown**. (Usually, to know the transition probability is not the case in many robotic applications.)\n",
        "\n",
        "Relative Entropy IRL method is inspired by Relative Entropy Policy Search (REPS).<br>\n",
        "Assuming that the expert is optimal, this method learns a reward function. And you can then use it to recover the expert's generalized policy.\n",
        "\n",
        "As you can see below, this method requires not only expert demonstrations, but also requires non-expert demonstrations by an arbitrary policy, for performing importance sampling.\n",
        "\n",
        "*(back to [index](https://github.com/tsmatz/imitation-learning-tutorials/))*"
      ],
      "id": "I8j13ZWe8ogd"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(np.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSz1KN7A0kyi",
        "outputId": "894ba0cc-0229-48cd-c183-4bedb03f5e2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.2\n"
          ]
        }
      ],
      "id": "PSz1KN7A0kyi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpZufgx1veLO"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "id": "TpZufgx1veLO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "autMsTegxadV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0595215-3329-4eeb-964d-e950ab77a81a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m119.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "get_ipython().system('pip install torch numpy')"
      ],
      "id": "autMsTegxadV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaWwPFttIVXg",
        "outputId": "73de5651-6afb-4ec9-f4aa-15e15243f306"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/.git/\n",
            "remote: Enumerating objects: 141, done.\u001b[K\n",
            "remote: Counting objects: 100% (141/141), done.\u001b[K\n",
            "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "remote: Total 141 (delta 90), reused 76 (delta 34), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (141/141), 1.45 MiB | 12.60 MiB/s, done.\n",
            "Resolving deltas: 100% (90/90), done.\n",
            "From https://github.com/tsmatz/imitation-learning-tutorials\n",
            " * branch            master     -> FETCH_HEAD\n",
            " * [new branch]      master     -> origin/master\n"
          ]
        }
      ],
      "source": [
        "# Clone from Github Repository\n",
        "! git init .\n",
        "! git remote add origin https://github.com/RichardMinsooGo-RL-Gym/Imitation-learning-Gridworld.git\n",
        "! git pull origin master\n",
        "# ! git pull origin main"
      ],
      "id": "ZaWwPFttIVXg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2EAsH858ogk"
      },
      "source": [
        "## Overview of Relative Entropy Inverse Reinforcement Learning method\n",
        "\n",
        "First of all, let's briefly follow Relative Entropy IRL along with the original paper [[A Boularias et al., 2011](https://proceedings.mlr.press/v15/boularias11a/boularias11a.pdf)].\n",
        "\n",
        "Suppose $P(\\cdot)$ and $Q(\\cdot)$ are probability distributions.<br>\n",
        "The following relative entropy (or KL-divergence) $\\verb|Rel|(P || Q)$ is a distance of distributions between $P$ and $Q$, in which relative entropy always satisfies $\\verb|Rel|(P || Q) \\geq 0$ with equality if, and only if, $P(\\tau) = Q(\\tau)$\n",
        "\n",
        "$\\displaystyle \\verb|Rel|(P || Q) \\coloneqq \\sum_{\\tau} P(\\tau) \\ln{\\frac{P(\\tau)}{Q(\\tau)}}$\n",
        "\n",
        "> Note : The relative entropy (KL-divergence) is $\\displaystyle \\int P(\\tau) \\ln{\\frac{P(\\tau)}{Q(\\tau)}} d\\tau$ in continuous sapce, and $\\displaystyle \\sum_{\\tau} P(\\tau) \\ln{\\frac{P(\\tau)}{Q(\\tau)}} $ in discrete space.<br>\n",
        "> In this example, we assume it's discrete space.\n",
        "\n",
        "Assuming that $Q(\\cdot)$ is the distribution of expert policy (baseline policy) on trajectories $\\mathcal{T}$, Relative Entropy IRL is a method to find the distribution $P(\\cdot)$ to minimize :\n",
        "\n",
        "$\\displaystyle \\sum_{\\tau \\in \\mathcal{T}} P(\\tau) \\ln{\\frac{P(\\tau)}{Q(\\tau)}} $\n",
        "\n",
        "subject to the following constraints. :\n",
        "\n",
        "- $ \\left| \\sum_{\\tau \\in \\mathcal{T}} P(\\tau) f_i^{\\tau} - \\hat{f}_i \\right| \\leq \\epsilon_i \\;\\;\\; \\forall i \\in \\{1, \\ldots ,k\\}$\n",
        "- $ \\sum_{\\tau \\in \\mathcal{T}} P(\\tau) = 1 $\n",
        "- $ P(\\tau) \\geq 0 \\;\\;\\; \\forall \\tau \\in \\mathcal{T} $\n",
        "\n",
        "where $f_i^{\\tau}$ is the $i$-th element in the feature vector of the trajectory $\\tau$, and $\\hat{f}_i$ is the $i$-th element in the expected feature vector which is induced by the expert demonstrations.\n",
        "\n",
        "The first constraint is the feature expectation matching constraint. (See [here](./03_maxent_irl.ipynb) for the feature expectation matching in IRL.)<br>\n",
        "When $\\gamma$ is a discount, $f_i^{\\tau} = \\sum_t \\gamma^t f_i(s_t)$, where $t$ is time-step and $f_i(s_t)$ is the feature of state $s_t$. (Because the expected return is $\\sum_t \\gamma^t \\theta_i f_i(s_t) $, where $\\theta_i$ is $i$-th element of reward weight.)<br>\n",
        "The above $\\epsilon_i$ is obtained by Hoeffding bound as follows.\n",
        "\n",
        "$\\displaystyle \\epsilon_i = \\sqrt{\\frac{-\\ln{(1-\\delta)}}{2N}} \\frac{\\gamma^{H+1}-1}{\\gamma-1} \\left( \\max_s f_i(s) - \\min_s f_i(s) \\right) $\n",
        "\n",
        "where $N$ is the number of sampled trajectories and $\\delta$ is a confidence probability.\n",
        "\n",
        "> Note : I note that the following holds:<br>\n",
        "> $\\displaystyle \\frac{\\gamma^{H+1} - 1}{\\gamma - 1} = \\gamma^H + \\gamma^{H-1} + \\cdots + \\gamma + 1$\n",
        "\n",
        "> Note : In [original paper](https://proceedings.mlr.press/v15/boularias11a/boularias11a.pdf), $f(s, a)$ (not $f(s)$) is used as a feature of each step. $f(s, a)$ is a feature for taking an action $a$ on state $s$, and the reward should also be linear to the feature $f(s, a)$.<br>\n",
        "> In this example, I'll use $f(s)$ (in which, the reward is dependant only on the next state $s$) to simplify.\n",
        "\n",
        "When $\\delta$ is given, the probability that the difference between the feature counts given the distribution $P$ and the true feature counts is larger than $2\\epsilon$ becomes $\\delta$.\n",
        "\n",
        "By applying Lagrange multipliers ($\\theta$ and $\\eta$) with KKT (Karush–Kuhn–Tucker) condition, you can get the following Lagrangian. ($\\theta$ becomes a reward weight.) :\n",
        "\n",
        "$\\displaystyle L(P,\\theta,\\eta) = \\sum_{\\tau \\in \\mathcal{T}} P(\\tau) \\ln{\\frac{P(\\tau)}{Q(\\tau)}} - \\sum_{i=1}^k \\theta_i \\left( \\sum_{\\tau \\in \\mathcal{T}} P(\\tau) f_i^{\\tau}-\\hat{f}_i \\right) - \\sum_{i=1}^k |\\theta_i| \\epsilon_i + \\eta \\left( \\sum_{\\tau \\in \\mathcal{T}} P(\\tau) - 1 \\right) $\n",
        "\n",
        "> Note : See [here](https://tsmatz.wordpress.com/2020/06/01/svm-and-kernel-functions-mathematics/) for KKT (Karush–Kuhn–Tucker) condition and dual representation in optimization.\n",
        "\n",
        "By applying KKT condition $\\partial_{P(\\tau)} L(P,\\theta,\\eta) = 0$, we then get :\n",
        "\n",
        "$\\displaystyle \\ln{\\frac{P(\\tau)}{Q(\\tau)}} -\\sum_{i=1}^k \\theta_i f_i^{\\tau} + \\eta + 1 = 0 $\n",
        "\n",
        "Hence we can get the optimal $P(\\tau)$ by :\n",
        "\n",
        "$\\displaystyle P(\\tau)=Q(\\tau) \\exp{\\left( \\sum_{i=1}^k \\theta_i f_i^{\\tau} - \\eta - 1 \\right)}$\n",
        "\n",
        "Applying normalization, you can then get :\n",
        "\n",
        "$\\displaystyle P(\\tau|\\theta)=\\frac{1}{Z(\\theta)} Q(\\tau) \\exp{\\left( \\sum_{i=1}^k \\theta_i f_i^{\\tau} \\right)} \\;\\;\\;\\;\\; (1)$\n",
        "\n",
        "where\n",
        "\n",
        "$\\displaystyle Z(\\theta)\\coloneqq\\sum_{\\tau \\in \\mathcal{T}} Q(\\tau) \\exp{\\left( \\sum_{i=1}^k \\theta_i f_i^{\\tau} \\right)}$\n",
        "\n",
        "> Note : By normalization constraint $\\sum_{\\tau \\in \\mathcal{T}} P(\\tau) = 1$, you can also get :<br>\n",
        "> $\\exp(\\eta+1)=Z(\\theta)$\n",
        "\n",
        "By substituting and erasing variables, you can get the dual problem to maximize the following $g(\\theta)$ :\n",
        "\n",
        "$\\displaystyle g(\\theta) = \\sum_{i=1}^k \\theta_i \\hat{f}_i - \\ln{Z(\\theta)} - \\sum_{i=1}^k |\\theta_i| \\epsilon_i $\n",
        "\n",
        "The function $g$ is concave and differentiable everywhere except for $\\theta_i=0$, hence, it can be maximized by gradient-based optimization.<br>\n",
        "Now the partial differentiation of $g$ is given by :\n",
        "\n",
        "$\\displaystyle \\frac{\\partial}{\\partial \\theta_i}g(\\theta)=\\hat{f}_i - \\sum_{\\tau \\in \\mathcal{T}} P(\\tau|\\theta) f_i^{\\tau} - \\alpha_i \\epsilon_i \\;\\;\\;\\;\\; (2)$\n",
        "\n",
        "where $\\alpha_i=1$ if $\\theta_i \\geq 0$ and $\\alpha_i=-1$ otherwise.\n",
        "\n",
        "> Note : You can apply gradient method by:<br>\n",
        "> $\\displaystyle \\nabla g(\\theta) = \\sum_{i} \\frac{\\partial}{\\partial \\theta_i} g(\\theta) $\n",
        "\n",
        "When the transition probability $p(s_{t+1}|s_t,a_t)$ is known, you can easily get $P(\\tau | \\theta)$ by (1), and then you can get the gradient $\\frac{\\partial}{\\partial \\theta_i}g(\\theta)$ for optimization.<br>\n",
        "However, the transition probability is often unknow in practices.\n",
        "\n",
        "In such case, you can get $P(\\tau | \\theta)$ by importance sampling with an arbitrary policy.<br>\n",
        "Now let's see how to formulate $P(\\tau | \\theta)$ with importance sampling.\n",
        "\n",
        "First, let me decompose $Q(\\tau)$ by :\n",
        "\n",
        "$\\displaystyle Q(\\tau) = D(\\tau)U(\\tau)$\n",
        "\n",
        "where $D(\\tau)$ is the joint probability of transitions\n",
        "\n",
        "$\\displaystyle D(\\tau) = d_0(s_1) \\prod_{t=1}^H p(s_{t+1}|s_t,a_t)$\n",
        "\n",
        "and $U(\\tau)$ is the joint probability of the actions on trajectory $\\tau$.\n",
        "\n",
        "With this decomposition, the equation (1) becomes :\n",
        "\n",
        "$\\displaystyle P(\\tau|\\theta)=\\frac{D(\\tau)U(\\tau) \\exp{\\left( \\sum_{i=1}^k \\theta_i f_i^{\\tau} \\right)}}{\\sum_{\\tau \\in \\mathcal{T}} D(\\tau)U(\\tau) \\exp{\\left( \\sum_{i=1}^k \\theta_i f_i^{\\tau} \\right)}}$\n",
        "\n",
        "Now we approximate by importance sampling with an arbitrary policy $\\pi$.<br>\n",
        "Let $N$ be the number of trajectory samples, $\\mathcal{T}_N^{\\pi}$ be a set of sampled trajectories, and $\\pi(\\tau)$ be the joint probability of the actions on trajectory $\\tau$ in this given policy $\\pi$.<br>\n",
        "By applying importance sampling and equation (1), the second term in (2) is then approximated as follows. :\n",
        "\n",
        "$\\displaystyle \\sum_{\\tau \\in \\mathcal{T}} P(\\tau|\\theta) f_i^{\\tau}$\n",
        "\n",
        "$\\displaystyle \\simeq \\frac{1}{N} \\sum_{\\tau \\in \\mathcal{T}_N^{\\pi}} \\frac{P(\\tau|\\theta)}{D(\\tau)\\pi(\\tau)} f_i^{\\tau}$\n",
        "\n",
        "$\\displaystyle = \\frac{1}{N} \\frac{\\sum_{\\tau \\in \\mathcal{T}_N^{\\pi}}\\frac{D(\\tau)U(\\tau)\\exp \\left( \\sum_{j=1}^k \\theta_j f_j^{\\tau} \\right)}{D(\\tau)\\pi(\\tau)}f_i^{\\tau}}{\\sum_{\\tau \\in \\mathcal{T}} D(\\tau)U(\\tau) \\exp \\left( \\sum_{j=1}^k \\theta_j f_j^{\\tau} \\right)}$\n",
        "\n",
        "$\\displaystyle \\simeq \\frac{\\frac{1}{N} \\sum_{\\tau \\in \\mathcal{T}_N^{\\pi}} \\frac{D(\\tau)U(\\tau)\\exp \\left( \\sum_{j=1}^k \\theta_j f_j^{\\tau} \\right)}{D(\\tau)\\pi(\\tau)} f_i^{\\tau}}{\\frac{1}{N} \\sum_{\\tau \\in \\mathcal{T}_N^{\\pi}} \\frac{D(\\tau)U(\\tau)\\exp \\left( \\sum_{j=1}^k \\theta_j f_j^{\\tau} \\right)}{D(\\tau)\\pi(\\tau)}}$\n",
        "\n",
        "$\\displaystyle = \\frac{\\sum_{\\tau \\in \\mathcal{T}_N^{\\pi}} \\frac{U(\\tau)}{\\pi(\\tau)} \\exp \\left( \\sum_{j=1}^k \\theta_j f_j^{\\tau} \\right) f_i^{\\tau}}{\\sum_{\\tau \\in \\mathcal{T}_N^{\\pi}} \\frac{U(\\tau)}{\\pi(\\tau)} \\exp \\left( \\sum_{j=1}^k \\theta_j f_j^{\\tau} \\right)} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; (3)$\n",
        "\n",
        "As you can see above, the transition probabilities $D(\\tau)$ is erased and not needed in this equation.\n",
        "\n",
        "Now you can get the gradient $\\nabla g(\\theta)$ by equation (2), and you can optimize $\\theta$ with gradient-based method."
      ],
      "id": "Q2EAsH858ogk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtKNkwQB8ogo"
      },
      "source": [
        "## Implementation"
      ],
      "id": "ZtKNkwQB8ogo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwmojliA8ogo"
      },
      "source": [
        "Now let's implement above algorithm.\n",
        "\n",
        "In GridWorld example, we know the transition probability $p(s_{t+1}|s_t,a_t)$ and don't need to apply importance sampling.<br>\n",
        "But in this example, I assume that the transition probability is unknown and I then apply importance sampling.\n",
        "\n",
        "> Note : To speed up computation, I'll implement all operations with PyTorch tensors."
      ],
      "id": "hwmojliA8ogo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5784QeW8ogp"
      },
      "source": [
        "### 1. Restore environment and load expert's data"
      ],
      "id": "E5784QeW8ogp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVxXJ9Fa8ogq"
      },
      "source": [
        "Before we start, we need to install the required packages."
      ],
      "id": "wVxXJ9Fa8ogq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n2kX5_-8ogq",
        "outputId": "70e6187a-5100-4e11-bbfe-eb9afc54a6f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch numpy matplotlib"
      ],
      "id": "2n2kX5_-8ogq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrOUnVkx8ogt"
      },
      "source": [
        "This algorithm needs to compute the joint probability and then needs precision.<br>\n",
        "In order to prevent from vanishing values, I apply double precision for float operations."
      ],
      "id": "wrOUnVkx8ogt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJVQA4Rb8ogu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "torch.set_default_dtype(torch.float64)"
      ],
      "id": "PJVQA4Rb8ogu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rt6dzzD8ogv"
      },
      "source": [
        "Firstly, I restore GridWorld environment from JSON file. (For details about this environment, see [Readme.md](https://github.com/tsmatz/imitation-learning-tutorials/blob/master/Readme.md).)\n",
        "\n",
        "This time, all members are implemented as PyTorch tensor operations.\n",
        "\n",
        "> Note : See [this script](./00_generate_expert_trajectories.ipynb) for generating the same environment."
      ],
      "id": "4rt6dzzD8ogv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBWP9UzF8ogv"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from gridworld import GridWorld\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "with open(\"gridworld.json\", \"r\") as f:\n",
        "    json_object = json.load(f)\n",
        "    env = GridWorld(**json_object, device=device)"
      ],
      "id": "VBWP9UzF8ogv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwnABsTx8ogw"
      },
      "source": [
        "Now I visualize our GridWorld environment.\n",
        "\n",
        "The number in each cell indicates the reward score on this state.<br>\n",
        "The goal state is on the right-bottom corner (in which the reward is ```10.0```), and the initial state is uniformly picked up from the gray-colored cells.<br>\n",
        "If the agent can reach to goal state without losing any rewards, it will get ```10.0``` for total reward.\n",
        "\n",
        "See [Readme.md](https://github.com/tsmatz/imitation-learning-tutorials/blob/master/Readme.md) for details about the game rule of this environment."
      ],
      "id": "SwnABsTx8ogw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zieA6eYl8ogw",
        "outputId": "0179dd8e-4baf-48fa-ab82-f9e892d0353b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table><tr><td>0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>-1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>-1</td><td>-1</td><td>0</td><td>0</td><td>-1</td><td>-1</td></tr><tr><td>0</td><td>0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>0</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-1</td></tr><tr><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>0</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td>0</td><td>0</td></tr><tr><td>0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-1</td></tr><tr><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>-1</td><td>0</td><td>-1</td><td>0</td><td>-1</td><td>-1</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>0</td><td>-1</td><td>0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td>-1</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>0</td><td>0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td></tr><tr><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td>0</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td></tr><tr><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td></tr><tr><td>0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td></tr><tr><td>0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td></tr><tr><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td></tr><tr><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td>0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td></tr><tr><td>0</td><td>0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>0</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td>0</td><td>0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td>0</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td>0</td><td>0</td><td>0</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">10</td></tr></table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "valid_states_all = torch.cat((env.valid_states, torch.tensor([env.grid_size-1,env.grid_size-1]).to(device).unsqueeze(dim=0)))\n",
        "valid_states_all = valid_states_all[:,0] * env.grid_size + valid_states_all[:,1]\n",
        "\n",
        "html_text = \"<table>\"\n",
        "for row in range(env.grid_size):\n",
        "    html_text += \"<tr>\"\n",
        "    for col in range(env.grid_size):\n",
        "        if row*env.grid_size + col in valid_states_all:\n",
        "            html_text += \"<td bgcolor=\\\"gray\\\">\"\n",
        "        else:\n",
        "            html_text += \"<td>\"\n",
        "        html_text += str(env.reward_map[row*env.grid_size+col].tolist())\n",
        "        html_text += \"</td>\"\n",
        "    html_text += \"</tr>\"\n",
        "html_text += \"</table>\"\n",
        "\n",
        "display(HTML(html_text))"
      ],
      "id": "zieA6eYl8ogw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgb38xrG8ogy"
      },
      "source": [
        "Load expert's data (demonstrations) which is saved in ```./expert_data``` folder in this repository.\n",
        "\n",
        "> Note : See [this script](./00_generate_expert_trajectories.ipynb) for generating expert dataset."
      ],
      "id": "qgb38xrG8ogy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEP_qWrQ8ogz"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "dest_dir = \"./expert_data\"\n",
        "checkpoint_file = \"ckpt0.pkl\"\n",
        "\n",
        "# load expert data from pickle\n",
        "with open(f\"{dest_dir}/{checkpoint_file}\", \"rb\") as f:\n",
        "    exp_data = pickle.load(f)\n",
        "exp_states = torch.tensor(exp_data[\"states\"]).to(device)\n",
        "exp_actions = torch.tensor(exp_data[\"actions\"]).to(device)\n",
        "timestep_lens = exp_data[\"timestep_lens\"]"
      ],
      "id": "QEP_qWrQ8ogz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIH8z3oF8ogz"
      },
      "source": [
        "### 2. Get expert's feature expectation $\\hat{f}$ and expert's action probabilities"
      ],
      "id": "xIH8z3oF8ogz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOITgkwA8ogz"
      },
      "source": [
        "We can easily get the feature expectation $\\hat{f}$ (the following ```f_exp```) by using expert's demonstrations as follows.<br>\n",
        "(Here we apply discount $\\gamma=0.99$ unlike [previous example](./03_maxent_irl.ipynb).)\n",
        "\n",
        "$\\displaystyle \\hat{f} = \\sum_{\\tau} \\sum_t \\gamma^t f(s_t)$\n",
        "\n",
        "I note that expert's dataset doesn't have the final state, but the expert's feature expectation needs the whole states (including goal state).<br>\n",
        "Hence I add the final state in each trajectory."
      ],
      "id": "eOITgkwA8ogz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBudxppq8og0"
      },
      "outputs": [],
      "source": [
        "gamma = 0.99\n",
        "\n",
        "def get_discounted_traj_feature(traj_state_features, gamma):\n",
        "    \"\"\"\n",
        "    Convert from state's feature array to discounted trajectory feature\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    traj_state_features : float[N, H, K] or float[H, K]\n",
        "        State's feature array.\n",
        "        N is the number of trajectories, H is the number of timestep (i.e, horizon),\n",
        "        and K is dimension of feature.\n",
        "        If it's a single trajectory, you can use float[H, K] as input.\n",
        "    gamma : float\n",
        "        A discount value\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    traj_features : float[N, K] or float[K]\n",
        "        When fs_0^{\\tau}, fs_1^{\\tau}, ... , fs_H^{\\tau} is the state's features in trajectory,\n",
        "        it returns fs_0^{\\tau} + gamma * fs_1^{\\tau} + ... + gamma^H + fs_H^{\\tau} .\n",
        "        If the input type is float[H, K], the output type is float[K].\n",
        "    \"\"\"\n",
        "\n",
        "    # get horizon\n",
        "    horizon = traj_state_features.size(dim=-2)\n",
        "\n",
        "    # apply weight (discount)\n",
        "    seed = torch.ones(horizon).to(device) * gamma\n",
        "    exponent = torch.arange(horizon).to(device)\n",
        "    weight = torch.pow(seed, exponent)\n",
        "    traj_features_weighted = torch.mul(traj_state_features, weight.unsqueeze(dim=-1))\n",
        "\n",
        "    # sum up\n",
        "    return torch.sum(traj_features_weighted, dim=-2)"
      ],
      "id": "iBudxppq8og0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TigqPNt8og0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torch.nn import functional as F\n",
        "\n",
        "STATE_SIZE = env.grid_size*env.grid_size  # 2500\n",
        "ACTION_SIZE = env.action_size             # 4\n",
        "\n",
        "# initialize\n",
        "sum_of_features = torch.zeros(STATE_SIZE).to(device)\n",
        "\n",
        "#\n",
        "# get final state in all episodes\n",
        "#\n",
        "\n",
        "# collect states before final states in all episodes\n",
        "current_timestep = 0\n",
        "prev_states = []\n",
        "for timestep_len in timestep_lens:\n",
        "    prev_states.append(exp_states[current_timestep+timestep_len-1])\n",
        "    current_timestep += timestep_len\n",
        "prev_states = torch.tensor(prev_states).to(device)\n",
        "\n",
        "# collect actions before final states in all episodes\n",
        "current_timestep = 0\n",
        "prev_actions = []\n",
        "for timestep_len in timestep_lens:\n",
        "    prev_actions.append(exp_actions[current_timestep+timestep_len-1])\n",
        "    current_timestep += timestep_len\n",
        "prev_actions = torch.tensor(prev_actions).to(device)\n",
        "\n",
        "# get final states in all episodes\n",
        "final_states = env.step(prev_actions, prev_states, trans_state_only=True)\n",
        "\n",
        "#\n",
        "# loop and sum all episodes in demonstration\n",
        "#\n",
        "\n",
        "current_timestep = 0\n",
        "for i, timestep_len in enumerate(timestep_lens):\n",
        "    # pick up state's id array in a single trajectory\n",
        "    traj_states_exp = exp_states[current_timestep:current_timestep+timestep_len]\n",
        "\n",
        "    # add final state\n",
        "    traj_states_exp = torch.cat((traj_states_exp, final_states[i].unsqueeze(dim=0)))\n",
        "\n",
        "    # convert into state's features (i.e, one-hot)\n",
        "    traj_state_features_exp = F.one_hot(traj_states_exp, num_classes=STATE_SIZE)\n",
        "\n",
        "    # get trajectory feature with discount\n",
        "    traj_feature = get_discounted_traj_feature(\n",
        "        traj_state_features_exp.double(),\n",
        "        gamma\n",
        "    )\n",
        "\n",
        "    # sum up\n",
        "    sum_of_features += traj_feature\n",
        "\n",
        "    # proceed to next trajectory\n",
        "    current_timestep += timestep_len\n",
        "\n",
        "# divide by the number of trajectories\n",
        "f_exp = sum_of_features / len(timestep_lens)"
      ],
      "id": "2TigqPNt8og0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP15VX7i8og1"
      },
      "source": [
        "Next we create a matrix of expert action's probabilities ```action_prob_exp``` with shape ```[STATE_SIZE, ACTION_SIZE]```, in which the probability of action $a$ in state $s$ is the value of element ```[s, a]```.<br>\n",
        "Later this matrix is used to get the joint probability of the actions on trajectory $\\tau$,  i.e, $U(\\tau)$."
      ],
      "id": "yP15VX7i8og1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8Pwxa188og6",
        "outputId": "1f8a5563-273d-4bfd-d656-6334cfd99481"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2500, 0.2500, 0.2500, 0.2500],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
              "        ...,\n",
              "        [1.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.0000, 0.0000, 0.5000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# initialize\n",
        "current_timestep = 0\n",
        "action_count = torch.zeros(STATE_SIZE, ACTION_SIZE).to(device)\n",
        "\n",
        "# loop all trajectories\n",
        "for timestep_len in timestep_lens:\n",
        "    # pick up state's and action's id array in a single trajectory\n",
        "    states_in_traj = exp_states[current_timestep:current_timestep+timestep_len]\n",
        "    actions_in_traj = exp_actions[current_timestep:current_timestep+timestep_len]\n",
        "\n",
        "    # add 1 in element [s, a] to count actions\n",
        "    horizon = len(states_in_traj)\n",
        "    for i in range(horizon):\n",
        "        s = states_in_traj[i]\n",
        "        a = actions_in_traj[i]\n",
        "        action_count[s, a] += 1.0\n",
        "\n",
        "    # proceed to next trajectory\n",
        "    current_timestep += timestep_len\n",
        "\n",
        "# set 1 if all actions are zero in each state\n",
        "# (to prevent from dividing by zero)\n",
        "action_count += torch.all(action_count==0.0, dim=1).double().unsqueeze(dim=-1)\n",
        "\n",
        "# get action probability in each state\n",
        "action_sum = torch.sum(action_count, dim=1, keepdim=True)\n",
        "action_prob_exp = torch.div(action_count, action_sum)\n",
        "\n",
        "action_prob_exp"
      ],
      "id": "Z8Pwxa188og6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDgzEyPt8og6"
      },
      "source": [
        "### 3. Create a function to generate $N$ trajectorie's samples"
      ],
      "id": "RDgzEyPt8og6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8CW9BRe8og7"
      },
      "source": [
        "Now I create a function to generate trajectory samples with an arbitrary policy $\\pi$.\n",
        "\n",
        "In this example, I simply perform uniform sampling as an arbitrary policy.<br>\n",
        "However, we should prevent $U(\\tau)$ from being zero when we pick up a sample trajectory $\\tau$.\n",
        "\n",
        "To do this, firstly we create an action's probability matrix ```action_prob_pi```, with which the action is uniformly picked up, but the probability has zero when the corresponding probability in ```action_prob_exp``` is zero."
      ],
      "id": "K8CW9BRe8og7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JYZpKCW8og7",
        "outputId": "fa16a7ab-1b0a-44b7-f57f-6e80de304b4e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2500, 0.2500, 0.2500, 0.2500],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
              "        ...,\n",
              "        [1.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.0000, 0.0000, 0.5000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "action_prob_pi = torch.ones(STATE_SIZE, ACTION_SIZE).to(device)\n",
        "action_prob_pi *= action_prob_exp.bool()\n",
        "action_sum = torch.sum(action_prob_pi, dim=1, keepdim=True)\n",
        "action_prob_pi = torch.div(action_prob_pi, action_sum)\n",
        "\n",
        "action_prob_pi"
      ],
      "id": "5JYZpKCW8og7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGTB3_Gt8og8"
      },
      "source": [
        "I note that ```action_prob_pi``` is uniform distribution, unlike previous ```action_prob_exp```.<br>\n",
        "For instance, ```action_prob_exp[6]``` (action's probabilities of expert in state ```6```) is ```[0.0000, 0.3333, 0.0000, 0.6667]```, but the corresponding ```action_prob_pi[6]``` has uniform probabilities in possible action ```1``` and ```3```."
      ],
      "id": "qGTB3_Gt8og8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZIb9wt38og8",
        "outputId": "8590dd28-b9bb-472d-8db0-42f6b97d2390"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0000, 0.3333, 0.0000, 0.6667], device='cuda:0')\n",
            "tensor([0.0000, 0.5000, 0.0000, 0.5000], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(action_prob_exp[6])\n",
        "print(action_prob_pi[6])"
      ],
      "id": "GZIb9wt38og8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdIVikbh8og9"
      },
      "source": [
        "Now I create a function to generate $N$ trajectorie's samples.<br>\n",
        "This function uses above probability matrix to pick up samples.\n",
        "\n",
        "This function returns the following 2 tensors.\n",
        "\n",
        "1. A tensor to include the visited states. The shape of this tensor is ```[N, H+1]```, in which the visited state of the timestep $t$ in $n$-th trajectory is the value of element ```[n, t]```.\n",
        "2. A tensor to include the taken actions. The shape of this tensor is ```[N, H]```, in which the taken action of the timestep $t$ in $n$-th trajectory is the value of element ```[n, t]```.\n",
        "\n",
        "> Note : Because it includes the final state (see above), the shape of visited states is ```[N, H+1]```, not ```[N, H]```.\n",
        "\n",
        "When the number of timesteps doesn't reach to ```H``` in some trajectory, the exceeded timestep's elements in the state's tensor are filled with ```IGNORE_STATE=2500```, and the action's tensor are filled with ```IGNORE_ACTION=4```.\n",
        "\n",
        "> Note : You can run as a batch to speed up. (Here I run each inference one by one.)"
      ],
      "id": "jdIVikbh8og9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYlYlH6y8og9"
      },
      "outputs": [],
      "source": [
        "# The index 4 is an ignore-action.\n",
        "# (0, 1, 2, 3 are valid actions.)\n",
        "IGNORE_ACTION = ACTION_SIZE\n",
        "# The index 2500 is an ignore-state.\n",
        "# (0, 1, ... , 2499 are valid states.)\n",
        "IGNORE_STATE = STATE_SIZE\n",
        "\n",
        "def create_trajectories(traj_num, horizen, action_prob_pi):\n",
        "    # initialize results\n",
        "    actions = torch.ones(traj_num, horizen, dtype=int).to(device) * IGNORE_ACTION\n",
        "    states = torch.ones(traj_num, horizen+1, dtype=int).to(device) * IGNORE_STATE\n",
        "\n",
        "    # collect data\n",
        "    for traj in range(traj_num):\n",
        "        # initialize episode\n",
        "        done = False\n",
        "        s = env.reset(batch_size=1)\n",
        "        s = s.squeeze(dim=0)\n",
        "        states[traj, 0] = s\n",
        "        # do until the episode ends\n",
        "        for step in range(horizen):\n",
        "            # pick up action using above action_prob_pi\n",
        "            a = torch.multinomial(action_prob_pi[s], 1, replacement=True).squeeze(dim=0)\n",
        "            actions[traj, step] = a\n",
        "            # step to the next state\n",
        "            s, _, term, trunc = env.step(a.unsqueeze(dim=0), s.unsqueeze(dim=0))\n",
        "            s = s.squeeze(dim=0)\n",
        "            done = torch.logical_or(term, trunc)\n",
        "            states[traj, step+1] = s\n",
        "            # exit loop when it's done\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "    return states, actions"
      ],
      "id": "iYlYlH6y8og9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQmVLlCx8og-"
      },
      "source": [
        "The generated policy ```action_prob_pi``` is an uniform distribution, and it won't get optimal trajectories unlike expert distribution.<br>\n",
        "As you can see below, it can reach to the goal state and the length of trajectory is then small in the expert distribution. On contrary, the length of trajectory is large in the uniform distribution $\\pi$."
      ],
      "id": "xQmVLlCx8og-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOSO5D3l8og-",
        "outputId": "9e3edcb5-11a6-43de-acbb-90963c2baed7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** length (timestep size) of trajectories picked up by pi *****\n",
            "tensor([201, 201, 201, 201, 201, 201, 201, 201, 201, 201], device='cuda:0')\n",
            "***** length (timestep size) of trajectories picked up by expert *****\n",
            "tensor([ 17, 201,  32, 201,  29,  25,  12,  47,  49,  33], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "test_states, test_actions = create_trajectories(10, 200, action_prob_pi)\n",
        "print(\"***** length (timestep size) of trajectories picked up by pi *****\")\n",
        "print(torch.count_nonzero(test_states != IGNORE_STATE, dim=1))\n",
        "test_states, test_actions = create_trajectories(10, 200, action_prob_exp)\n",
        "print(\"***** length (timestep size) of trajectories picked up by expert *****\")\n",
        "print(torch.count_nonzero(test_states != IGNORE_STATE, dim=1))"
      ],
      "id": "GOSO5D3l8og-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jGpnFLU8og_"
      },
      "source": [
        "### 4. Create a matrix used to get features"
      ],
      "id": "_jGpnFLU8og_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg5hLtQi8og_"
      },
      "source": [
        "In order to get features in trajectory, we can't use ```torch.nn.functional.one_hot()```, because we have ```IGNORE_STATE```. (See above.)<br>\n",
        "When it's ```IGNORE_STATE```, we want to convert the state into the feature, in which elements are all zeros.\n",
        "\n",
        "To do this, we now create a feature-mapping tensor $F$ with the shape ```[STATE_SIZE + 1, STATE_SIZE]```, in which ```i```-th row has state's feature for the state id=```i```. With this mapping tensor, ```F[IGNORE_STATE,:]``` becomes ```IGNORE_STATE``` feature, which elements are all zeros."
      ],
      "id": "pg5hLtQi8og_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tcy_08z8og_",
        "outputId": "659ab4d9-783d-47f4-a2b6-38601b3642c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "state_features = F.one_hot(torch.arange(STATE_SIZE).to(device), num_classes=STATE_SIZE)\n",
        "ignore_state_feature = torch.zeros(STATE_SIZE).to(device).unsqueeze(dim=0)\n",
        "state_features = torch.cat((state_features, ignore_state_feature))\n",
        "\n",
        "state_features"
      ],
      "id": "2tcy_08z8og_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcogvC4N8ohA"
      },
      "source": [
        "Suppose, $\\mathcal{T}^{\\verb|sample|}$ is a state's tensor which shape is ```[N, H]```, where ```N``` is the number of trajectories and ```H``` is the number of timestep (i.e, horizon).<br>\n",
        "By using mapping tensor $F$, you can get its state's features (which shape is ```[N, H, K]``` where ```K``` is the dimension of features) by $F [\\mathcal{T}^{\\verb|sample|}]$.\n",
        "\n",
        "For instance, the following state's matrix is converted into the following state's feature tensor, which shape is ```[2, 3, 2500]```. (Especially, the ```IGNORE_STATE``` state has the feature which elements are all zeros.)\n",
        "\n",
        "$\\displaystyle \\mathcal{T}^{\\verb|sample|} = \\begin{bmatrix} 0 & 1 & 2498 \\\\ 2498 & 2 & \\verb|IGNORE_STATE| \\\\ \\end{bmatrix} $"
      ],
      "id": "qcogvC4N8ohA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7B4xxoJ8ohA",
        "outputId": "008c7056-df77-414e-c48b-10e5cb1f0860"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1., 0., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 1., 0.]],\n",
              "\n",
              "        [[0., 0., 0.,  ..., 0., 1., 0.],\n",
              "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
              "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "test_trajectory_states = torch.tensor([\n",
        "    [0,    1, 2498],\n",
        "    [2498, 2, IGNORE_STATE]]).to(device)\n",
        "test_state_features = state_features[test_trajectory_states]\n",
        "test_state_features"
      ],
      "id": "d7B4xxoJ8ohA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMci9jIg8ohB"
      },
      "source": [
        "With state feature $f_i(s_t)$, the feature of trajectory $\\tau$ (which shape is ```[2, 2500]```) is then obtained by $f_i^{\\tau} = \\sum_t \\gamma^t f_i(s_t)$."
      ],
      "id": "AMci9jIg8ohB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzNqd9Zs8ohB",
        "outputId": "158cef51-71d7-40e7-e50a-6f82583cc9f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.9900, 0.0000,  ..., 0.0000, 0.9801, 0.0000],\n",
              "        [0.0000, 0.0000, 0.9900,  ..., 0.0000, 1.0000, 0.0000]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "get_discounted_traj_feature(test_state_features, gamma)"
      ],
      "id": "gzNqd9Zs8ohB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1m0_Tc_8ohC"
      },
      "source": [
        "### 5. Create a matrix used to get the joint probability $U(\\tau)$"
      ],
      "id": "B1m0_Tc_8ohC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQoNACE_8ohC"
      },
      "source": [
        "Now I create a probability matrix (tensor) to get $U(\\tau)$.<br>\n",
        "This matrix is almost same as ```action_prob_exp``` (an expert's state-action probability matrix), but it should handle special index, ```IGNORE_STATE``` and ```IGNORE_ACTION```.\n",
        "\n",
        "First, we create a matrix by extending the shape of ```action_prob_exp``` (which is ```[STATE_SIZE, ACTION_SIZE]```) to the shape ```[STATE_SIZE, ACTION_SIZE + 1]```, in which the action in ```IGNORE_ACTION``` column always has probability ```1.0```."
      ],
      "id": "SQoNACE_8ohC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8KZBaVs8ohK",
        "outputId": "aea215e6-b92e-43dc-c83c-1c87ec1f365f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
              "        ...,\n",
              "        [1.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
              "        [0.5000, 0.0000, 0.0000, 0.5000, 1.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 1.0000]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "ignore_actions_all = torch.ones(STATE_SIZE).to(device).unsqueeze(dim=1)\n",
        "probmat_u = torch.cat((action_prob_exp, ignore_actions_all), dim=1)\n",
        "\n",
        "probmat_u"
      ],
      "id": "K8KZBaVs8ohK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esDkZ-jK8ohL"
      },
      "source": [
        "We also add the action probabilities of ```IGNORE_STATE``` state, which elements are also all ```1.0```."
      ],
      "id": "esDkZ-jK8ohL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blVlj6to8ohL",
        "outputId": "34d345b4-7bfb-4a28-b0a9-f1717b2aa0ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
              "        ...,\n",
              "        [0.5000, 0.0000, 0.0000, 0.5000, 1.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "ignore_state_actions = torch.ones(ACTION_SIZE + 1).to(device).unsqueeze(dim=0)\n",
        "probmat_u = torch.cat((probmat_u, ignore_state_actions))\n",
        "\n",
        "probmat_u"
      ],
      "id": "blVlj6to8ohL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CryHrIie8ohM"
      },
      "source": [
        "Now I denote the above matrix (tensor) ```probmat_u``` as ${\\verb|Pr|}^{\\verb|exp|}$.\n",
        "\n",
        "Suppose, $\\mathcal{T}^{\\verb|sample|}_s$ is one-hot state's matrix, which shape is ```[N, H, S]```, where ```N``` is the number of trajectories, ```H``` is the number of timestep (i.e, horizon), and ```S``` is the number of states. (I note that ```S``` is ```STATE_SIZE+1```, which includes ```IGNORE_STATE```.)<br>\n",
        "Also, suppose,  $\\mathcal{T}^{\\verb|sample|}_a$ is the corresponding one-hot action's matrix, which shape is ```[N, H, A]```, where ```A``` is the number of actions. (I note that ```A``` is ```ACTION_SIZE+1```, which includes ```IGNORE_ACTION```.)\n",
        "\n",
        "In this assumption, the probabilities of taken actions in each states are obtained by the following equation.<br>\n",
        "I note that the following dot product operation is performed in each trajectories. :\n",
        "\n",
        "$\\displaystyle \\left( \\mathcal{T}^{\\verb|sample|}_s {\\verb|Pr|}^{\\verb|exp|} \\right) \\cdot \\mathcal{T}^{\\verb|sample|}_a $\n",
        "\n",
        "For instance, if the actions ```[[2,IGNORE_ACTION,3],[1,2,3]]``` is taken in the states ```[[0,1,2498],[2498,2,IGNORE_STATE]]```, each probabilties of the taken actions become ```[[0.2500, 1.0000, 0.5000],[0.0000, 0.2500, 1.0000]]``` by the following computation."
      ],
      "id": "CryHrIie8ohM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94sMekbi8ohM",
        "outputId": "16d15039-d5db-43cb-8775-aa8d7cbc3115"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 1, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 1, 0, 0]],\n",
              "\n",
              "        [[0, 0, 0,  ..., 1, 0, 0],\n",
              "         [0, 0, 1,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 1]]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "states_onehot = F.one_hot(torch.tensor([\n",
        "    [0,    1, 2498],\n",
        "    [2498, 2, IGNORE_STATE]]).to(device), num_classes=STATE_SIZE+1)\n",
        "\n",
        "states_onehot"
      ],
      "id": "94sMekbi8ohM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voKN2tzQ8ohN",
        "outputId": "cac29797-4573-46fb-ab83-c70e7964df40"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0, 0, 1, 0, 0],\n",
              "         [0, 0, 0, 0, 1],\n",
              "         [0, 0, 0, 1, 0]],\n",
              "\n",
              "        [[0, 1, 0, 0, 0],\n",
              "         [0, 0, 1, 0, 0],\n",
              "         [0, 0, 0, 1, 0]]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "actions_onehot = F.one_hot(torch.tensor([\n",
        "    [2, IGNORE_ACTION, 3],\n",
        "    [1, 2,             3]]).to(device), num_classes=ACTION_SIZE+1)\n",
        "\n",
        "actions_onehot"
      ],
      "id": "voKN2tzQ8ohN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p215lcEP8ohN",
        "outputId": "7a09886f-33a3-4fe8-9a3f-b95642f742df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
              "         [0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
              "         [0.5000, 0.0000, 0.0000, 0.5000, 1.0000]],\n",
              "\n",
              "        [[0.5000, 0.0000, 0.0000, 0.5000, 1.0000],\n",
              "         [0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
              "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "action_probs_in_each_states = torch.matmul(states_onehot.double(), probmat_u)\n",
        "\n",
        "action_probs_in_each_states"
      ],
      "id": "p215lcEP8ohN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOJeF5Av8ohO",
        "outputId": "9d218061-6022-4b56-c76a-a13385235815"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2500, 1.0000, 0.5000],\n",
              "        [0.0000, 0.2500, 1.0000]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "probs_for_taken_actions = torch.sum(torch.mul(action_probs_in_each_states, actions_onehot.double()), dim=2)\n",
        "\n",
        "probs_for_taken_actions"
      ],
      "id": "VOJeF5Av8ohO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CER2bDjm8ohP"
      },
      "source": [
        "Then you can now easily get the joint probability $U(\\tau)$ in each trajectory by product operation as follows.\n",
        "\n",
        "> Note : The joint probability (by an expert) $U(\\tau)$ will vanish if the number of horizon ```H``` is large.<br>\n",
        "> So, in the training, I will first compute the elements in $\\frac{U(\\tau)}{\\pi(\\tau)}$, and then get the joint probability of these elements.\n",
        "\n",
        "> Note : If trajectory $\\tau$ is never visited by an expert (i.e, the probability is zero), its joint probability $U(\\tau)$ will become zero. As you saw above, this is prevented by picking up trajectorie's samples with above ```action_prob_pi```."
      ],
      "id": "CER2bDjm8ohP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGEVMooy8ohP",
        "outputId": "8de38c53-92f7-4e66-ce38-2feaeeba43e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1250, 0.0000], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "joint_probs = torch.prod(probs_for_taken_actions, dim=1)\n",
        "\n",
        "joint_probs"
      ],
      "id": "uGEVMooy8ohP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_FNReoK8ohP"
      },
      "source": [
        "### 6. Create a matrix used to get the joint probability $\\pi(\\tau)$"
      ],
      "id": "g_FNReoK8ohP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwMtOFgh8ohQ"
      },
      "source": [
        "We also create a state-action probability matrix to get the joint probability $\\pi(\\tau)$.<br>\n",
        "This matrix is almost same as ```action_prob_pi``` (uniform probability matrix), but it should also handle special index, ```IGNORE_STATE``` and ```IGNORE_ACTION```."
      ],
      "id": "EwMtOFgh8ohQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_-iolrk8ohQ",
        "outputId": "4cbaf34f-e529-4eb0-d4bb-4dde909dfd24"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
              "        ...,\n",
              "        [0.5000, 0.0000, 0.0000, 0.5000, 1.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# add IGNORE_ACTION with probability 1.0\n",
        "probmat_pi = torch.cat((action_prob_pi, ignore_actions_all), dim=1)\n",
        "# add IGNORE_STATE with probability 1.0\n",
        "probmat_pi = torch.cat((probmat_pi, ignore_state_actions))\n",
        "\n",
        "probmat_pi"
      ],
      "id": "7_-iolrk8ohQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faadG_5V8ohQ"
      },
      "source": [
        "When I denote the matrix ```probmat_pi``` as ${\\verb|Pr|}^{\\pi}$, the probabilities of taken actions in each states are also obtained by the following equation.\n",
        "\n",
        "$\\displaystyle \\left( \\mathcal{T}^{\\verb|sample|}_s {\\verb|Pr|}^{\\pi} \\right) \\cdot \\mathcal{T}^{\\verb|sample|}_a $\n",
        "\n",
        "Then you can also easily get the joint probability $\\pi(\\tau)$ by the product operation."
      ],
      "id": "faadG_5V8ohQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5r0Wa2o8ohR"
      },
      "source": [
        "### 7. Create a function to get $\\sum_{\\tau \\in \\mathcal{T}} P(\\tau|\\theta) f_i^{\\tau}$"
      ],
      "id": "d5r0Wa2o8ohR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hej8ZgiE8ohR"
      },
      "source": [
        "Now we're ready to create a function to get the second term in equation (2).\n",
        "\n",
        "In our example (GridWorld), we know the transition probability $p(s_{t+1}|s_t,a_t)$ and don't need to apply importance sampling.<br>\n",
        "But in this example, I assume that the transition probability is unknown and I'll then apply importance sampling as follows.\n",
        "\n",
        "$\\displaystyle \\sum_{\\tau \\in \\mathcal{T}} P(\\tau|\\theta) f_i^{\\tau} \\simeq \\frac{\\sum_{\\tau \\in \\mathcal{T}_N^{\\pi}} \\frac{U(\\tau)}{\\pi(\\tau)} \\exp \\left( \\sum_{j=1}^k \\theta_j f_j^{\\tau} \\right) f_i^{\\tau}}{\\sum_{\\tau \\in \\mathcal{T}_N^{\\pi}} \\frac{U(\\tau)}{\\pi(\\tau)} \\exp \\left( \\sum_{j=1}^k \\theta_j f_j^{\\tau} \\right)} $\n",
        "\n",
        "To avoid vanishing of the joint probability $U(\\tau)$ and $\\pi(\\tau)$, I don't directly compute $U(\\tau)$ and $\\pi(\\tau)$.<br>\n",
        "Instead, I'll compute the elements of $\\frac{U(\\tau)}{\\pi(\\tau)}$ in every actions and then I'll apply product's operation to get the joint probabilities of $\\frac{U(\\tau)}{\\pi(\\tau)}$."
      ],
      "id": "Hej8ZgiE8ohR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8sM3jH-8ohS"
      },
      "outputs": [],
      "source": [
        "def get_second_term(visited_states, taken_actions, probmat_u, probmat_pi, theta, gamma):\n",
        "    # compute f_i^{\\tau} -- shape:(N, STATE_SIZE)\n",
        "    traj_state_features = state_features[visited_states]\n",
        "    feature_term = get_discounted_traj_feature(traj_state_features, gamma)\n",
        "    # compute exp(\\sum_{j=1}^k \\theta_j f_j^{\\tau}) -- shape:(N,)\n",
        "    exp_seed = torch.sum(torch.mul(feature_term, theta.unsqueeze(dim=0)), dim=1)\n",
        "    exp_term = torch.exp(exp_seed - torch.max(exp_seed))  # to prevent from being inf\n",
        "    # create elements (probabilities in every actions) of U(\\tau) -- shape:(N,H)\n",
        "    states_onehot = F.one_hot(visited_states[:,:-1], num_classes=STATE_SIZE+1)\n",
        "    actions_onehot = F.one_hot(taken_actions, num_classes=ACTION_SIZE+1)\n",
        "    action_probs_in_each_states_u = torch.matmul(states_onehot.double(), probmat_u)\n",
        "    probs_for_taken_actions_u = torch.sum(torch.mul(action_probs_in_each_states_u, actions_onehot.double()), dim=2)\n",
        "    # create elements (probabilities in every actions) of \\pi(\\tau) -- shape:(N,H)\n",
        "    action_probs_in_each_states_pi = torch.matmul(states_onehot.double(), probmat_pi)\n",
        "    probs_for_taken_actions_pi = torch.sum(torch.mul(action_probs_in_each_states_pi, actions_onehot.double()), dim=2)\n",
        "    # create elements (probability ratio in every actions) of U(\\tau) / \\pi(\\tau) -- shape:(N,H)\n",
        "    probs_ratio = torch.div(probs_for_taken_actions_u, probs_for_taken_actions_pi)\n",
        "    # compute U(\\tau) / \\pi(\\tau) (i.e, math product of elements) -- shape:(N,)\n",
        "    u_pi_ratio = torch.prod(probs_ratio, dim=1)\n",
        "    # compute denominator (sum up with trajectories) -- shape:()\n",
        "    elems = torch.mul(u_pi_ratio, exp_term)\n",
        "    denom = torch.sum(elems)\n",
        "    # compute numerator (sum up with trajectories) -- shape:(STATE_SIZE,)\n",
        "    norm = torch.sum(torch.mul(feature_term, elems.unsqueeze(dim=-1)), dim=0)\n",
        "    # final result -- shape:(STATE_SIZE,)\n",
        "    return torch.div(norm, denom)"
      ],
      "id": "Z8sM3jH-8ohS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIDrc15R8ohT"
      },
      "source": [
        "### 8. Set threshold $\\epsilon$"
      ],
      "id": "CIDrc15R8ohT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axj2aV3u8ohT"
      },
      "source": [
        "Now we get the following Hoeffding bound (thereshold) $\\epsilon$.\n",
        "\n",
        "$\\displaystyle \\epsilon_i = \\sqrt{\\frac{-\\ln{(1-\\delta)}}{2N}} \\frac{\\gamma^{H+1}-1}{\\gamma-1} \\left( \\max_s f_i(s) - \\min_s f_i(s) \\right) $\n",
        "\n",
        "where $N$ is the number of sampled trajectories and $\\delta$ is a confidence probability.\n",
        "\n",
        "> Note : As I have mentioned above, when the number of the taken actions is ```H```, the number of the visited states becomes ```H+1```.\n",
        "\n",
        "In this example, I empirically set ```0.0001``` as a confidence parameter $\\delta$. (So the probability that the difference between the feature counts given\n",
        "by the distribution $P$ and the true feature counts of the expert's policy is larger than $2\\epsilon$ is ```0.0001```.)"
      ],
      "id": "Axj2aV3u8ohT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNRRbHSU8ohU",
        "outputId": "cd042e65-8d8d-4cb4-84bb-c99fa7f7de6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Epsilon without weighted sum *****\n",
            "tensor([0.0022, 0.0022, 0.0022,  ..., 0.0022, 0.0022, 0.0022], device='cuda:0')\n",
            "***** Epsilon with weighted sum *****\n",
            "tensor([0.0599, 0.0599, 0.0599,  ..., 0.0599, 0.0599, 0.0599], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "N = 10           # the number of trajectorie's samples\n",
        "H = 30           # the number of horizon\n",
        "delta = 0.0001   # confidence parameter\n",
        "\n",
        "epsilon = torch.sqrt(-torch.log(torch.ones(STATE_SIZE).to(device) - delta) / (2.0*N))\n",
        "print(\"***** Epsilon without weighted sum *****\")\n",
        "print(epsilon)\n",
        "epsilon = epsilon * (gamma**(H+1) - 1.0) / (gamma - 1.0)\n",
        "print(\"***** Epsilon with weighted sum *****\")\n",
        "print(epsilon)"
      ],
      "id": "xNRRbHSU8ohU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AHwnOOF8ohV"
      },
      "source": [
        "### 9. Put it all together (Train and optimize parameter)"
      ],
      "id": "2AHwnOOF8ohV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5ke3N2B8ohV"
      },
      "source": [
        "Now we compute the following gradient and then optimize $\\theta$.\n",
        "\n",
        "$\\displaystyle \\frac{\\partial}{\\partial \\theta_i}g(\\theta)=\\hat{f}_i - \\sum_{\\tau \\in \\mathcal{T}} P(\\tau|\\theta) f_i^{\\tau} - \\alpha_i \\epsilon_i$\n",
        "\n",
        "In this example, we directly update $\\theta$ by the gradient (not using backprop) in 2000 iterations as follows.\n",
        "\n",
        "> Note : Because we try to maximize the dual problem $g(\\theta)$ (not minimize loss), we then apply **gradient ascent**, instead of gradient descent."
      ],
      "id": "J5ke3N2B8ohV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5U-QyaN8ohW"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    theta,\n",
        "    gamma,\n",
        "    num_samples,\n",
        "    horizon,\n",
        "    probmat_u,\n",
        "    probmat_pi,\n",
        "    action_prob_pi,\n",
        "    epsilon,\n",
        "    start_iter, end_iter,\n",
        "    lr,\n",
        "    verbose=True):\n",
        "\n",
        "    for iter_num in range(start_iter, end_iter):\n",
        "        # create N sample trajectories with pi\n",
        "        state_samples, action_samples = create_trajectories(num_samples, horizon, action_prob_pi)\n",
        "        # get \\sum_{\\tau} P(\\tau|\\theta) f_i^{\\tau}\n",
        "        second_term = get_second_term(state_samples, action_samples, probmat_u, probmat_pi, theta, gamma)\n",
        "        # get alpha\n",
        "        alpha = torch.sign(torch.where(theta == 0.0, 1.0, theta))\n",
        "        # compute gradient of g(\\theta)\n",
        "        nabla = f_exp - second_term - alpha * epsilon\n",
        "\n",
        "        # update theta (see above note)\n",
        "        theta += nabla * lr\n",
        "\n",
        "        # (output logs)\n",
        "        if verbose:\n",
        "            t_mean = torch.mean(torch.abs(nabla)).tolist()\n",
        "            t_max = torch.max(torch.abs(nabla)).tolist()\n",
        "            print(\"iter{}: nabla - average: {:1.4f}  max: {:2.4f}\".format(iter_num, t_mean, t_max), end=\"\\r\")\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\nDone\")"
      ],
      "id": "I5U-QyaN8ohW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2UslM8w8ohW",
        "outputId": "85931d14-3fe7-4a52-b126-325f5adca35a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter1999: nabla - average: 0.0713  max: 2.7909\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "# initialize parameters (theta)\n",
        "theta = torch.empty(STATE_SIZE).to(device)\n",
        "torch.nn.init.uniform_(theta, a=0.0, b=1.0)\n",
        "\n",
        "# optimize theta\n",
        "train(\n",
        "    theta=theta,\n",
        "    gamma=gamma,\n",
        "    num_samples=N,\n",
        "    horizon=H,\n",
        "    probmat_u=probmat_u,\n",
        "    probmat_pi=probmat_pi,\n",
        "    action_prob_pi=action_prob_pi,\n",
        "    epsilon=epsilon,\n",
        "    start_iter=0,\n",
        "    end_iter=2000,\n",
        "    lr=0.005,\n",
        ")"
      ],
      "id": "g2UslM8w8ohW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLjgoXdR8ohX"
      },
      "source": [
        "Now let's see the heatmap of the trained $\\theta$.<br>\n",
        "As you can see below, the states on the path toward the goal state are hot in this map. (See [Readme.md](https://github.com/tsmatz/imitation-learning-tutorials/blob/master/Readme.md) for game rule in this environment.)\n",
        "\n",
        "It's worth noting that this result is approximated by applying importance sampling as above.\n",
        "\n",
        "> Note : Strictly speaking, the reward in state $s_i$ is $\\theta^T \\cdot f(s_i)$.<br>\n",
        "> But in this example, the state's feature is just a one-hot vector, and I have then simply used $\\theta$ to check results.\n",
        "\n",
        "> Note : In the area of the following yellow spot (extremely hot), the expert doesn't also work well (hence the expert agent will walk around this area in order not to lose the reward), and then the visited frequencies in this area is becoming so large."
      ],
      "id": "FLjgoXdR8ohX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SfuvkPe8ohX",
        "outputId": "1df6bd76-2311-4acd-bf5c-5e70a9c59a7f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "state_vals = torch.reshape(theta, (env.grid_size, env.grid_size))\n",
        "plt.imshow(state_vals.cpu().numpy(), cmap=\"hot\", interpolation=\"nearest\")\n",
        "plt.show()"
      ],
      "id": "0SfuvkPe8ohX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huIWXg8w8ohX"
      },
      "outputs": [],
      "source": [],
      "id": "huIWXg8w8ohX"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}