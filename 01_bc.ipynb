{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f65db394-c47a-49e9-b1d2-0b29aae5e186",
   "metadata": {},
   "source": [
    "# Behavior Cloning (BC)\n",
    "\n",
    "In Behavior Cloning (BC), we find optimal parameter $\\theta$ in policy $\\pi_{\\theta}$ by solving a regression (or classification) problem using expert's dataset $\\mathcal{D}$ as a supervised learning.<br>\n",
    "Therefore, you can simply apply existing regression (or classification) methods - such as, Gaussian model, GMM, non-parametric method (LWR, GPR), or neural network learners.<br>\n",
    "See [my post](https://tsmatz.wordpress.com/2017/08/30/regression-in-machine-learning-math-for-beginners/) for the design choice of regression problems.\n",
    "\n",
    "In this notebook, I'll build neural network policy $\\pi_{\\theta}$ and then optimize parameters (weights) by minimizing cross-entropy loss in PyTorch.\n",
    "\n",
    "The trained policy is then available in regular reinforcement learning (RL) methods, if you refine models to get better performance. (See [here](https://github.com/tsmatz/reinforcement-learning-tutorials) for RL algorithms.)\n",
    "\n",
    "BC is a basic approach for imitation learning, and easily applied into the various scenarios.\n",
    "\n",
    "But it's worth noting that it also has the shortcomings to apply in some situations.<br>\n",
    "One of these is that the agent trained by BC might sometimes happens to encounter unknown states which are not included in the initial expert's behaviors. (Because expert dataset doesn't have enough data for failure scenarios.) In most cases, the trained agent in BC works well in success cases, but it fails when it encounters the irregular states.<br>\n",
    "In such cases, you can apply [DAgger](./02_dagger.ipynb) (next example), or the policy can be transferred to regular reinforcement learning after BC has been applied.\n",
    "\n",
    "Now let's start.\n",
    "\n",
    "*(back to [index](https://github.com/tsmatz/imitation-learning-tutorials/))*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62766662-86b8-4012-93b6-b5a36fbf273e",
   "metadata": {},
   "source": [
    "Before we start, we need to install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e277d08-839a-42a9-98ee-2a6607044036",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffda190f-7c7f-4836-8d4c-252e3c736d1e",
   "metadata": {},
   "source": [
    "## Restore environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020bea39-9434-4a4d-bc08-7df367d3d764",
   "metadata": {},
   "source": [
    "Firstly, I restore GridWorld environment from JSON file.\n",
    "\n",
    "For details about this environment, see [Readme.md](https://github.com/tsmatz/imitation-learning-tutorials/blob/master/Readme.md).\n",
    "\n",
    "> Note : See [this script](./00_generate_expert_trajectories.ipynb) for generating the same environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "211f0d0d-c49e-46b1-a827-cfbe1ebf04f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "GRID_SIZE = 50\n",
    "MAX_TIMESTEP = 200\n",
    "\n",
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    This environment is motivated by the following paper.\n",
    "    https://proceedings.mlr.press/v15/boularias11a/boularias11a.pdf\n",
    "\n",
    "    - It has 50 x 50 grids (cells).\n",
    "    - The agent has four actions for moving in one of the directions of the compass.\n",
    "    - If ```transition_prob``` = True, the actions succeed with probability 0.7,\n",
    "      a failure results in a uniform random transition to one of the adjacent states.\n",
    "    - A reward of 10 is given for reaching the goal state, located on the bottom-right corner.\n",
    "    - For the remaining states,\n",
    "      the reward function was randomly set to 0 with probability 2/3\n",
    "      and to âˆ’1 with probability 1/3.\n",
    "    - If the agent moves across the border, it's given the fail reward (i.e, reward=`-1`).\n",
    "    - The initial state is sampled from a uniform distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reward_map, valid_states, transition_prob=True):\n",
    "        \"\"\"\n",
    "        Initialize class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        reward_map : float[GRID_SIZE * GRID_SIZE]\n",
    "            Reward for each state.\n",
    "        valid_states : list(int[2])\n",
    "            List of states, in which the agent can reach to goal state without losing any rewards.\n",
    "            Each state is a 2d vector, [row, column].\n",
    "            When you call reset(), the initial state is picked up from these states.\n",
    "        transition_prob : bool\n",
    "            True if transition probability (above) is enabled.\n",
    "            False when we generate an expert agent without noise.\n",
    "        \"\"\"\n",
    "        self.reward_map = np.array(reward_map)\n",
    "        self.valid_states = np.array(valid_states)\n",
    "        self.transition_prob = transition_prob\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Randomly, get initial state (single state) from valid states.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        state : int\n",
    "            Return the picked-up state id.\n",
    "        \"\"\"\n",
    "        # initialize step count\n",
    "        self.step_count = 0\n",
    "        # pick up sample of valid states\n",
    "        state_2d = random.choice(self.valid_states)\n",
    "        # convert 2d index to 1d index\n",
    "        state_1d = state_2d[0] * GRID_SIZE + state_2d[1]\n",
    "        # return result\n",
    "        return state_1d\n",
    "\n",
    "    def step(self, action, state):\n",
    "        \"\"\"\n",
    "        Take action, proceed step, and return the result.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            Actions to take\n",
    "            (0=UP 1=DOWN 2=LEFT 3=RIGHT)\n",
    "        state : int\n",
    "            Current state id.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        new-state : int\n",
    "            New state id.\n",
    "        reward : int\n",
    "            The obtained reward.\n",
    "        done : bool\n",
    "            Flag to check whether it terminates.\n",
    "        \"\"\"\n",
    "        # if transition prob is enabled, apply transition\n",
    "        if self.transition_prob:\n",
    "            # the action succeeds with probability 0.7\n",
    "            prob = [.1]*4\n",
    "            prob[action] *= 7.0\n",
    "            action_onehot = np.random.multinomial(1, prob)\n",
    "        else:\n",
    "            action_onehot = np.zeros(4, dtype=int)\n",
    "            action_onehot[action] += 1\n",
    "        # get 2d state\n",
    "        mod, reminder = divmod(state, GRID_SIZE)\n",
    "        state_2d = np.array([mod, reminder])\n",
    "        # move state\n",
    "        # (0=UP 1=DOWN 2=LEFT 3=RIGHT)\n",
    "        up_and_down = action_onehot[1] - action_onehot[0]\n",
    "        left_and_right = action_onehot[3] - action_onehot[2]\n",
    "        new_state = state_2d + np.array([up_and_down, left_and_right])\n",
    "        # set reward\n",
    "        reward = 0.0\n",
    "        if (new_state[0] < 0) or (new_state[0] >= GRID_SIZE) or (new_state[1] < 0) or (new_state[1] >= GRID_SIZE):\n",
    "            # if location is out of border, set reward=-1\n",
    "            reward -= 1.0\n",
    "        else:\n",
    "            # if succeed, add reward of current state\n",
    "            state_1d = new_state[0] * GRID_SIZE + new_state[1]\n",
    "            reward += self.reward_map[state_1d]\n",
    "        # correct location\n",
    "        new_state = np.clip(new_state, 0, GRID_SIZE-1)\n",
    "        # return result\n",
    "        self.step_count += 1\n",
    "        return new_state[0] * GRID_SIZE + new_state[1], reward, (new_state[0]==GRID_SIZE-1 and new_state[1]==GRID_SIZE-1) or (self.step_count==MAX_TIMESTEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1da9931f-23e4-4ee7-a90c-41960bf431d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"gridworld.json\", \"r\") as f:\n",
    "    json_object = json.load(f)\n",
    "    env = GridWorld(**json_object, transition_prob=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010cb629-2b72-43da-8a13-ec2e79ce9d05",
   "metadata": {},
   "source": [
    "## Define policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c68de67-c4c7-445a-98ad-c48008785aea",
   "metadata": {},
   "source": [
    "Now I build a policy $\\pi_{\\theta}$.\n",
    "\n",
    "This network receives the current state (one-hot state) as input and returns the optimal action (action's logits) as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f3fb3dd-3dc0-4411-94bf-e1c7a0df826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#\n",
    "# Define model\n",
    "#\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(GRID_SIZE*GRID_SIZE, hidden_dim)\n",
    "        self.classify = nn.Linear(hidden_dim, 4)\n",
    "\n",
    "    def forward(self, s):\n",
    "        outs = self.hidden(s)\n",
    "        outs = F.relu(outs)\n",
    "        logits = self.classify(outs)\n",
    "        return logits\n",
    "\n",
    "#\n",
    "# Generate model\n",
    "#\n",
    "policy_func = PolicyNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cf5978-a292-47aa-9790-424560a93145",
   "metadata": {},
   "source": [
    "## Run agent before training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cf48e8-af1b-4600-811f-0afd401b43ed",
   "metadata": {},
   "source": [
    "For comparison, now I run this agent without any training.\n",
    "\n",
    "In this game, the maximum episode's reward without losing any rewards is ```10.0```. (See [Readme.md](https://github.com/tsmatz/imitation-learning-tutorials/blob/master/Readme.md) for game rule in this environment.)<br>\n",
    "As you can see below, it has low average of rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da7720a0-cd39-4b3e-ab8a-4bb3932f4ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature vector (which shape is (GRID_SIZE*GRID_SIZE,)) of state to feed model\n",
    "def get_feature(state):\n",
    "    \"\"\"\n",
    "    Return one-hot feature array from 2d states (as a batch).\n",
    "    e.g, [0,3] --> [0, 0, 0, 1, 0, ... ]\n",
    "    \"\"\"\n",
    "    # get one-hot array --> size (batch_size, GRID_SIZE * GRID_SIZE)\n",
    "    return F.one_hot(torch.tensor(state).to(device), num_classes=GRID_SIZE*GRID_SIZE)\n",
    "\n",
    "# Pick stochastic samples with policy model\n",
    "def pick_sample_and_logits(policy, s):\n",
    "    \"\"\"\n",
    "    Stochastically pick up action and logits with policy model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    policy : torch.nn.Module\n",
    "        Policy network to use\n",
    "    s : tensor of int[GRID_SIZE*GRID_SIZE])\n",
    "        The feature (one-hot) of state.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    action : tensor of int\n",
    "        The picked-up actions.\n",
    "    logits : tensor of int[4]\n",
    "        Logits defining categorical distribution.\n",
    "        This is needed to optimize model.\n",
    "    \"\"\"\n",
    "    # Get logits from state\n",
    "    # --> size : (4,)\n",
    "    inputs = s.unsqueeze(dim=0)\n",
    "    logits = policy(inputs.float())\n",
    "    logits = logits.squeeze(dim=0)\n",
    "    # From logits to probabilities\n",
    "    # --> size : (4,)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    # Pick up action's sample\n",
    "    # --> size : (1,)\n",
    "    a = torch.multinomial(probs, num_samples=1)\n",
    "    # --> size : ()\n",
    "    a = a.squeeze()\n",
    "\n",
    "    # Return\n",
    "    return a, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1456ab6-17f0-4921-9c9b-93674a7f2b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed  300 /  300 episodes ...\n",
      "Done\n",
      "Average reward is -66.23.\n"
     ]
    }
   ],
   "source": [
    "def evaluate_policy(policy, eval_num, verbose=False):\n",
    "    score_list = []\n",
    "    for i in range(eval_num):\n",
    "        score = 0\n",
    "        done = False\n",
    "        s = env.reset()\n",
    "        while not done:\n",
    "            s_onehot = get_feature(s)\n",
    "            with torch.no_grad():\n",
    "                a, _ = pick_sample_and_logits(policy, s_onehot)\n",
    "            s, r, done = env.step(a, s)\n",
    "            score += r\n",
    "        score_list.append(score)\n",
    "        if verbose:\n",
    "            print(\"Processed {:4d} / {:4d} episodes ...\".format(i + 1, eval_num), end=\"\\r\")\n",
    "    if verbose:\n",
    "        print(\"\\nDone\")\n",
    "    return sum(score_list) / len(score_list)\n",
    "\n",
    "avg = evaluate_policy(policy_func, 300, verbose=True)\n",
    "print(\"Average reward is {}.\".format(avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d0eff9-1759-4ea3-bba0-65f3b509f683",
   "metadata": {},
   "source": [
    "## Train policy\n",
    "\n",
    "Now we train our policy with expert data.\n",
    "\n",
    "> Note : The expert data is located in ```./expert_data``` folder in this repository. See [this script](./00_generate_expert_trajectories.ipynb) for generating expert dataset.\n",
    "\n",
    "In this training, I compute cross-entropy loss for categorical distribution and then optimize the policy with only expert dataset.<br>\n",
    "Unlike [reinforcement learning](https://github.com/tsmatz/reinforcement-learning-tutorials), the reward is unknown in this training.\n",
    "\n",
    "As you can see below, the average reward becomes high, and the policy is well-trained. (See [Readme.md](https://github.com/tsmatz/imitation-learning-tutorials/blob/master/Readme.md) for game rule in this environment.)\n",
    "\n",
    "> Note : You can run as a batch to speed up training, but here I run each inference one by one in order to simplify (make readable) our code. (Because the training is very simple.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "088aee9f-0f53-40e2-b5c9-2ac7d81d60f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed  1000 episodes in checkpoint ckpt0.pkl...\n",
      "Evaluation result (Average reward): 3.13\n",
      "Processed  2000 episodes in checkpoint ckpt0.pkl...\n",
      "Evaluation result (Average reward): 7.215\n",
      "Processed  3000 episodes in checkpoint ckpt0.pkl...\n",
      "Evaluation result (Average reward): 7.58\n",
      "Processed  4000 episodes in checkpoint ckpt0.pkl...\n",
      "Evaluation result (Average reward): 8.285\n",
      "Processed  5000 episodes in checkpoint ckpt0.pkl...\n",
      "Evaluation result (Average reward): 8.225\n",
      "Processed  6000 episodes in checkpoint ckpt0.pkl...\n",
      "Evaluation result (Average reward): 8.575\n",
      "Processed  7000 episodes in checkpoint ckpt0.pkl...\n",
      "Evaluation result (Average reward): 8.485\n",
      "Processed  8000 episodes in checkpoint ckpt0.pkl...\n",
      "Evaluation result (Average reward): 8.715\n",
      "Processed  9000 episodes in checkpoint ckpt0.pkl...\n",
      "Evaluation result (Average reward): 8.63\n",
      "Processed 10000 episodes in checkpoint ckpt0.pkl...\n",
      "Evaluation result (Average reward): 8.705\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# use the following expert dataset\n",
    "dest_dir = \"./expert_data\"\n",
    "checkpoint_files = [\"ckpt0.pkl\"]\n",
    "\n",
    "# create optimizer\n",
    "opt = torch.optim.AdamW(policy_func.parameters(), lr=0.001)\n",
    "\n",
    "for ckpt in checkpoint_files:\n",
    "    # load expert data from pickle\n",
    "    with open(f\"{dest_dir}/{ckpt}\", \"rb\") as f:\n",
    "        all_data = pickle.load(f)\n",
    "    all_states = all_data[\"states\"]\n",
    "    all_actions = all_data[\"actions\"]\n",
    "    timestep_lens = all_data[\"timestep_lens\"]\n",
    "    # loop all episodes in demonstration\n",
    "    current_timestep = 0\n",
    "    for i, timestep_len in enumerate(timestep_lens):\n",
    "        # pick up states and actions in a single episode\n",
    "        states = all_states[current_timestep:current_timestep+timestep_len]\n",
    "        actions = all_actions[current_timestep:current_timestep+timestep_len]\n",
    "        # collect loss and optimize (train)\n",
    "        opt.zero_grad()\n",
    "        loss = []\n",
    "        for s, a in zip(states, actions):\n",
    "            s_onehot = get_feature(s)\n",
    "            _, logits = pick_sample_and_logits(policy_func, s_onehot)\n",
    "            l = F.cross_entropy(logits, torch.tensor(a).to(device), reduction=\"none\")\n",
    "            loss.append(l)\n",
    "        total_loss = torch.stack(loss, dim=0)\n",
    "        total_loss.sum().backward()\n",
    "        opt.step()\n",
    "        # log\n",
    "        print(\"Processed {:5d} episodes in checkpoint {}...\".format(i + 1, ckpt), end=\"\\r\")\n",
    "        # run evaluation in each 1000 episodes\n",
    "        if i % 1000 == 999:\n",
    "            avg = evaluate_policy(policy_func, 200)\n",
    "            print(f\"\\nEvaluation result (Average reward): {avg}\")\n",
    "        # proceed to next episode\n",
    "        current_timestep += timestep_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2a9ea4-6fca-4934-b2a7-8c7f8e4af359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
