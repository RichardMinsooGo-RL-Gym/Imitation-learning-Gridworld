{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "21c48c2d-e7f0-4105-9f64-0b4daeb032f0",
      "metadata": {
        "id": "21c48c2d-e7f0-4105-9f64-0b4daeb032f0"
      },
      "source": [
        "# [Optional] Generate expert trajectories\n",
        "\n",
        "**Note : This repository already includes expert dataset and you don't then need to run this script.** (This script is for your customization.)\n",
        "\n",
        "Before running algorithms in imitaiton learning, we need expert's trajectories, which are used to learn behaviors or to recover rewards.<br>\n",
        "In this example, we generate optimal policy by applying reinforcement learning method (here I use PPO algorithm), and generate expert's trajectories with this trained agent.\n",
        "\n",
        "It's also worth noting that we use reward's function to get the optimal agent in this notebook, but reward's function can never be used for optimization in all imitation learning exercises in this repository.<br>\n",
        "That's why the methods in imitation learning matters.\n",
        "\n",
        "See [here](https://github.com/tsmatz/reinforcement-learning-tutorials) for theoretical background behind reinforcement learning algorithms. (Explanation of RL is out of scope in this repository.)\n",
        "\n",
        "*(back to [index](https://github.com/tsmatz/imitation-learning-tutorials/))*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a3619b8-6fdf-43c1-9398-06e486a0f187",
      "metadata": {
        "id": "7a3619b8-6fdf-43c1-9398-06e486a0f187"
      },
      "source": [
        "Before we start, we need to install the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "31abf851-de04-461b-b999-e2acfd3578ef",
      "metadata": {
        "id": "31abf851-de04-461b-b999-e2acfd3578ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2f70e31-c7cc-4ad6-f152-c82ad3bbf484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch numpy matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2fab8d1-cf21-49ca-bd18-a6d2194d2c3f",
      "metadata": {
        "id": "f2fab8d1-cf21-49ca-bd18-a6d2194d2c3f"
      },
      "source": [
        "## 1. Define GridWorld environment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4a4ced2-1e30-49f0-8677-9c208d6a3116",
      "metadata": {
        "id": "e4a4ced2-1e30-49f0-8677-9c208d6a3116"
      },
      "source": [
        "First we define GridWorld environment to be used in all exercises, and save as Python script file, ```gridworld.py```.<br>\n",
        "For details about this environment, see [Readme.md](https://github.com/tsmatz/imitation-learning-tutorials/blob/master/Readme.md).\n",
        "\n",
        "**Note : This repository already has ```gridworld.py``` and you don't then need to run this cell.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4b426d17-91e0-4d58-88b6-9945d6d1bfcf",
      "metadata": {
        "id": "4b426d17-91e0-4d58-88b6-9945d6d1bfcf",
        "outputId": "b2c1cd64-ac8f-42ac-ff36-b4ac73114668",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing gridworld.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile gridworld.py\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class GridWorld:\n",
        "    \"\"\"\n",
        "    This environment is motivated by the following paper.\n",
        "    https://proceedings.mlr.press/v15/boularias11a/boularias11a.pdf\n",
        "\n",
        "    - It has 50 x 50 grids (cells).\n",
        "    - The agent has four actions for moving in one of the directions of the compass.\n",
        "    - [Optional] If ```transition_prob``` = True, the actions succeed with probability 0.7,\n",
        "      a failure results in a uniform random transition to one of the adjacent states.\n",
        "    - A reward of 10 is given for reaching the goal state, located on the bottom-right corner.\n",
        "    - For the remaining states,\n",
        "      the reward function was randomly set to 0 with probability 2/3\n",
        "      and to âˆ’1 with probability 1/3.\n",
        "    - If the agent moves across the border, it's given the fail reward (i.e, reward=`-1`).\n",
        "    - The initial state is sampled from a uniform distribution.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, reward_map=None, valid_states=None, seed=None, transition_prob=False, max_timestep=200, device=\"cuda\"):\n",
        "        \"\"\"\n",
        "        Initialize class.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        reward_map : float[grid_size * grid_size]\n",
        "            Reward for each state.\n",
        "            Set this value when you load existing world definition (when seed=None).\n",
        "        valid_states : list(int[2])\n",
        "            List of states, in which the agent can reach to goal state without losing any rewards.\n",
        "            Each state is a 2d vector, [row, column].\n",
        "            When you call reset(), the initial state is picked up from these states.\n",
        "            Set this value when you load existing world definition (when seed=None).\n",
        "        seed : int\n",
        "            Seed value to generate new grid (maze).\n",
        "            Set this value when you create a new world.\n",
        "            (Above ```reward_map``` and ```valid_states``` are newly generated.)\n",
        "        transition_prob : bool\n",
        "            True if transition probability (above) is enabled.\n",
        "            False when we generate an expert agent without noise.\n",
        "            (If transition_prob=True, it only returns next states in step() function.)\n",
        "        max_timestep : int\n",
        "            The maximum number of time-step (horizon).\n",
        "            When it doesn't have finite horizon, set None as max_timestep.\n",
        "            (If max_timestep=None, it doesn't return trunc flag in step() function.)\n",
        "        device : string\n",
        "            Device info (\"cuda\", \"cpu\", etc).\n",
        "        \"\"\"\n",
        "\n",
        "        self.device = device\n",
        "        self.transition_prob = transition_prob\n",
        "        self.grid_size = 50\n",
        "        self.action_size = 4\n",
        "        self.max_timestep = max_timestep\n",
        "        self.goal_reward = 10\n",
        "\n",
        "        if seed is None:\n",
        "            ############################\n",
        "            ### Load from definition ###\n",
        "            ############################\n",
        "            self.reward_map = torch.tensor(reward_map).to(self.device)\n",
        "            self.valid_states = torch.tensor(valid_states).to(self.device)\n",
        "        else:\n",
        "            ################################\n",
        "            ### Generate a new GridWorld ###\n",
        "            ################################\n",
        "            # generate grid\n",
        "            self.reward_map = torch.zeros(self.grid_size * self.grid_size, dtype=torch.int).to(self.device)\n",
        "            # bottom-right is goal state\n",
        "            self.reward_map[-1] = self.goal_reward\n",
        "            # set reward=âˆ’1 with probability 1/3\n",
        "            sample_n = np.floor((self.grid_size * self.grid_size - 1) / 3).astype(int)\n",
        "            rng = np.random.default_rng(seed)\n",
        "            sample_loc = rng.choice(self.grid_size * self.grid_size - 1, size=sample_n, replace=False)\n",
        "            sample_loc = torch.from_numpy(sample_loc).to(self.device)\n",
        "            self.reward_map[sample_loc] = -1\n",
        "            # seek valid states\n",
        "            valid_states_list = self._greedy_seek_valid_states([self.grid_size-1, self.grid_size-1], [])\n",
        "            valid_states_list.remove([self.grid_size-1, self.grid_size-1])\n",
        "            self.valid_states = torch.tensor(valid_states_list).to(self.device)\n",
        "\n",
        "    def _greedy_seek_valid_states(self, state, old_state_list):\n",
        "        \"\"\"\n",
        "        This method recursively seeks valid state.\n",
        "        e.g, if some state is surrounded by the states with reward=-1,\n",
        "        this state is invalid, because it cannot reach to the goal state\n",
        "        without losing rewards.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state : int[2]\n",
        "            State to start seeking. It then seeks this state and all child's states.\n",
        "            This state must be the list of [row, column].\n",
        "        old_state_list : int[N, 2]\n",
        "            List of states already checked.\n",
        "            Each state must be the list of [row, column].\n",
        "            These items are then skipped for seeking.\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        valid_states : int[N, 2]\n",
        "            List of new valid states.\n",
        "            Each state must be the list of [row, column].\n",
        "        \"\"\"\n",
        "        # build new list\n",
        "        new_state_list = []\n",
        "        # if the state is already included in the list, do nothing\n",
        "        if state in old_state_list:\n",
        "            return new_state_list\n",
        "        # if the state has reward=-1, do nothing\n",
        "        if self.reward_map[state[0]*self.grid_size+state[1]] == -1:\n",
        "            return new_state_list\n",
        "        # else add the state into the list\n",
        "        new_state_list.append(state)\n",
        "        # move up\n",
        "        if state[0] > 0:\n",
        "            next_state = list(map(lambda i, j: i + j, state, [-1, 0]))\n",
        "            new_state_list += self._greedy_seek_valid_states(\n",
        "                next_state,\n",
        "                old_state_list + new_state_list)\n",
        "        # move down\n",
        "        if state[0] < self.grid_size - 1:\n",
        "            next_state = list(map(lambda i, j: i + j, state, [1, 0]))\n",
        "            new_state_list += self._greedy_seek_valid_states(\n",
        "                next_state,\n",
        "                old_state_list + new_state_list)\n",
        "        # move left\n",
        "        if state[1] > 0:\n",
        "            next_state = list(map(lambda i, j: i + j, state, [0, -1]))\n",
        "            new_state_list += self._greedy_seek_valid_states(\n",
        "                next_state,\n",
        "                old_state_list + new_state_list)\n",
        "        # move right\n",
        "        if state[1] < self.grid_size - 1:\n",
        "            next_state = list(map(lambda i, j: i + j, state, [0, 1]))\n",
        "            new_state_list += self._greedy_seek_valid_states(\n",
        "                next_state,\n",
        "                old_state_list + new_state_list)\n",
        "        # return result\n",
        "        return new_state_list\n",
        "\n",
        "    def reset(self, batch_size):\n",
        "        \"\"\"\n",
        "        Randomly, get initial state (single state) from valid states.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size : int\n",
        "            The number of returned states.\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        state : torch.tensor((batch_size), dtype=int)\n",
        "            Return the picked-up state id.\n",
        "        \"\"\"\n",
        "        # initialize step count\n",
        "        self.step_count = 0\n",
        "        # pick up sample of valid states\n",
        "        indices = torch.multinomial(torch.ones(len(self.valid_states)).to(self.device), batch_size, replacement=True)\n",
        "        state_2d = self.valid_states[indices]\n",
        "        # convert 2d index to 1d index\n",
        "        state_1d = state_2d[:,0] * self.grid_size + state_2d[:,1]\n",
        "        # return result\n",
        "        return state_1d\n",
        "\n",
        "    def step(self, actions, states, trans_state_only=False, transition_prob=None):\n",
        "        \"\"\"\n",
        "        Take action, proceed step, and return the result.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        actions : torch.tensor((batch_size), dtype=int)\n",
        "            Actions to take\n",
        "            (0=UP 1=DOWN 2=LEFT 3=RIGHT)\n",
        "        states : torch.tensor((batch_size), dtype=int)\n",
        "            Current state id.\n",
        "        trans_state_only : bool\n",
        "            Set TRUE, when you call only for getting next state by stateless without reset()\n",
        "            (If transition_prob=True, it only returns next states in step() function.)\n",
        "        transition_prob : bool\n",
        "            Set this property, if you overrite default ```transition_prob``` property.\n",
        "            (For this property, see above in __init__() method.)\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        new-states : torch.tensor((batch_size), dtype=int)\n",
        "            New state id.\n",
        "        rewards : torch.tensor((batch_size), dtype=float)\n",
        "            The obtained reward.\n",
        "        term : torch.tensor((batch_size), dtype=bool)\n",
        "            Flag to check whether it reaches to the goal and terminates.\n",
        "        trunc : torch.tensor((batch_size), dtype=bool)\n",
        "            Flag to check whether it's truncated by reaching to max time-step.\n",
        "            (When max_timestep is None, this is not returned.)\n",
        "        \"\"\"\n",
        "        # get batch size\n",
        "        batch_size = actions.shape[0]\n",
        "        # if transition prob is enabled, apply stochastic transition\n",
        "        if transition_prob is None:\n",
        "            trans_prob = self.transition_prob # set default\n",
        "        else:\n",
        "            trans_prob = transition_prob      # overrite\n",
        "        if trans_prob:\n",
        "            # the action succeeds with probability 0.7\n",
        "            prob = torch.ones(batch_size, self.action_size).to(self.device)\n",
        "            mask = F.one_hot(actions, num_classes=self.action_size).bool()\n",
        "            prob = torch.where(mask, 7.0, prob)\n",
        "            selected_actions = torch.multinomial(prob, 1, replacement=True)\n",
        "            selected_actions = selected_actions.squeeze(dim=1)\n",
        "            action_onehot = F.one_hot(selected_actions, num_classes=self.action_size)\n",
        "        else:\n",
        "            # deterministic (probability=1.0 in one state)\n",
        "            action_onehot = F.one_hot(actions, num_classes=self.action_size)\n",
        "        # get 2d state\n",
        "        mod = torch.div(states, self.grid_size, rounding_mode=\"floor\")\n",
        "        reminder = torch.remainder(states, self.grid_size)\n",
        "        state_2d = torch.cat((mod.unsqueeze(dim=-1), reminder.unsqueeze(dim=-1)), dim=-1)\n",
        "        # move state\n",
        "        # (0=UP 1=DOWN 2=LEFT 3=RIGHT)\n",
        "        up_and_down = action_onehot[:,1] - action_onehot[:,0]\n",
        "        left_and_right = action_onehot[:,3] - action_onehot[:,2]\n",
        "        move = torch.cat((up_and_down.unsqueeze(dim=-1), left_and_right.unsqueeze(dim=-1)), dim=-1)\n",
        "        new_states = state_2d + move\n",
        "        # set reward\n",
        "        if not(trans_state_only):\n",
        "            rewards = torch.zeros(batch_size).to(self.device)\n",
        "            rewards = torch.where(new_states[:,0] < 0, -1.0, rewards)\n",
        "            rewards = torch.where(new_states[:,0] >= self.grid_size, -1.0, rewards)\n",
        "            rewards = torch.where(new_states[:,1] < 0, -1.0, rewards)\n",
        "            rewards = torch.where(new_states[:,1] >= self.grid_size, -1.0, rewards)\n",
        "        # correct location\n",
        "        new_states = torch.clip(new_states, min=0, max=self.grid_size-1)\n",
        "        # if succeed, add reward of current state\n",
        "        states_1d = new_states[:,0] * self.grid_size + new_states[:,1]\n",
        "        if not(trans_state_only):\n",
        "            rewards = torch.where(rewards>=0.0, rewards+self.reward_map[states_1d], rewards)\n",
        "            self.step_count += 1\n",
        "        # return result\n",
        "        if trans_state_only:\n",
        "            return states_1d\n",
        "        elif self.max_timestep is None:\n",
        "            return states_1d, rewards, rewards==self.reward_map[self.grid_size * self.grid_size - 1]\n",
        "        else:\n",
        "            return states_1d, rewards, rewards==self.reward_map[self.grid_size * self.grid_size - 1], torch.tensor(self.step_count==self.max_timestep).to(self.device).unsqueeze(dim=0).expand(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a31d89c1-ad1d-46bc-9136-f74f0e7edeeb",
      "metadata": {
        "id": "a31d89c1-ad1d-46bc-9136-f74f0e7edeeb"
      },
      "source": [
        "Create an environment with a fixed seed, ```1000```.<br>\n",
        "I note that transition probability is disabled in this environment (i.e, ```transition_prob=False``` in constructor)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8c189a9f-3af9-4f24-9cc7-10a9dc565da6",
      "metadata": {
        "id": "8c189a9f-3af9-4f24-9cc7-10a9dc565da6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from gridworld import GridWorld\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "env = GridWorld(seed=1000, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f038a233-3c90-4426-91c8-16d90b494791",
      "metadata": {
        "id": "f038a233-3c90-4426-91c8-16d90b494791"
      },
      "source": [
        "Now I visualize our GridWorld environment.\n",
        "\n",
        "The number in each cell indicates the reward score on this state.<br>\n",
        "The goal state is on the right-bottom corner (in which the reward is ```10.0```), and the initial state is uniformly picked up from the gray-colored cells.<br>\n",
        "If the agent can reach to goal state without losing any rewards, it will get ```10.0``` for total reward.\n",
        "\n",
        "See [Readme.md](https://github.com/tsmatz/imitation-learning-tutorials/blob/master/Readme.md) for details about the game rule of this environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0ecb5950-ea91-448c-ac77-735fca21ffdd",
      "metadata": {
        "id": "0ecb5950-ea91-448c-ac77-735fca21ffdd",
        "outputId": "239ce540-3153-4940-9403-15b79cb94b41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table><tr><td>0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>-1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>-1</td><td>-1</td><td>0</td><td>0</td><td>-1</td><td>-1</td></tr><tr><td>0</td><td>0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>0</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-1</td></tr><tr><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>0</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td>0</td><td>0</td></tr><tr><td>0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-1</td></tr><tr><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>-1</td><td>0</td><td>-1</td><td>0</td><td>-1</td><td>-1</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>0</td><td>-1</td><td>0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td>-1</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>0</td><td>0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td></tr><tr><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td>0</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td></tr><tr><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td></tr><tr><td>0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>0</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td></tr><tr><td>0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td></tr><tr><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td></tr><tr><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>0</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td></tr><tr><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td>0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td></tr><tr><td>0</td><td>0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>0</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td>0</td><td>0</td><td>-1</td><td>-1</td><td>0</td><td>-1</td><td>0</td><td>0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td></tr><tr><td>-1</td><td>0</td><td>0</td><td>0</td><td>-1</td><td>0</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td>-1</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">0</td><td bgcolor=\"gray\">0</td><td>-1</td><td bgcolor=\"gray\">10</td></tr></table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "valid_states_all = torch.cat((env.valid_states, torch.tensor([env.grid_size-1,env.grid_size-1]).to(device).unsqueeze(dim=0)))\n",
        "valid_states_all = valid_states_all[:,0] * env.grid_size + valid_states_all[:,1]\n",
        "\n",
        "html_text = \"<table>\"\n",
        "for row in range(env.grid_size):\n",
        "    html_text += \"<tr>\"\n",
        "    for col in range(env.grid_size):\n",
        "        if row*env.grid_size + col in valid_states_all:\n",
        "            html_text += \"<td bgcolor=\\\"gray\\\">\"\n",
        "        else:\n",
        "            html_text += \"<td>\"\n",
        "        html_text += str(env.reward_map[row*env.grid_size+col].tolist())\n",
        "        html_text += \"</td>\"\n",
        "    html_text += \"</tr>\"\n",
        "html_text += \"</table>\"\n",
        "\n",
        "display(HTML(html_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6696c9b7-5eaa-4090-9c87-bda071bb333c",
      "metadata": {
        "id": "6696c9b7-5eaa-4090-9c87-bda071bb333c"
      },
      "source": [
        "Save (serialize) this environment as JSON in order to recover the same environment in the following examples.\n",
        "\n",
        "**Note : This repository already has this JSON file (```gridworld.json```) and you don't then need to run this cell.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f74b1631-06f9-42b6-b034-bd3ef71470f2",
      "metadata": {
        "id": "f74b1631-06f9-42b6-b034-bd3ef71470f2"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def serialize_gridworld(env):\n",
        "    env_dict = env.__dict__\n",
        "    return {\n",
        "        \"reward_map\": env_dict[\"reward_map\"].tolist(),\n",
        "        \"valid_states\": env_dict[\"valid_states\"].tolist()\n",
        "    }\n",
        "\n",
        "with open(\"gridworld.json\", \"w\") as f:\n",
        "    json_str = json.dumps(env, default=serialize_gridworld)\n",
        "    f.write(json_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05930441-1729-4f40-bc04-01cf958f61dc",
      "metadata": {
        "id": "05930441-1729-4f40-bc04-01cf958f61dc"
      },
      "source": [
        "[Optional] If you load the GridWorld environment with the existing JSON file (```gridworld.json```), please run as follows.<br>\n",
        "(In the following exercises, it runs as follows to load an environment.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6fa81b92-a3a1-4338-bc63-ebf4ae0b42f3",
      "metadata": {
        "id": "6fa81b92-a3a1-4338-bc63-ebf4ae0b42f3"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"gridworld.json\", \"r\") as f:\n",
        "    json_object = json.load(f)\n",
        "    env = GridWorld(**json_object, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78cb7b2e-07b9-4bed-9e8b-410df523bc29",
      "metadata": {
        "id": "78cb7b2e-07b9-4bed-9e8b-410df523bc29"
      },
      "source": [
        "## 2. Define models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54d72704-f1e1-40b4-9411-adf3bbf3cd10",
      "metadata": {
        "id": "54d72704-f1e1-40b4-9411-adf3bbf3cd10"
      },
      "source": [
        "Now we define and instantiate the expert model (neural network) by using state-of-the-art reinforcement learning algorithm, PPO (Proximal Policy Optimization).<br>\n",
        "Here I don't explain the algorithm about PPO, but please refer [reinforcement learning tutorials](https://github.com/tsmatz/reinforcement-learning-tutorials) for details.\n",
        "\n",
        "In PPO, it uses 2 models - actor model and value model.<br>\n",
        "Firstly, I then define these 2 models as follows. (Later these models will be trained.)\n",
        "\n",
        "> Note : GridWorld is a primitive environment and I assume that both the action (logits) and value is linear to the feature of state, i.e, $\\verb|estimated value| = \\mathbf{w}^T \\cdot \\phi(s) $, where $\\phi(s)$ is the feature of state $s$.<br>\n",
        "> Thus, both networks have no hidden layers (see below) to speed up the training. (Depending on promblems, you should design other networks.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "54704310-1ba9-4a5a-8344-074151874f54",
      "metadata": {
        "id": "54704310-1ba9-4a5a-8344-074151874f54"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "STATE_SIZE = env.grid_size * env.grid_size  # 2500\n",
        "ACTION_SIZE = env.action_size               # 4\n",
        "\n",
        "class ActorNet(nn.Module):\n",
        "    def __init__(self, hidden_dim=16):\n",
        "        super().__init__()\n",
        "        self.output = nn.Linear(STATE_SIZE, ACTION_SIZE, bias=False)\n",
        "\n",
        "    def forward(self, state, mask=None):\n",
        "        \"\"\"\n",
        "        Set mask (size (TIMESTEP_SIZE, BATCH_SIZE)), when the number of timestep differs.\n",
        "        \"\"\"\n",
        "        logits = self.output(state)\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(dim=-1)\n",
        "            mask = mask.expand(-1, -1, ACTION_SIZE)\n",
        "            logits = logits.masked_fill(mask, 0.0)\n",
        "        return logits\n",
        "\n",
        "class ValueNet(nn.Module):\n",
        "    def __init__(self, hidden_dim=16):\n",
        "        super().__init__()\n",
        "        self.output = nn.Linear(STATE_SIZE, 1, bias=False)\n",
        "\n",
        "    def forward(self, state, mask=None):\n",
        "        \"\"\"\n",
        "        Set mask (size (TIMESTEP_SIZE, BATCH_SIZE)), when the number of timestep differs.\n",
        "        \"\"\"\n",
        "        value = self.output(state)\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(dim=-1)\n",
        "            value = value.masked_fill(mask, 0.0)\n",
        "        return value\n",
        "\n",
        "#\n",
        "# Generate model\n",
        "#\n",
        "actor_func = ActorNet().to(device)\n",
        "value_func = ValueNet().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d13042ca-abcb-4a0d-91cc-fedd04487f31",
      "metadata": {
        "id": "d13042ca-abcb-4a0d-91cc-fedd04487f31"
      },
      "source": [
        "I also define the helper functions as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f82dc674-fa2d-4b03-ba50-6d8a0fe4df1b",
      "metadata": {
        "id": "f82dc674-fa2d-4b03-ba50-6d8a0fe4df1b"
      },
      "outputs": [],
      "source": [
        "# Pick up stochastic samples using previous actor model\n",
        "def pick_sample_and_logp(policy, s):\n",
        "    \"\"\"\n",
        "    Stochastically pick up action and logits with policy model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    policy : torch.nn.Module\n",
        "        Policy network to use\n",
        "    s : torch.tensor((BATCH_SIZE, STATE_SIZE), dtype=int)\n",
        "        The feature (one-hot) of state.\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    action : torch.tensor((BATCH_SIZE), dtype=int)\n",
        "        The picked-up actions.\n",
        "    logits : torch.tensor((BATCH_SIZE, ACTION_SIZE), dtype=float)\n",
        "        Logits defining categorical distribution.\n",
        "        This is needed to optimize model.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        # Get logits from state\n",
        "        # --> size : (BATCH_SIZE, ACTION_SIZE)\n",
        "        logits = policy(s.float())\n",
        "        # From logits to probabilities\n",
        "        # --> size : (BATCH_SIZE, ACTION_SIZE)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # Pick up action's sample\n",
        "        # --> size : (BATCH_SIZE, 1)\n",
        "        a = torch.multinomial(probs, num_samples=1)\n",
        "        # --> size : (BATCH_SIZE, )\n",
        "        a = a.squeeze(dim=1)\n",
        "        # Calculate log probability\n",
        "        logprb = -F.cross_entropy(logits, a, reduction=\"none\")\n",
        "\n",
        "        # Return\n",
        "        return a, logits, logprb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ef2b342-df27-4123-9376-8ceebe18c387",
      "metadata": {
        "id": "8ef2b342-df27-4123-9376-8ceebe18c387"
      },
      "source": [
        "## 3. Train and generate expert model\n",
        "\n",
        "**Note : This repository already has trained models (```expert_actor.pt```, ```expert_value.pt```) and you don't then need to run this cell.**\n",
        "\n",
        "Now let's train models with PPO algorithm.<br>\n",
        "\n",
        "In this notebook, I have just used source code in [reinforcement learning tutorials](https://github.com/tsmatz/reinforcement-learning-tutorials) without any explanation, but please refer the original notebook for theoretical background behind PPO.\n",
        "\n",
        "In this training,\n",
        "\n",
        "- To speed up training, it runs training as a batch.\n",
        "- The environment doesn't have transition probability and the optimal total reward in a single episode always becomes ```10.0```. (If it's optimized, it'll be close to ```10.0```.)\n",
        "- The goal for the agent is to reach the goal state without losing rewards. Thus, I have set no discount rate (```DISCOUNT = 1.0```) in this training.\n",
        "\n",
        "> Here I set ```-100``` as unknown action, because PyTorch cross-entropy function (```torch.nn.functional.cross_entropy()```) has a property ```ignore_index``` which default value is ```-100```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f60cbbc3-b2df-471b-9d97-81ad938d5596",
      "metadata": {
        "id": "f60cbbc3-b2df-471b-9d97-81ad938d5596",
        "outputId": "750d6f29-7bba-47ee-aaff-118319bde53c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "source": [
        "#\n",
        "# Train model\n",
        "# (see https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/04-ppo.ipynb)\n",
        "#\n",
        "# Operations are processed as a batch to speed up,\n",
        "# and all working tensor has dimension: (step_count, batch_size, ...)\n",
        "#\n",
        "import numpy as np\n",
        "\n",
        "DISCOUNT = 1.0            # No Discount\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# [TODO] change this value if transition_prob=TRUE\n",
        "THRESHOLD = 9.0  # if transition_prob=FALSE\n",
        "# THRESHOLD = -0.95  # if transition_prob=TRUE\n",
        "\n",
        "# These coefficients are experimentally determined in practice.\n",
        "kl_coeff = 100000.0  # weight coefficient for KL-divergence loss\n",
        "vf_coeff = 100.00  # weight coefficient for value loss\n",
        "\n",
        "reward_records = []\n",
        "all_params = list(actor_func.parameters()) + list(value_func.parameters())\n",
        "opt = torch.optim.AdamW(all_params, lr=LEARNING_RATE)\n",
        "for i in range(999999):\n",
        "\n",
        "    # print(\"Epochs :\", i+1)\n",
        "\n",
        "    #\n",
        "    # Run episode till done as a batch to generate tensors\n",
        "    #\n",
        "    done = torch.tensor([False]).to(device)\n",
        "    # define working items\n",
        "    # (tensor shape is (TIMESTEP_SIZE, BATCH_SIZE) or (TIMESTEP_SIZE, BATCH_SIZE, C) where C is the number of classes)\n",
        "    states_work  = torch.empty((0,BATCH_SIZE,STATE_SIZE), dtype=torch.int).to(device)\n",
        "    actions_work = torch.empty((0,BATCH_SIZE), dtype=torch.int).to(device)\n",
        "    logits_work  = torch.empty((0,BATCH_SIZE,ACTION_SIZE), dtype=torch.float).to(device)\n",
        "    logprbs_work = torch.empty((0,BATCH_SIZE), dtype=torch.float).to(device)\n",
        "    rewards_work = torch.empty((0,BATCH_SIZE), dtype=torch.float).to(device)\n",
        "\n",
        "    # define done items\n",
        "    states_done  = []\n",
        "    actions_done = []\n",
        "    logits_done  = []\n",
        "    logprbs_done = []\n",
        "    rewards_done = []\n",
        "\n",
        "    # start\n",
        "    s = env.reset(BATCH_SIZE)\n",
        "    while not (torch.prod(done) == 1):\n",
        "\n",
        "        s_onehot = F.one_hot(s, num_classes=STATE_SIZE)\n",
        "        states_work = torch.cat((states_work, s_onehot.unsqueeze(dim=0)), dim=0)\n",
        "        a, l, p = pick_sample_and_logp(actor_func, s_onehot)\n",
        "        s, r, term, trunc = env.step(a, s)\n",
        "        done = torch.logical_or(term, trunc)\n",
        "\n",
        "        actions_work = torch.cat((actions_work, a.unsqueeze(dim=0)), dim=0)\n",
        "        logits_work  = torch.cat((logits_work,  l.unsqueeze(dim=0)), dim=0)\n",
        "        logprbs_work = torch.cat((logprbs_work, p.unsqueeze(dim=0)), dim=0)\n",
        "        rewards_work = torch.cat((rewards_work, r.unsqueeze(dim=0)), dim=0)\n",
        "\n",
        "        # pick up batch to be done and append to done-list\n",
        "        done_indices = done.nonzero().squeeze(dim=1)\n",
        "        if done_indices.numel() > 0:\n",
        "            states_done.append(states_work[:,done_indices,:])\n",
        "            actions_done.append(actions_work[:,done_indices])\n",
        "            logits_done.append(logits_work[:,done_indices,:])\n",
        "            logprbs_done.append(logprbs_work[:,done_indices])\n",
        "            rewards_done.append(rewards_work[:,done_indices])\n",
        "        # filter batch to run (not to be done)\n",
        "        work_indices = (done==False).nonzero().squeeze(dim=1)\n",
        "        if work_indices.numel() > 0:\n",
        "            states_work = states_work[:,work_indices,:]\n",
        "            actions_work = actions_work[:,work_indices]\n",
        "            logits_work = logits_work[:,work_indices,:]\n",
        "            logprbs_work = logprbs_work[:,work_indices]\n",
        "            rewards_work = rewards_work[:,work_indices]\n",
        "        # also filter the current state\n",
        "        if work_indices.numel() > 0:\n",
        "            s = s[work_indices]\n",
        "\n",
        "    #\n",
        "    # Prepare tensors for training\n",
        "    #\n",
        "\n",
        "    # fill values (0 or -100) to fit to maximum timestep\n",
        "    timestep_size = env.step_count\n",
        "    states_done   = [torch.cat((s, torch.zeros((timestep_size-s.shape[0],s.shape[1],STATE_SIZE), dtype=torch.int).to(device)), dim=0) for s in states_done]\n",
        "    actions_done  = [torch.cat((a, torch.ones((timestep_size-a.shape[0],a.shape[1]), dtype=torch.int).to(device)*-100), dim=0) for a in actions_done]\n",
        "    logits_done   = [torch.cat((l, torch.zeros((timestep_size-l.shape[0],l.shape[1],ACTION_SIZE), dtype=torch.float).to(device)), dim=0) for l in logits_done]\n",
        "    logprbs_done  = [torch.cat((p, torch.zeros((timestep_size-p.shape[0],p.shape[1]), dtype=torch.float).to(device)), dim=0) for p in logprbs_done]\n",
        "    rewards_done  = [torch.cat((r, torch.zeros((timestep_size-r.shape[0],r.shape[1]), dtype=torch.float).to(device)), dim=0) for r in rewards_done]\n",
        "\n",
        "    # generate tensor from the list of tensor\n",
        "    # (tensor shape is (TIMESTEP_SIZE, BATCH_SIZE) or (TIMESTEP_SIZE, BATCH_SIZE, C) where C is the number of classes)\n",
        "    states  = torch.cat(states_done, dim=1)\n",
        "    actions = torch.cat(actions_done, dim=1)\n",
        "    logits  = torch.cat(logits_done, dim=1)\n",
        "    logprbs = torch.cat(logprbs_done, dim=1)\n",
        "    rewards = torch.cat(rewards_done, dim=1)\n",
        "    states  = states.float()\n",
        "\n",
        "    #\n",
        "    # Generate cumulative rewards\n",
        "    #\n",
        "    cum_rewards = torch.zeros_like(rewards).to(device)\n",
        "    for j in reversed(range(timestep_size)):\n",
        "        cum_rewards[j,:] = rewards[j,:] + (cum_rewards[j+1,:]*DISCOUNT if j+1 < timestep_size else 0)\n",
        "\n",
        "    #\n",
        "    # Train and optimize model parameters\n",
        "    #\n",
        "    opt.zero_grad()\n",
        "    # get values and logits with new parameters\n",
        "    values_new = value_func(states, mask=(actions==-100))\n",
        "    logits_new = actor_func(states, mask=(actions==-100))\n",
        "    # get advantages\n",
        "    advantages = cum_rewards.unsqueeze(dim=-1) - values_new\n",
        "    # calculate P_new / P_old\n",
        "    logprbs_new = -F.cross_entropy(logits_new.transpose(1,2), actions, reduction=\"none\")\n",
        "    logprbs_new = logprbs_new.unsqueeze(dim=-1)\n",
        "    prob_ratio = torch.exp(logprbs_new - logprbs.unsqueeze(dim=-1))\n",
        "    # calculate KL-div for Categorical distribution\n",
        "    l0 = logits - torch.amax(logits, dim=-1, keepdim=True) # to reduce quantity\n",
        "    l1 = logits_new - torch.amax(logits_new, dim=-1, keepdim=True) # to reduce quantity\n",
        "    e0 = torch.exp(l0)\n",
        "    e1 = torch.exp(l1)\n",
        "    e_sum0 = torch.sum(e0, dim=-1, keepdim=True)\n",
        "    e_sum1 = torch.sum(e1, dim=-1, keepdim=True)\n",
        "    p0 = e0 / e_sum0\n",
        "    kl = torch.sum(\n",
        "        p0 * (l0 - torch.log(e_sum0) - l1 + torch.log(e_sum1)),\n",
        "        dim=-1,\n",
        "        keepdim=True)\n",
        "    # get value loss\n",
        "    vf_loss = F.mse_loss(\n",
        "        values_new,\n",
        "        cum_rewards.unsqueeze(dim=-1),\n",
        "        reduction=\"none\")\n",
        "    # get total loss\n",
        "    loss = -advantages * prob_ratio + kl * kl_coeff + vf_loss * vf_coeff\n",
        "    # optimize\n",
        "    loss.sum().backward()\n",
        "    opt.step()\n",
        "\n",
        "    #\n",
        "    # Output statistics (average in batch)\n",
        "    #\n",
        "    print(\"Run iteration{} with total reward {:6.1f}  episode length {:5.1f}\".format(\n",
        "        i+1,\n",
        "        torch.mean(torch.sum(rewards, dim=0)).tolist(),\n",
        "        torch.mean(torch.sum(actions!=-100, dim=0).float()).tolist()), end=\"\\r\")\n",
        "    reward_records.append(torch.mean(torch.sum(rewards, dim=0)).tolist())\n",
        "\n",
        "    #\n",
        "    # Stop if reward mean is over a threshold\n",
        "    #\n",
        "    if np.average(reward_records[-100:]) > THRESHOLD:\n",
        "        break\n",
        "\n",
        "print(\"\\nDone\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f18e6a95-6f02-430b-88f8-6ea316f3d11c",
      "metadata": {
        "id": "f18e6a95-6f02-430b-88f8-6ea316f3d11c"
      },
      "source": [
        "Save (serialize) the generated expert model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00c501dd-42be-48b1-bcbf-93126783a247",
      "metadata": {
        "id": "00c501dd-42be-48b1-bcbf-93126783a247"
      },
      "outputs": [],
      "source": [
        "torch.save(actor_func.state_dict(), \"expert_actor.pt\")\n",
        "torch.save(value_func.state_dict(), \"expert_value.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0550249-b59f-4b8b-9126-688825a986b6",
      "metadata": {
        "id": "e0550249-b59f-4b8b-9126-688825a986b6"
      },
      "source": [
        "Show how total reward in a single episode (the average in batch) has transitioned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe3a8e06-d647-40dc-b779-3370ad3a4505",
      "metadata": {
        "id": "fe3a8e06-d647-40dc-b779-3370ad3a4505"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Generate 50 interval average\n",
        "average_reward = []\n",
        "for idx in range(len(reward_records)):\n",
        "    avg_list = np.empty(shape=(1,), dtype=int)\n",
        "    if idx < 50:\n",
        "        avg_list = reward_records[:idx+1]\n",
        "    else:\n",
        "        avg_list = reward_records[idx-49:idx+1]\n",
        "    average_reward.append(np.average(avg_list))\n",
        "plt.plot(reward_records)\n",
        "plt.plot(average_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95d38278-771d-40bc-bc41-2271c4f9bf65",
      "metadata": {
        "id": "95d38278-771d-40bc-bc41-2271c4f9bf65"
      },
      "source": [
        "Show how the trained agent transits in GridWorld."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6316925-c555-427f-a051-e09f9d25ae62",
      "metadata": {
        "id": "a6316925-c555-427f-a051-e09f9d25ae62"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "# get all initial states\n",
        "valid_states_all = torch.cat((env.valid_states, torch.tensor([env.grid_size-1,env.grid_size-1]).to(device).unsqueeze(dim=0)))\n",
        "valid_states_all = valid_states_all[:,0] * env.grid_size + valid_states_all[:,1]\n",
        "\n",
        "# create direction table\n",
        "with torch.no_grad():\n",
        "    s = torch.arange(STATE_SIZE).to(device)\n",
        "    s_onehot = F.one_hot(s, num_classes=STATE_SIZE).float()\n",
        "    logits = actor_func(s_onehot)\n",
        "    direction = torch.argmax(logits, dim=-1)\n",
        "    direction_table = torch.reshape(direction, (env.grid_size, env.grid_size))\n",
        "    direction_table = direction_table.cpu().numpy()\n",
        "\n",
        "# show table\n",
        "html_text = \"<table>\"\n",
        "for row in range(env.grid_size):\n",
        "    html_text += \"<tr>\"\n",
        "    for col in range(env.grid_size):\n",
        "        if row*env.grid_size + col in valid_states_all:\n",
        "            html_text += \"<td bgcolor=\\\"gray\\\">\"\n",
        "            #\n",
        "            # show direction\n",
        "            #\n",
        "            index = direction_table[row, col]\n",
        "            if index == 0:\n",
        "                html_text += \"&#x2191;\" # up\n",
        "            elif index == 1:\n",
        "                html_text += \"&#x2193;\" # down\n",
        "            elif index == 2:\n",
        "                html_text += \"&#x2190;\" # left\n",
        "            elif index == 3:\n",
        "                html_text += \"&#x2192;\" # right\n",
        "        else:\n",
        "            html_text += \"<td>\"\n",
        "        html_text += \"</td>\"\n",
        "    html_text += \"</tr>\"\n",
        "html_text += \"</table>\"\n",
        "\n",
        "display(HTML(html_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "086c614f-6c2b-469b-bc6d-3ef04ed8bff0",
      "metadata": {
        "id": "086c614f-6c2b-469b-bc6d-3ef04ed8bff0"
      },
      "source": [
        "If you load the pre-trained model, please run as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5c8acc0-94d9-43da-92ed-bc92c76f1dc8",
      "metadata": {
        "id": "e5c8acc0-94d9-43da-92ed-bc92c76f1dc8"
      },
      "outputs": [],
      "source": [
        "actor_func.load_state_dict(torch.load(\"expert_actor.pt\"))\n",
        "value_func.load_state_dict(torch.load(\"expert_value.pt\"))\n",
        "actor_func = actor_func.eval()\n",
        "value_func = value_func.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5655f9c6-3122-4efb-bb36-1dc5381c1e1a",
      "metadata": {
        "id": "5655f9c6-3122-4efb-bb36-1dc5381c1e1a"
      },
      "source": [
        "## 4. Generate expert trajectories (expert data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68f43776-e743-4641-929c-6a4c38f54dc3",
      "metadata": {
        "id": "68f43776-e743-4641-929c-6a4c38f54dc3"
      },
      "source": [
        "Now we have a trained expert.<br>\n",
        "Next generate expert's data with this trained agent.\n",
        "\n",
        "In this example, I generate 100,000 trajectories (episodes) and save all data into 10 separated files (in which, each file has 10,000 trajectories)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e51f9537-ba46-4c2c-8644-63739ec07874",
      "metadata": {
        "id": "e51f9537-ba46-4c2c-8644-63739ec07874"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "# total number of episodes to run\n",
        "episode_num = 100000\n",
        "# number of episodes to save in a single checkpoint file\n",
        "episode_num_in_ckpt = 10000\n",
        "# batch size to run inference\n",
        "# (the number of episodes in each file)\n",
        "inf_batch_size = 100\n",
        "# directory name to save files\n",
        "dest_dir = \"./expert_data\"\n",
        "\n",
        "assert episode_num % episode_num_in_ckpt == 0\n",
        "assert episode_num_in_ckpt % inf_batch_size == 0\n",
        "\n",
        "total_iter_num = int(episode_num / inf_batch_size)\n",
        "\n",
        "Path(dest_dir).mkdir(exist_ok=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(total_iter_num):\n",
        "        #\n",
        "        # Run episode till done as a batch to generate tensors\n",
        "        #\n",
        "        done = torch.tensor([False]).to(device)\n",
        "        # define working items\n",
        "        states_work = torch.empty((inf_batch_size,0), dtype=torch.int).to(device)\n",
        "        actions_work = torch.empty((inf_batch_size,0), dtype=torch.int).to(device)\n",
        "        rewards_work = torch.empty((inf_batch_size,0), dtype=torch.float).to(device)\n",
        "        # define done items\n",
        "        states_done = []\n",
        "        actions_done = []\n",
        "        rewards_done = []\n",
        "        # start\n",
        "        s = env.reset(inf_batch_size)\n",
        "        while not (torch.prod(done) == 1):\n",
        "            s_onehot = F.one_hot(s, num_classes=STATE_SIZE)\n",
        "            states_work = torch.cat((states_work, s.unsqueeze(dim=1)), dim=1)\n",
        "            a, _, _ = pick_sample_and_logp(actor_func, s_onehot)\n",
        "            s, r, term, trunc = env.step(a, s)\n",
        "            done = torch.logical_or(term, trunc)\n",
        "            actions_work = torch.cat((actions_work, a.unsqueeze(dim=1)), dim=1)\n",
        "            rewards_work = torch.cat((rewards_work, r.unsqueeze(dim=1)), dim=1)\n",
        "            # pick up batch to be done and append to done-list\n",
        "            done_indices = done.nonzero().squeeze(dim=1)\n",
        "            if done_indices.numel() > 0:\n",
        "                states_done.append(states_work[done_indices,:])\n",
        "                actions_done.append(actions_work[done_indices,:])\n",
        "                rewards_done.append(rewards_work[done_indices,:])\n",
        "            # filter batch to run (not to be done)\n",
        "            work_indices = (done==False).nonzero().squeeze(dim=1)\n",
        "            if work_indices.numel() > 0:\n",
        "                states_work = states_work[work_indices,:]\n",
        "                actions_work = actions_work[work_indices,:]\n",
        "                rewards_work = rewards_work[work_indices,:]\n",
        "            # also filter the current state\n",
        "            if work_indices.numel() > 0:\n",
        "                s = s[work_indices]\n",
        "\n",
        "        #\n",
        "        # Save results as numpy array\n",
        "        #\n",
        "\n",
        "        # e.g, [tensor([[1,3,1],[2,1,1]]), tensor([[1,2,1,3]]), ...]\n",
        "\n",
        "        # split tensors into list of tensors\n",
        "        # --> [[tensor([[1,3,1]]), tensor([[2,1,1]])], [tensor([[1,2,1,3]])], ...]\n",
        "        states_done = [torch.split(s, 1, dim=0) for s in states_done]\n",
        "        actions_done = [torch.split(a, 1, dim=0) for a in actions_done]\n",
        "        rewards_done = [torch.split(r, 1, dim=0) for r in rewards_done]\n",
        "        # flatten into 1-dimension list\n",
        "        # --> [tensor([[1,3,1]]), tensor([[2,1,]]), tensor([[1,2,1,3]]), ...]\n",
        "        states_done = [s2 for s1 in states_done for s2 in s1]\n",
        "        actions_done = [a2 for a1 in actions_done for a2 in a1]\n",
        "        rewards_done = [r2 for r1 in rewards_done for r2 in r1]\n",
        "        # squeeze in each element\n",
        "        # --> [tensor([1,3,1]), tensor([2,1,1]), tensor([1,2,1,3]), ...]\n",
        "        states_done = [s.squeeze(dim=0) for s in states_done]\n",
        "        actions_done = [s.squeeze(dim=0) for s in actions_done]\n",
        "        rewards_done = [s.squeeze(dim=0) for s in rewards_done]\n",
        "        # shuffle\n",
        "        all_done = list(zip(states_done, actions_done, rewards_done))\n",
        "        random.shuffle(all_done)\n",
        "        states_done, actions_done, rewards_done = zip(*all_done)\n",
        "        states_done, actions_done, rewards_done = list(states_done), list(actions_done), list(rewards_done)\n",
        "        # get step length in each episode\n",
        "        step_lens = [a.shape[0] for a in actions_done]\n",
        "        # flatten\n",
        "        states_done = torch.cat(states_done, dim=0)\n",
        "        actions_done = torch.cat(actions_done, dim=0)\n",
        "        rewards_done = torch.cat(rewards_done, dim=0)\n",
        "        # to numpy\n",
        "        states_done = states_done.cpu().numpy()\n",
        "        actions_done = actions_done.cpu().numpy()\n",
        "        rewards_done = rewards_done.cpu().numpy()\n",
        "        step_lens = np.array(step_lens)\n",
        "        # output progress\n",
        "        print(\"Processed {:6d} / {:6d} episodes ...\".format(inf_batch_size * (i + 1), episode_num), end=\"\\r\")\n",
        "        # save in each episode_num_in_ckpt\n",
        "        if (i + 1) % (episode_num_in_ckpt / inf_batch_size) == 1:\n",
        "            # initialize\n",
        "            states_store = states_done\n",
        "            actions_store = actions_done\n",
        "            rewards_store = rewards_done\n",
        "            timestep_lens_store = step_lens\n",
        "        else:\n",
        "            # add to list\n",
        "            states_store = np.concatenate((states_store, states_done), axis=0)\n",
        "            actions_store = np.concatenate((actions_store, actions_done), axis=0)\n",
        "            rewards_store = np.concatenate((rewards_store, rewards_done), axis=0)\n",
        "            timestep_lens_store = np.concatenate((timestep_lens_store, step_lens), axis=0)\n",
        "            # save\n",
        "            if (i + 1) % (episode_num_in_ckpt / inf_batch_size) == 0:\n",
        "                ckpt_num = int((i + 1) / (episode_num_in_ckpt / inf_batch_size) - 1)\n",
        "                with open(f\"{dest_dir}/ckpt{ckpt_num}.pkl\",\"wb\") as f:\n",
        "                    pickle.dump({\n",
        "                        \"states\": states_store,\n",
        "                        \"actions\": actions_store,\n",
        "                        \"rewards\": rewards_store,\n",
        "                        \"timestep_lens\": timestep_lens_store,\n",
        "                    }, f)\n",
        "print(\"\\nDone\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70641cd5-6ef6-4ba7-b05c-6d1621daeb4f",
      "metadata": {
        "id": "70641cd5-6ef6-4ba7-b05c-6d1621daeb4f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}