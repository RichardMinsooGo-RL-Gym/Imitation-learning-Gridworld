{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f35fd98-8613-45a2-959b-e5da21d9d4d3",
   "metadata": {},
   "source": [
    "# Relative Entropy Inverse Reinforcement Learning (RelEnt IRL)\n",
    "\n",
    "Relative Entropy Inverse Reinforcement Learning (shortly, RelEnt IRL) is a model-free IRL method, in which you don't need to know the system dynamics.<br>\n",
    "Unlike [Maximum Entropy IRL](./03_maxent_irl.ipynb) (previous example), you can run this method, **even when the transition probabilities are unknown**. (Usually, to know the transition function is not the case in many robotic applications.)\n",
    "\n",
    "Assuming that the expert is optimal, this RelEnt IRL method (inspired by Relative Entropy Policy Search, REPS) learns a reward function. And you can then use it to recover the expert's generalized policy.\n",
    "\n",
    "As you can see below, this method requires not only expert demonstrations, but also requires non-expert demonstrations by an arbitrary policy, for performing importance sampling.\n",
    "\n",
    "*(back to [index](https://github.com/tsmatz/imitation-learning-tutorials/))*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63d7169-4138-4f2b-8770-2c5d2d6f3aae",
   "metadata": {},
   "source": [
    "## Overview of Relative Entropy Inverse Reinforcement Learning method\n",
    "\n",
    "First of all, let's briefly follow RelEnt IRL along with the original paper [[A Boularias et al., 2011](https://proceedings.mlr.press/v15/boularias11a/boularias11a.pdf)].\n",
    "\n",
    "Suppose $P(\\cdot)$ and $Q(\\cdot)$ are probability distributions.<br>\n",
    "The following relative entropy (or KL-divergence) $\\verb|Rel|(P || Q)$ is a distance of distributions between $P$ and $Q$, in which relative entropy always satisfies $\\verb|Rel|(P || Q) \\geq 0$ with equality if, and only if, $P(\\tau) = Q(\\tau)$\n",
    "\n",
    "$\\displaystyle \\verb|Rel|(P || Q) \\coloneqq \\sum_{\\tau} P(\\tau) \\ln{\\frac{P(\\tau)}{Q(\\tau)}}$\n",
    "\n",
    "> Note : The relative entropy (KL-divergence) is $\\displaystyle \\int P(\\tau) \\ln{\\frac{P(\\tau)}{Q(\\tau)}} d\\tau$ in continuous sapce, and $\\displaystyle \\sum_{\\tau} P(\\tau) \\ln{\\frac{P(\\tau)}{Q(\\tau)}} $ in discrete space.<br>\n",
    "> In this example, we assume it's discrete space.\n",
    "\n",
    "Assuming that $Q(\\cdot)$ is the distribution of expert policy (baseline policy) on trajectories $\\mathcal{T}$, Relative Entropy Inverse Reinforcement Learning (RelEnt IRL) is a method to find the distribution $P(\\cdot)$ to minimize :\n",
    "\n",
    "$\\displaystyle \\sum_{\\tau \\in \\mathcal{T}} P(\\tau) \\ln{\\frac{P(\\tau)}{Q(\\tau)}} $\n",
    "\n",
    "subject to the following constraints. :\n",
    "\n",
    "- $ \\left| \\sum_{\\tau \\in \\mathcal{T}} P(\\tau) f_i^{\\tau} - \\hat{f}_i \\right| \\leq \\epsilon_i \\;\\;\\; \\forall i \\in \\{1, \\ldots ,k\\}$\n",
    "- $ \\sum_{\\tau \\in \\mathcal{T}} P(\\tau) = 1 $\n",
    "- $ P(\\tau) \\geq 0 \\;\\;\\; \\forall \\tau \\in \\mathcal{T} $\n",
    "\n",
    "where $f_i^{\\tau}$ is the $i$-th element in the feature vector of the trajectory $\\tau$, and $\\hat{f}_i$ is the $i$-th element in the expected feature vector which is induced by the expert demonstrations.\n",
    "\n",
    "The first constraint is the feature expectation matching constraint. (See [here](./03_maxent_irl.ipynb) for the feature expectation matching in IRL.)<br>\n",
    "When $\\gamma$ is a discount, $f_i^{\\tau} = \\sum_t \\gamma^t f_i(s_t)$, where $t$ is time-step and $f_i(s_t)$ is the feature of state $s_t$. (Because the expected return is $\\sum_t \\gamma^t \\theta_i f_i(s_t) $, where $\\theta_i$ is $i$-th element of reward weight.)<br>\n",
    "The above $\\epsilon_i$ is obtained by Hoeffding bound as follows.\n",
    "\n",
    "$\\displaystyle \\epsilon_i = \\sqrt{\\frac{-\\ln{(1-\\delta)}}{2N}} \\frac{\\gamma^{H+1}-1}{\\gamma-1} \\left( \\max_s f_i(s) - \\min_s f_i(s) \\right) $\n",
    "\n",
    "where $N$ is the number of sampled trajectories and $\\delta$ is a confidence probability.\n",
    "\n",
    "> Note : I note that the following holds:<br>\n",
    "> $\\displaystyle \\frac{\\gamma^{H+1} - 1}{\\gamma - 1} = \\gamma^H + \\gamma^{H-1} + \\cdots + \\gamma + 1$\n",
    "\n",
    "When $\\delta$ is given, the probability that the difference between the feature counts given the distribution $P$ and the true feature counts is larger than $2\\epsilon$ becomes $\\delta$.\n",
    "\n",
    "By applying Lagrange multipliers ($\\theta$ and $\\eta$) with KKT (Karush–Kuhn–Tucker) condition, you can get the following Lagrangian. ($\\theta$ becomes a reward weight.) :\n",
    "\n",
    "$\\displaystyle L(P,\\theta,\\eta) = \\sum_{\\tau \\in \\mathcal{T}} P(\\tau) \\ln{\\frac{P(\\tau)}{Q(\\tau)}} - \\sum_{i=1}^k \\theta_i \\left( \\sum_{\\tau \\in \\mathcal{T}} P(\\tau) f_i^{\\tau}-\\hat{f}_i \\right) - \\sum_{i=1}^k |\\theta_i| \\epsilon_i + \\eta \\left( \\sum_{\\tau \\in \\mathcal{T}} P(\\tau) - 1 \\right) $\n",
    "\n",
    "> Note : See [here](https://tsmatz.wordpress.com/2020/06/01/svm-and-kernel-functions-mathematics/) for KKT (Karush–Kuhn–Tucker) condition and dual representation in optimization.\n",
    "\n",
    "By applying KKT condition $\\partial_{P(\\tau)} L(P,\\theta,\\eta) = 0$, we then get :\n",
    "\n",
    "$\\displaystyle \\ln{\\frac{P(\\tau)}{Q(\\tau)}} -\\sum_{i=1}^k \\theta_i f_i^{\\tau} + \\eta + 1 = 0 $\n",
    "\n",
    "Hence we can get the optimal $P(\\tau)$ by :\n",
    "\n",
    "$\\displaystyle P(\\tau)=Q(\\tau) \\exp{\\left( \\sum_{i=1}^k \\theta_i f_i^{\\tau} - \\eta - 1 \\right)}$\n",
    "\n",
    "Applying normalization, you can then get :\n",
    "\n",
    "$\\displaystyle P(\\tau|\\theta)=\\frac{1}{Z(\\theta)} Q(\\tau) \\exp{\\left( \\sum_{i=1}^k \\theta_i f_i^{\\tau} \\right)} \\;\\;\\;\\;\\; (1)$\n",
    "\n",
    "where\n",
    "\n",
    "$\\displaystyle Z(\\theta)\\coloneqq\\sum_{\\tau \\in \\mathcal{T}} Q(\\tau) \\exp{\\left( \\sum_{i=1}^k \\theta_i f_i^{\\tau} \\right)}$\n",
    "\n",
    "> Note : By normalization constraint $\\sum_{\\tau \\in \\mathcal{T}} P(\\tau) = 1$, you can also get :<br>\n",
    "> $\\exp(\\eta+1)=Z(\\theta)$\n",
    "\n",
    "By substituting and erasing variables, you can get the dual problem to maximize the following $g(\\theta)$ :\n",
    "\n",
    "$\\displaystyle g(\\theta) = \\sum_{i=1}^k \\theta_i \\hat{f}_i - \\ln{Z(\\theta)} - \\sum_{i=1}^k |\\theta_i| \\epsilon_i $\n",
    "\n",
    "The function $g$ is concave and differentiable everywhere except for $\\theta_i=0$, hence, it can be maximized by gradient-based optimization.<br>\n",
    "Now the partial differentiation of $g$ is given by :\n",
    "\n",
    "$\\displaystyle \\frac{\\partial}{\\partial \\theta_i}g(\\theta)=\\hat{f}_i - \\sum_{\\tau \\in \\mathcal{T}} P(\\tau|\\theta) f_i^{\\tau} - \\alpha_i \\epsilon_i \\;\\;\\;\\;\\; (2)$\n",
    "\n",
    "where $\\alpha_i=1$ if $\\theta_i \\geq 0$ and $\\alpha_i=-1$ otherwise.\n",
    "\n",
    "> Note : You can apply gradient method by:<br>\n",
    "> $\\displaystyle \\nabla g(\\theta) = \\sum_{i} \\frac{\\partial}{\\partial \\theta_i} g(\\theta) $\n",
    "\n",
    "When the transition probability $p(s_{t+1}|s_t,a_t)$ is known, you can easily get $P(\\tau | \\theta)$ by (1), and then you can get the gradient $\\frac{\\partial}{\\partial \\theta_i}g(\\theta)$ for optimization.<br>\n",
    "However, the transition probability is often unknow in practices.\n",
    "\n",
    "In such case, you can get $P(\\tau | \\theta)$ by importance sampling with an arbitrary policy.<br>\n",
    "Now let's see how to formulate $P(\\tau | \\theta)$ with importance sampling.\n",
    "\n",
    "First, let me decompose $Q(\\tau)$ by :\n",
    "\n",
    "$\\displaystyle Q(\\tau) = D(\\tau)U(\\tau)$\n",
    "\n",
    "where $D(\\tau)$ is the joint probability of transitions\n",
    "\n",
    "$\\displaystyle D(\\tau) = d_0(s_1) \\prod_{t=1}^H p(s_{t+1}|s_t,a_t)$\n",
    "\n",
    "and $U(\\tau)$ is the joint probability of the actions on trajectory $\\tau$.\n",
    "\n",
    "With this decomposition, the equation (1) becomes :\n",
    "\n",
    "$\\displaystyle P(\\tau|\\theta)=\\frac{D(\\tau)U(\\tau) \\exp{\\left( \\sum_{i=1}^k \\theta_i f_i^{\\tau} \\right)}}{\\sum_{\\tau \\in \\mathcal{T}} D(\\tau)U(\\tau) \\exp{\\left( \\sum_{i=1}^k \\theta_i f_i^{\\tau} \\right)}}$\n",
    "\n",
    "Now we approximate by importance sampling with an arbitrary policy $\\pi$.<br>\n",
    "Let $N$ be the number of trajectory samples, $\\mathcal{T}_N^{\\pi}$ be a set of sampled trajectories, and $\\pi(\\tau)$ be the joint probability of the actions on trajectory $\\tau$ in this given policy $\\pi$.<br>\n",
    "By applying importance sampling and equation (1), the second term in (2) is then approximated as follows. :\n",
    "\n",
    "$\\displaystyle \\sum_{\\tau \\in \\mathcal{T}} P(\\tau|\\theta) f_i^{\\tau}$\n",
    "\n",
    "$\\displaystyle \\simeq \\frac{1}{N} \\sum_{\\tau \\in \\mathcal{T}_N^{\\pi}} \\frac{P(\\tau|\\theta)}{D(\\tau)\\pi(\\tau)} f_i^{\\tau}$\n",
    "\n",
    "$\\displaystyle = \\frac{1}{N} \\frac{\\sum_{\\tau \\in \\mathcal{T}_N^{\\pi}}\\frac{D(\\tau)U(\\tau)\\exp \\left( \\sum_{j=1}^k \\theta_j f_j^{\\tau} \\right)}{D(\\tau)\\pi(\\tau)}f_i^{\\tau}}{\\sum_{\\tau \\in \\mathcal{T}} D(\\tau)U(\\tau) \\exp \\left( \\sum_{j=1}^k \\theta_j f_j^{\\tau} \\right)}$\n",
    "\n",
    "$\\displaystyle \\simeq \\frac{\\frac{1}{N} \\sum_{\\tau \\in \\mathcal{T}_N^{\\pi}} \\frac{D(\\tau)U(\\tau)\\exp \\left( \\sum_{j=1}^k \\theta_j f_j^{\\tau} \\right)}{D(\\tau)\\pi(\\tau)} f_i^{\\tau}}{\\frac{1}{N} \\sum_{\\tau \\in \\mathcal{T}_N^{\\pi}} \\frac{D(\\tau)U(\\tau)\\exp \\left( \\sum_{j=1}^k \\theta_j f_j^{\\tau} \\right)}{D(\\tau)\\pi(\\tau)}}$\n",
    "\n",
    "$\\displaystyle = \\frac{\\sum_{\\tau \\in \\mathcal{T}_N^{\\pi}} \\frac{U(\\tau)}{\\pi(\\tau)} \\exp \\left( \\sum_{j=1}^k \\theta_j f_j^{\\tau} \\right) f_i^{\\tau}}{\\sum_{\\tau \\in \\mathcal{T}_N^{\\pi}} \\frac{U(\\tau)}{\\pi(\\tau)} \\exp \\left( \\sum_{j=1}^k \\theta_j f_j^{\\tau} \\right)} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; (3)$\n",
    "\n",
    "As you can see above, the transition probabilities $D(\\tau)$ is erased and not needed in this equation.\n",
    "\n",
    "Now you can get the gradient $\\nabla g(\\theta)$ by equation (2), and you can optimize $\\theta$ with gradient-based method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be5b2c2-bcef-4ea3-9824-822eecb6c8a4",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e961eadd-fb2b-414f-99aa-45c72abc37d4",
   "metadata": {},
   "source": [
    "Now let's implement above algorithm.\n",
    "\n",
    "In GridWorld example, we know the transition probability $p(s_{t+1}|s_t,a_t)$ and don't need to apply importance sampling.<br>\n",
    "But in this example, I assume that the transition probability is unknown and I then apply importance sampling.\n",
    "\n",
    "> Note : To speed up computation, I'll implement all operations with PyTorch tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c8991d-e711-42c3-a3aa-f5700bc59017",
   "metadata": {},
   "source": [
    "### 1. Restore environment and load expert's data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aadf22f-b732-47ed-9650-60707b3763b8",
   "metadata": {},
   "source": [
    "Before we start, we need to install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f62b66f-f7ca-4378-93a6-89e72f0786a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1357037-4677-41ba-b7bf-0baa28d7de52",
   "metadata": {},
   "source": [
    "This algorithm needs to compute the joint probability and then needs precision.<br>\n",
    "In order to prevent from vanishing values, I use double precision for float operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bb0e38c-281d-4550-8b17-a30f3db48e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e7101f-e123-43ad-9607-8a4389e8b618",
   "metadata": {},
   "source": [
    "Now I define environment (GridWorld).\n",
    "\n",
    "> Note : All members are implemented as PyTorch tensor operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46a6f0e3-b337-431a-9b97-c0557c274802",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" All members are tensor operations. \"\"\"\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "GRID_SIZE = 50\n",
    "MAX_TIMESTEP = 200\n",
    "\n",
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    This environment is motivated by the following paper.\n",
    "    https://proceedings.mlr.press/v15/boularias11a/boularias11a.pdf\n",
    "\n",
    "    - It has 50 x 50 grids (cells).\n",
    "    - The agent has four actions for moving in one of the directions of the compass.\n",
    "    - If ```transition_prob``` = True, the actions succeed with probability 0.7,\n",
    "      a failure results in a uniform random transition to one of the adjacent states.\n",
    "    - A reward of 10 is given for reaching the goal state, located on the bottom-right corner.\n",
    "    - For the remaining states,\n",
    "      the reward function was randomly set to 0 with probability 2/3\n",
    "      and to −1 with probability 1/3.\n",
    "    - If the agent moves across the border, it's given the fail reward (i.e, reward=`-1`).\n",
    "    - The initial state is sampled from a uniform distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reward_map, valid_states, transition_prob=True):\n",
    "        \"\"\"\n",
    "        Initialize class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        reward_map : float[GRID_SIZE * GRID_SIZE]\n",
    "            Reward for each state.\n",
    "        valid_states : list(int[2])\n",
    "            List of states, in which the agent can reach to goal state without losing any rewards.\n",
    "            Each state is a 2d vector, [row, column].\n",
    "            When you call reset(), the initial state is picked up from these states.\n",
    "        transition_prob : bool\n",
    "            True if transition probability (above) is enabled.\n",
    "            False when we generate an expert agent without noise.\n",
    "        \"\"\"\n",
    "        self.reward_map = torch.tensor(reward_map).to(device)\n",
    "        self.valid_states = torch.tensor(valid_states).to(device)\n",
    "        self.transition_prob = transition_prob\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Randomly, get initial state (single state) from valid states.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        state : int\n",
    "            Return the picked-up state id.\n",
    "        \"\"\"\n",
    "        # initialize step count\n",
    "        self.step_count = 0\n",
    "        # pick up sample of valid states\n",
    "        idx = torch.multinomial(torch.ones(len(self.valid_states)).to(device), 1).squeeze(dim=0)\n",
    "        state_2d = self.valid_states[idx]\n",
    "        # convert 2d index to 1d index\n",
    "        state_1d = state_2d[0] * GRID_SIZE + state_2d[1]\n",
    "        # return result\n",
    "        return state_1d\n",
    "\n",
    "    def step(self, action, state, trans_state_only=False):\n",
    "        \"\"\"\n",
    "        Take action, proceed step, and return the result.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            Actions to take\n",
    "            (0=UP 1=DOWN 2=LEFT 3=RIGHT)\n",
    "        state : int\n",
    "            Current state id.\n",
    "        trans_state_only : bool\n",
    "            Set TRUE, when you call to get next state by stateless without reset()\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        new-state : int\n",
    "            New state id.\n",
    "        reward : int\n",
    "            The obtained reward.\n",
    "        done : bool\n",
    "            Flag to check whether it terminates.\n",
    "        \"\"\"\n",
    "        # if transition prob is enabled, apply transition\n",
    "        if self.transition_prob:\n",
    "            # the action succeeds with probability 0.7\n",
    "            prob = torch.ones(4).to(device)\n",
    "            mask = F.one_hot(action, num_classes=4).bool()\n",
    "            prob = torch.where(mask, 7.0, prob)\n",
    "            selected_action = torch.multinomial(prob, 1, replacement=True)\n",
    "            selected_action = selected_action.squeeze(dim=0)\n",
    "            action_onehot = F.one_hot(selected_action, num_classes=4)\n",
    "        else:\n",
    "            action_onehot = torch.zeros(4, dtype=int).to(device)\n",
    "            action_onehot[action] += 1\n",
    "        # get 2d state\n",
    "        mod = torch.div(state, GRID_SIZE, rounding_mode=\"floor\")\n",
    "        reminder = torch.remainder(state, GRID_SIZE)\n",
    "        state_2d = torch.cat((mod.unsqueeze(dim=-1), reminder.unsqueeze(dim=-1)), dim=-1)\n",
    "        # move state\n",
    "        # (0=UP 1=DOWN 2=LEFT 3=RIGHT)\n",
    "        up_and_down = action_onehot[1] - action_onehot[0]\n",
    "        left_and_right = action_onehot[3] - action_onehot[2]\n",
    "        move = torch.cat((up_and_down.unsqueeze(dim=-1), left_and_right.unsqueeze(dim=-1)), dim=-1)\n",
    "        new_state = state_2d + move\n",
    "        # set reward\n",
    "        reward = torch.tensor(0.0).to(device)\n",
    "        if not(trans_state_only):\n",
    "            if (new_state[0] < 0) or (new_state[0] >= GRID_SIZE) or (new_state[1] < 0) or (new_state[1] >= GRID_SIZE):\n",
    "                # if location is out of border, set reward=-1\n",
    "                reward -= 1.0\n",
    "            else:\n",
    "                # if succeed, add reward of current state\n",
    "                state_1d = new_state[0] * GRID_SIZE + new_state[1]\n",
    "                reward += self.reward_map[state_1d]\n",
    "        # correct location\n",
    "        new_state = torch.clip(new_state, min=0, max=GRID_SIZE-1)\n",
    "        # return result\n",
    "        if trans_state_only:\n",
    "            return new_state[0] * GRID_SIZE + new_state[1]\n",
    "        else:\n",
    "            self.step_count += 1\n",
    "            return new_state[0] * GRID_SIZE + new_state[1], reward, (new_state[0]==GRID_SIZE-1 and new_state[1]==GRID_SIZE-1) or (self.step_count==MAX_TIMESTEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15586645-f599-4c32-9b9a-e9a41029e8a4",
   "metadata": {},
   "source": [
    "Restore GridWorld environment from JSON file. (See [this script](./00_generate_expert_trajectories.ipynb) for generating the same environment.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f774050-fef6-4433-a436-56e9c73bcb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"gridworld.json\", \"r\") as f:\n",
    "    json_object = json.load(f)\n",
    "    env = GridWorld(**json_object, transition_prob=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139782fd-c2a3-4995-8634-6c288ef6fd63",
   "metadata": {},
   "source": [
    "Load expert's data (demonstrations) which is saved in ```./expert_data``` folder in this repository.\n",
    "\n",
    "> Note : See [this script](./00_generate_expert_trajectories.ipynb) for generating expert dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d456d820-5a79-4c7f-88aa-7471364757d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "dest_dir = \"./expert_data\"\n",
    "checkpoint_file = \"ckpt0.pkl\"\n",
    "\n",
    "# load expert data from pickle\n",
    "with open(f\"{dest_dir}/{checkpoint_file}\", \"rb\") as f:\n",
    "    exp_data = pickle.load(f)\n",
    "exp_states = torch.tensor(exp_data[\"states\"]).to(device)\n",
    "exp_actions = torch.tensor(exp_data[\"actions\"]).to(device)\n",
    "timestep_lens = exp_data[\"timestep_lens\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58be719-8418-4648-ab8f-1a427bd75d1a",
   "metadata": {},
   "source": [
    "### 2. Get expert's feature expectation $\\hat{f}$ and expert's action probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2d1fd8-11e8-4b21-baba-d55063f72334",
   "metadata": {},
   "source": [
    "We can easily get the feature expectation $\\hat{f}$ (the following ```f_exp```) by using expert's demonstrations as follows.<br>\n",
    "(Here we apply discount $\\gamma=0.99$ unlike [previous example](./03_maxent_irl.ipynb).)\n",
    "\n",
    "$\\displaystyle \\hat{f} = \\sum_{\\tau} \\sum_t \\gamma^t f(s_t)$\n",
    "\n",
    "I note that expert's dataset doesn't have the final state, but the expert's feature expectation needs the whole states (including goal state).<br>\n",
    "Hence I have added the final state in each trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92bc62de-994a-4c0f-a32b-e648977790a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "def get_discounted_traj_feature(traj_state_features, gamma):\n",
    "    \"\"\"\n",
    "    Convert from state's feature array to discounted trajectory feature\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    traj_state_features : float[N, H, K] or float[H, K]\n",
    "        State's feature array.\n",
    "        N is the number of trajectories, H is the number of timestep (i.e, horizon),\n",
    "        and K is dimension of feature.\n",
    "        If it's a single trajectory, you can use float[H, K] as input.\n",
    "    gamma : float\n",
    "        A discount value\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    traj_features : float[N, K] or float[K]\n",
    "        When fs_0^{\\tau}, fs_1^{\\tau}, ... , fs_H^{\\tau} is the state's features in trajectory,\n",
    "        it returns fs_0^{\\tau} + gamma * fs_1^{\\tau} + ... + gamma^H + fs_H^{\\tau} .\n",
    "        If the input type is float[H, K], the output type is float[K].\n",
    "    \"\"\"\n",
    "\n",
    "    # get horizon\n",
    "    horizon = traj_state_features.size(dim=-2)\n",
    "\n",
    "    # apply weight (discount)\n",
    "    seed = torch.ones(horizon).to(device) * gamma\n",
    "    exponent = torch.arange(horizon).to(device)\n",
    "    weight = torch.pow(seed, exponent)\n",
    "    traj_features_weighted = torch.mul(traj_state_features, weight.unsqueeze(dim=-1))\n",
    "\n",
    "    # sum up\n",
    "    return torch.sum(traj_features_weighted, dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4888345f-f9e2-4bd6-bcea-65b08a237047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "STATE_SIZE = GRID_SIZE*GRID_SIZE\n",
    "ACTION_SIZE = 4\n",
    "\n",
    "# initialize\n",
    "current_timestep = 0\n",
    "sum_of_features = torch.zeros(STATE_SIZE).to(device)\n",
    "\n",
    "# loop all trajectories in demonstration\n",
    "for timestep_len in timestep_lens:\n",
    "    # pick up state's id array in a single trajectory\n",
    "    traj_states_exp = exp_states[current_timestep:current_timestep+timestep_len]\n",
    "\n",
    "    # add final state (see above)\n",
    "    final_action_exp = exp_actions[current_timestep+timestep_len - 1]\n",
    "    final_state_exp = env.step(final_action_exp, traj_states_exp[-1], trans_state_only=True)\n",
    "    traj_states_exp = torch.cat((traj_states_exp, final_state_exp.unsqueeze(dim=-1)))\n",
    "\n",
    "    # convert into state's features (i.e, one-hot)\n",
    "    horizon = len(traj_states_exp)\n",
    "    traj_state_features_exp = torch.zeros(horizon, STATE_SIZE, dtype=int).to(device)\n",
    "    traj_state_features_exp[torch.arange(horizon), traj_states_exp] = 1\n",
    "\n",
    "    # get trajectory feature with discount\n",
    "    traj_feature = get_discounted_traj_feature(\n",
    "        traj_state_features_exp.double(),\n",
    "        gamma\n",
    "    )\n",
    "\n",
    "    # sum up\n",
    "    sum_of_features += traj_feature\n",
    "\n",
    "    # proceed to next trajectory\n",
    "    current_timestep += timestep_len\n",
    "\n",
    "# divide by the number of trajectories\n",
    "f_exp = sum_of_features / len(timestep_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b25785-a6b2-4de2-97ce-1eb94d2f490f",
   "metadata": {},
   "source": [
    "Next we create a matrix of expert action's probabilities ```action_prob_exp``` with shape ```[STATE_SIZE, ACTION_SIZE]```, in which the probability of action $a$ in state $s$ is the value of element ```[s, a]```.<br>\n",
    "Later this matrix is used to get the joint probability of the actions on trajectory $\\tau$,  i.e, $U(\\tau)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3216704-5e83-42fd-ae18-0f3c63787899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        ...,\n",
       "        [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.0000, 0.0000, 0.5000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize\n",
    "current_timestep = 0\n",
    "action_count = torch.zeros(STATE_SIZE, ACTION_SIZE).to(device)\n",
    "\n",
    "# loop all trajectories\n",
    "for timestep_len in timestep_lens:\n",
    "    # pick up state's and action's id array in a single trajectory\n",
    "    states_in_traj = exp_states[current_timestep:current_timestep+timestep_len]\n",
    "    actions_in_traj = exp_actions[current_timestep:current_timestep+timestep_len]\n",
    "\n",
    "    # add 1 in element [s, a] to count actions\n",
    "    horizon = len(states_in_traj)\n",
    "    for i in range(horizon):\n",
    "        s = states_in_traj[i]\n",
    "        a = actions_in_traj[i]\n",
    "        action_count[s, a] += 1.0\n",
    "\n",
    "    # proceed to next trajectory\n",
    "    current_timestep += timestep_len\n",
    "\n",
    "# set 1 if all actions are zero in each state\n",
    "# (to prevent from dividing by zero)\n",
    "action_count += torch.all(action_count==0.0, dim=1).double().unsqueeze(dim=-1)\n",
    "\n",
    "# get action probability in each state\n",
    "action_sum = torch.sum(action_count, dim=1, keepdim=True)\n",
    "action_prob_exp = torch.div(action_count, action_sum)\n",
    "\n",
    "action_prob_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063c901c-739f-4a69-a3a6-8bd8230d5297",
   "metadata": {},
   "source": [
    "### 3. Create a function to generate $N$ trajectorie's samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a40fc49-ac2e-40ae-ba54-162c10cd2caf",
   "metadata": {},
   "source": [
    "Now I create a function to generate trajectory samples with an arbitrary policy $\\pi$.\n",
    "\n",
    "In this example, I simply perform uniform sampling as an arbitrary policy.<br>\n",
    "However, we should prevent $U(\\tau)$ from being zero when we pick up a sample trajectory $\\tau$.\n",
    "\n",
    "To do this, firstly we create an action's probability matrix ```action_prob_pi```, with which the action is uniformly picked up, but the probability has zero when the corresponding probability in ```action_prob_exp``` is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "821960b3-e683-4436-af91-f38472886216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        ...,\n",
       "        [1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.0000, 0.0000, 0.5000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_prob_pi = torch.ones(STATE_SIZE, ACTION_SIZE).to(device)\n",
    "action_prob_pi *= action_prob_exp.bool()\n",
    "action_sum = torch.sum(action_prob_pi, dim=1, keepdim=True)\n",
    "action_prob_pi = torch.div(action_prob_pi, action_sum)\n",
    "\n",
    "action_prob_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed5a80c-f679-440a-a9b0-2080e1c0f0a7",
   "metadata": {},
   "source": [
    "I note that ```action_prob_pi``` is uniform distribution, unlike previous ```action_prob_exp```.<br>\n",
    "For instance, ```action_prob_exp[6]``` (action's probabilities of expert in state ```6```) is ```[0.0000, 0.3333, 0.0000, 0.6667]```, but the corresponding ```action_prob_pi[6]``` has uniform probabilities in possible action ```1``` and ```3```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1d40ac6-5c22-4e61-a939-d342884070f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.3333, 0.0000, 0.6667], device='cuda:0')\n",
      "tensor([0.0000, 0.5000, 0.0000, 0.5000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(action_prob_exp[6])\n",
    "print(action_prob_pi[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6121902-049e-4efe-9d42-39844abe3c9f",
   "metadata": {},
   "source": [
    "Now I create a function to generate $N$ trajectorie's samples.<br>\n",
    "This function uses above probability matrix to pick up samples.\n",
    "\n",
    "This function returns the following 2 tensors.\n",
    "\n",
    "1. A tensor to include the visited states. The shape of this tensor is ```[N, H+1]```, in which the visited state of the timestep $t$ in $n$-th trajectory is the value of element ```[n, t]```.\n",
    "2. A tensor to include the taken actions. The shape of this tensor is ```[N, H]```, in which the taken action of the timestep $t$ in $n$-th trajectory is the value of element ```[n, t]```.\n",
    "\n",
    "Please take care for the shape of these tensors.<br>\n",
    "The number of pairs of visited state and its taken action is ```H```. But the result tensor of visited states should also include the final state and the length becomes ```H+1```.\n",
    "\n",
    "When the number of timesteps doesn't reach to ```H``` in some trajectory, the exceeded timestep's elements in the state's tensor are filled with ```IGNORE_STATE=2500```, and the action's tensor are filled with ```IGNORE_ACTION=4```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98c5490d-37d6-42fb-ab28-5573ff41a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The index 4 is an ignore-action.\n",
    "# (0, 1, 2, 3 are valid actions.)\n",
    "IGNORE_ACTION = ACTION_SIZE\n",
    "# The index 2500 is an ignore-state.\n",
    "# (0, 1, ... , 2499 are valid states.)\n",
    "IGNORE_STATE = STATE_SIZE\n",
    "\n",
    "def create_trajectories(traj_num, horizen, action_prob_pi):\n",
    "    # initialize results\n",
    "    actions = torch.ones(traj_num, horizen, dtype=int).to(device) * IGNORE_ACTION\n",
    "    states = torch.ones(traj_num, horizen+1, dtype=int).to(device) * IGNORE_STATE\n",
    "\n",
    "    # collect data\n",
    "    for traj in range(traj_num):\n",
    "        # initialize episode\n",
    "        done = False\n",
    "        s = env.reset()\n",
    "        states[traj, 0] = s\n",
    "        # do until the episode ends\n",
    "        for step in range(horizen):\n",
    "            # pick up action using above action_prob_pi\n",
    "            a = torch.multinomial(action_prob_pi[s], 1, replacement=True).squeeze(dim=0)\n",
    "            actions[traj, step] = a\n",
    "            # step to the next state\n",
    "            s, _, done = env.step(a, s)\n",
    "            states[traj, step+1] = s\n",
    "            # exit loop when it's done\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    return states, actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2588e1cc-c2cb-4b06-8642-328fcba88aee",
   "metadata": {},
   "source": [
    "The generated policy ```action_prob_pi``` is an uniform distribution, and it won't get optimal trajectories unlike expert distribution.<br>\n",
    "As you can see below, it can reach to the goal state and the length of trajectory is then small in the expert distribution. On contrary, the length of trajectory is large in the uniform distribution $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fc8985f-ef10-42e3-b42e-f6916cd530c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** length (timestep size) of trajectories picked up by pi *****\n",
      "tensor([201, 201, 201, 201, 201, 201, 201, 201, 201, 201], device='cuda:0')\n",
      "***** length (timestep size) of trajectories picked up by expert *****\n",
      "tensor([14, 66, 34, 51, 77, 81, 70, 10, 47, 43], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_states, test_actions = create_trajectories(10, 200, action_prob_pi)\n",
    "print(\"***** length (timestep size) of trajectories picked up by pi *****\")\n",
    "print(torch.count_nonzero(test_states != IGNORE_STATE, dim=1))\n",
    "test_states, test_actions = create_trajectories(10, 200, action_prob_exp)\n",
    "print(\"***** length (timestep size) of trajectories picked up by expert *****\")\n",
    "print(torch.count_nonzero(test_states != IGNORE_STATE, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a43fb7-fb00-4839-a085-f07aeb685e84",
   "metadata": {},
   "source": [
    "### 4. Create a matrix used to get features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cbd85d-19a9-4431-bd9a-c5f769c4e267",
   "metadata": {},
   "source": [
    "In order to get features in trajectory, we can't use ```torch.nn.functional.one_hot()```, because we have ```IGNORE_STATE```. (See above.)<br>\n",
    "When it's ```IGNORE_STATE```, we want to convert the state into the feature, in which elements are all zeros.\n",
    "\n",
    "To do this, we now create a feature-mapping tensor $F$ with the shape ```[STATE_SIZE + 1, STATE_SIZE]```, in which ```i```-th row has state's feature for the state id=```i```. With this mapping tensor, ```F[IGNORE_STATE,:]``` becomes ```IGNORE_STATE``` feature, which elements are all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "812c0ab5-daec-46e3-bdeb-95c54d0a53b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_features = F.one_hot(torch.arange(STATE_SIZE).to(device), num_classes=STATE_SIZE)\n",
    "ignore_state_feature = torch.zeros(STATE_SIZE).to(device).unsqueeze(dim=0)\n",
    "state_features = torch.cat((state_features, ignore_state_feature))\n",
    "\n",
    "state_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83b5db8-e5ed-4702-9d59-b3f6bd722a6e",
   "metadata": {},
   "source": [
    "Suppose, $\\mathcal{T}^{\\verb|sample|}$ is a state's tensor which shape is ```[N, H]```, where ```N``` is the number of trajectories and ```H``` is the number of timestep (i.e, horizon).<br>\n",
    "By using mapping tensor $F$, you can get its state's features (which shape is ```[N, H, K]``` where ```K``` is the dimension of features) by $F [\\mathcal{T}^{\\verb|sample|}]$.\n",
    "\n",
    "For instance, the following state's matrix is converted into the following state's feature tensor, which shape is ```[2, 3, 2500]```. (Especially, the ```IGNORE_STATE``` state has the feature which elements are all zeros.)\n",
    "\n",
    "$\\displaystyle \\mathcal{T}^{\\verb|sample|} = \\begin{bmatrix} 0 & 1 & 2498 \\\\ 2498 & 2 & \\verb|IGNORE_STATE| \\\\ \\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79f3b45a-37bc-4df2-a5f8-fbc2e05a8620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_trajectory_states = torch.tensor([\n",
    "    [0,    1, 2498],\n",
    "    [2498, 2, IGNORE_STATE]]).to(device)\n",
    "test_state_features = state_features[test_trajectory_states]\n",
    "test_state_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b263d9-e40e-45e5-b924-cfd98384b933",
   "metadata": {},
   "source": [
    "With state feature $f_i(s_t)$, the feature of trajectory $\\tau$ (which shape is ```[2, 2500]```) is then obtained by $f_i^{\\tau} = \\sum_t \\gamma^t f_i(s_t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b417fa7-1dbb-4562-bc89-9239a3776e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.9900, 0.0000,  ..., 0.0000, 0.9801, 0.0000],\n",
       "        [0.0000, 0.0000, 0.9900,  ..., 0.0000, 1.0000, 0.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_discounted_traj_feature(test_state_features, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58afb5b-d6bf-4fce-b75c-999aa5e71130",
   "metadata": {},
   "source": [
    "### 5. Create a matrix used to get the joint probability $U(\\tau)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05800cde-9971-443a-9bfe-ca1aea794761",
   "metadata": {},
   "source": [
    "Now I create a probability matrix (tensor) to get $U(\\tau)$.<br>\n",
    "This matrix is almost same as ```action_prob_exp``` (an expert's state-action probability matrix), but it should handle special index, ```IGNORE_STATE``` and ```IGNORE_ACTION```.\n",
    "\n",
    "First, we create a matrix by extending the shape of ```action_prob_exp``` (which is ```[STATE_SIZE, ACTION_SIZE]```) to the shape ```[STATE_SIZE, ACTION_SIZE + 1]```, in which the action in ```IGNORE_ACTION``` column always has probability ```1.0```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2e77560-4f37-4238-95b3-606bcbe25f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
       "        ...,\n",
       "        [1.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
       "        [0.5000, 0.0000, 0.0000, 0.5000, 1.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 1.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ignore_actions_all = torch.ones(STATE_SIZE).to(device).unsqueeze(dim=1)\n",
    "probmat_u = torch.cat((action_prob_exp, ignore_actions_all), dim=1)\n",
    "\n",
    "probmat_u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631c2459-b894-4d35-bc4a-8b56a27aad42",
   "metadata": {},
   "source": [
    "We also add the action probabilities of ```IGNORE_STATE``` state, which elements are also all ```1.0```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4c9d94e-6dc8-4d90-9822-36bca5789485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
       "        ...,\n",
       "        [0.5000, 0.0000, 0.0000, 0.5000, 1.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ignore_state_actions = torch.ones(ACTION_SIZE + 1).to(device).unsqueeze(dim=0)\n",
    "probmat_u = torch.cat((probmat_u, ignore_state_actions))\n",
    "\n",
    "probmat_u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f7412e-72cf-48f0-a4ff-78e3a87e8689",
   "metadata": {},
   "source": [
    "Now I denote the above matrix (tensor) ```probmat_u``` as ${\\verb|Pr|}^{\\verb|exp|}$.\n",
    "\n",
    "Suppose, $\\mathcal{T}^{\\verb|sample|}_s$ is one-hot state's matrix, which shape is ```[N, H, S]```, where ```N``` is the number of trajectories, ```H``` is the number of timestep (i.e, horizon), and ```S``` is the number of states. (I note that ```S``` is ```STATE_SIZE+1```, which includes ```IGNORE_STATE```.)<br>\n",
    "Also, suppose,  $\\mathcal{T}^{\\verb|sample|}_a$ is the corresponding one-hot action's matrix, which shape is ```[N, H, A]```, where ```A``` is the number of actions. (I note that ```A``` is ```ACTION_SIZE+1```, which includes ```IGNORE_ACTION```.)\n",
    "\n",
    "In this assumption, the probabilities of taken actions in each states are obtained by the following equation.<br>\n",
    "I note that the following dot product operation is performed in each trajectories. :\n",
    "\n",
    "$\\displaystyle \\left( \\mathcal{T}^{\\verb|sample|}_s {\\verb|Pr|}^{\\verb|exp|} \\right) \\cdot \\mathcal{T}^{\\verb|sample|}_a $\n",
    "\n",
    "For instance, if the actions ```[[2,IGNORE_ACTION,3],[1,2,3]]``` is taken in the states ```[[0,1,2498],[2498,2,IGNORE_STATE]]```, each probabilties of the taken actions become ```[[0.2500, 1.0000, 0.5000],[0.0000, 0.2500, 1.0000]]``` by the following computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fe3e53d-cea8-4af1-8a8c-5d67011dee04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 1, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 1, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 1, 0, 0],\n",
       "         [0, 0, 1,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 1]]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_onehot = F.one_hot(torch.tensor([\n",
    "    [0,    1, 2498],\n",
    "    [2498, 2, IGNORE_STATE]]).to(device), num_classes=STATE_SIZE+1)\n",
    "\n",
    "states_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f47500d0-dc5c-40c6-8eea-981db5b297c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 1, 0]],\n",
       "\n",
       "        [[0, 1, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 1, 0]]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_onehot = F.one_hot(torch.tensor([\n",
    "    [2, IGNORE_ACTION, 3],\n",
    "    [1, 2,             3]]).to(device), num_classes=ACTION_SIZE+1)\n",
    "\n",
    "actions_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03195424-2625-4aa1-8ae4-a5eeecadb0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
       "         [0.5000, 0.0000, 0.0000, 0.5000, 1.0000]],\n",
       "\n",
       "        [[0.5000, 0.0000, 0.0000, 0.5000, 1.0000],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_probs_in_each_states = torch.matmul(states_onehot.double(), probmat_u)\n",
    "\n",
    "action_probs_in_each_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2731ff1-f8ed-4d9a-b690-9d14f64aceac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2500, 1.0000, 0.5000],\n",
       "        [0.0000, 0.2500, 1.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_for_taken_actions = torch.sum(torch.mul(action_probs_in_each_states, actions_onehot.double()), dim=2)\n",
    "\n",
    "probs_for_taken_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3182fa19-b100-4862-9513-d898ed2d902c",
   "metadata": {},
   "source": [
    "Then you can now easily get the joint probability $U(\\tau)$ in each trajectory by product operation as follows.\n",
    "\n",
    "> Note : The joint probability (by an expert) $U(\\tau)$ will vanish if the number of horizon ```H``` is large.<br>\n",
    "> So, in the training, I will first compute the elements in $\\frac{U(\\tau)}{\\pi(\\tau)}$, and then get the joint probability of these elements.\n",
    "\n",
    "> Note : If trajectory $\\tau$ is never visited by an expert (i.e, the probability is zero), its joint probability $U(\\tau)$ will become zero. As you saw above, this is prevented by picking up trajectorie's samples with above ```action_prob_pi```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b0ea78b-45b9-4afc-aa9f-583ad615c689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1250, 0.0000], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_probs = torch.prod(probs_for_taken_actions, dim=1)\n",
    "\n",
    "joint_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ccc335-f0b1-4c5d-bd85-2ac49f5bf311",
   "metadata": {},
   "source": [
    "### 6. Create a matrix used to get the joint probability $\\pi(\\tau)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5637032-6931-46c7-9745-c7a6aeee416a",
   "metadata": {},
   "source": [
    "We also create a state-action probability matrix to get the joint probability $\\pi(\\tau)$.<br>\n",
    "This matrix is almost same as ```action_prob_pi``` (uniform probability matrix), but it should also handle special index, ```IGNORE_STATE``` and ```IGNORE_ACTION```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7d5a40c-d3cb-48c0-8285-c44a586b3bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
       "        ...,\n",
       "        [0.5000, 0.0000, 0.0000, 0.5000, 1.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add IGNORE_ACTION with probability 1.0\n",
    "probmat_pi = torch.cat((action_prob_pi, ignore_actions_all), dim=1)\n",
    "# add IGNORE_STATE with probability 1.0\n",
    "probmat_pi = torch.cat((probmat_pi, ignore_state_actions))\n",
    "\n",
    "probmat_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5060be-29a4-47cb-ba2e-beb26ad04bf0",
   "metadata": {},
   "source": [
    "When I denote the matrix ```probmat_pi``` as ${\\verb|Pr|}^{\\pi}$, the probabilities of taken actions in each states are also obtained by the following equation.\n",
    "\n",
    "$\\displaystyle \\left( \\mathcal{T}^{\\verb|sample|}_s {\\verb|Pr|}^{\\pi} \\right) \\cdot \\mathcal{T}^{\\verb|sample|}_a $\n",
    "\n",
    "Then you can also easily get the joint probability $\\pi(\\tau)$ by the product operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d986380d-79b0-4018-b3f3-98a7fe0bc816",
   "metadata": {},
   "source": [
    "### 7. Create a function to get $\\sum_{\\tau \\in \\mathcal{T}} P(\\tau|\\theta) f_i^{\\tau}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403f574a-3549-4c58-afc0-dcb105cffd2e",
   "metadata": {},
   "source": [
    "Now we're ready to create a function to get the second term in equation (2).\n",
    "\n",
    "In our example (GridWorld), we know the transition probability $p(s_{t+1}|s_t,a_t)$ and don't need to apply importance sampling.<br>\n",
    "But in this example, I assume that the transition probability is unknown and I'll then apply importance sampling as follows.\n",
    "\n",
    "$\\displaystyle \\sum_{\\tau \\in \\mathcal{T}} P(\\tau|\\theta) f_i^{\\tau} \\simeq \\frac{\\sum_{\\tau \\in \\mathcal{T}_N^{\\pi}} \\frac{U(\\tau)}{\\pi(\\tau)} \\exp \\left( \\sum_{j=1}^k \\theta_j f_j^{\\tau} \\right) f_i^{\\tau}}{\\sum_{\\tau \\in \\mathcal{T}_N^{\\pi}} \\frac{U(\\tau)}{\\pi(\\tau)} \\exp \\left( \\sum_{j=1}^k \\theta_j f_j^{\\tau} \\right)} $\n",
    "\n",
    "To avoid vanishing of the joint probability $U(\\tau)$ and $\\pi(\\tau)$, I don't directly compute $U(\\tau)$ and $\\pi(\\tau)$.<br>\n",
    "Instead, I'll compute the elements of $\\frac{U(\\tau)}{\\pi(\\tau)}$ in every actions and then I'll apply product's operation to get the joint probabilities of $\\frac{U(\\tau)}{\\pi(\\tau)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93239d43-13a8-45dc-bc76-604023b247cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_second_term(visited_states, taken_actions, probmat_u, probmat_pi, theta, gamma):\n",
    "    # compute f_i^{\\tau} -- shape:(N, STATE_SIZE)\n",
    "    traj_state_features = state_features[visited_states]\n",
    "    feature_term = get_discounted_traj_feature(traj_state_features, gamma)\n",
    "    # compute exp(\\sum_{j=1}^k \\theta_j f_j^{\\tau}) -- shape:(N,)\n",
    "    exp_seed = torch.sum(torch.mul(feature_term, theta.unsqueeze(dim=0)), dim=1)\n",
    "    exp_term = torch.exp(exp_seed - torch.max(exp_seed))  # to prevent from being inf\n",
    "    # create elements (probabilities in every actions) of U(\\tau) -- shape:(N,H)\n",
    "    states_onehot = F.one_hot(visited_states[:,:-1], num_classes=STATE_SIZE+1)\n",
    "    actions_onehot = F.one_hot(taken_actions, num_classes=ACTION_SIZE+1)\n",
    "    action_probs_in_each_states_u = torch.matmul(states_onehot.double(), probmat_u)\n",
    "    probs_for_taken_actions_u = torch.sum(torch.mul(action_probs_in_each_states_u, actions_onehot.double()), dim=2)\n",
    "    # create elements (probabilities in every actions) of \\pi(\\tau) -- shape:(N,H)\n",
    "    action_probs_in_each_states_pi = torch.matmul(states_onehot.double(), probmat_pi)\n",
    "    probs_for_taken_actions_pi = torch.sum(torch.mul(action_probs_in_each_states_pi, actions_onehot.double()), dim=2)\n",
    "    # create elements (probability ratio in every actions) of U(\\tau) / \\pi(\\tau) -- shape:(N,H)\n",
    "    probs_ratio = torch.div(probs_for_taken_actions_u, probs_for_taken_actions_pi)\n",
    "    # compute U(\\tau) / \\pi(\\tau) (i.e, math product of elements) -- shape:(N,)\n",
    "    u_pi_ratio = torch.prod(probs_ratio, dim=1)\n",
    "    # compute denominator (sum up with trajectories) -- shape:()\n",
    "    elems = torch.mul(u_pi_ratio, exp_term)\n",
    "    denom = torch.sum(elems)\n",
    "    # compute numerator (sum up with trajectories) -- shape:(STATE_SIZE,)\n",
    "    norm = torch.sum(torch.mul(feature_term, elems.unsqueeze(dim=-1)), dim=0)\n",
    "    # final result -- shape:(STATE_SIZE,)\n",
    "    return torch.div(norm, denom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a923d1ae-77db-4641-8fd2-056bc4116d37",
   "metadata": {},
   "source": [
    "### 8. Set threshold $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f57925e-d298-4d3a-b752-eb02b046c8ee",
   "metadata": {},
   "source": [
    "Now we get the following Hoeffding bound (thereshold) $\\epsilon$.\n",
    "\n",
    "$\\displaystyle \\epsilon_i = \\sqrt{\\frac{-\\ln{(1-\\delta)}}{2N}} \\frac{\\gamma^{H+1}-1}{\\gamma-1} \\left( \\max_s f_i(s) - \\min_s f_i(s) \\right) $\n",
    "\n",
    "where $N$ is the number of sampled trajectories and $\\delta$ is a confidence probability.\n",
    "\n",
    "> Note : As I have mentioned above, when the number of the taken actions is ```H```, the number of the visited states becomes ```H+1```.\n",
    "\n",
    "In this example, I empirically set ```0.0001``` as a confidence parameter $\\delta$. (So the probability that the difference between the feature counts given\n",
    "by the distribution $P$ and the true feature counts of the expert's policy is larger than $2\\epsilon$ is ```0.0001```.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddca0048-3027-46ee-b063-e0b900e8f381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Epsilon without weighted sum *****\n",
      "tensor([0.0022, 0.0022, 0.0022,  ..., 0.0022, 0.0022, 0.0022], device='cuda:0')\n",
      "***** Epsilon with weighted sum *****\n",
      "tensor([0.0599, 0.0599, 0.0599,  ..., 0.0599, 0.0599, 0.0599], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "N = 10           # the number of trajectorie's samples\n",
    "H = 30           # the number of horizon\n",
    "delta = 0.0001   # confidence parameter\n",
    "\n",
    "epsilon = torch.sqrt(-torch.log(torch.ones(STATE_SIZE).to(device) - delta) / (2.0*N))\n",
    "print(\"***** Epsilon without weighted sum *****\")\n",
    "print(epsilon)\n",
    "epsilon = epsilon * (gamma**(H+1) - 1.0) / (gamma - 1.0)\n",
    "print(\"***** Epsilon with weighted sum *****\")\n",
    "print(epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5559ddc6-463f-48fa-8ad1-be7395a5eb9f",
   "metadata": {},
   "source": [
    "### 9. Put it all together (Train and optimize parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25ff582-b646-4d73-bfff-4d795f3d371d",
   "metadata": {},
   "source": [
    "Now we compute the following gradient and then optimize $\\theta$.\n",
    "\n",
    "$\\displaystyle \\frac{\\partial}{\\partial \\theta_i}g(\\theta)=\\hat{f}_i - \\sum_{\\tau \\in \\mathcal{T}} P(\\tau|\\theta) f_i^{\\tau} - \\alpha_i \\epsilon_i$\n",
    "\n",
    "In this example, we directly update $\\theta$ by the gradient (not using backprop) in 2000 iterations as follows.\n",
    "\n",
    "> Note : Because we try to maximize the dual problem $g(\\theta)$ (not minimize loss), we then apply **gradient ascent**, instead of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "587e5005-c04d-48d4-ac58-1e72dfd0c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    theta,\n",
    "    gamma,\n",
    "    num_samples,\n",
    "    horizon,\n",
    "    probmat_u,\n",
    "    probmat_pi,\n",
    "    action_prob_pi,\n",
    "    epsilon,\n",
    "    start_iter, end_iter,\n",
    "    lr,\n",
    "    verbose=True):\n",
    "\n",
    "    for iter_num in range(start_iter, end_iter):\n",
    "        # create N sample trajectories with pi\n",
    "        state_samples, action_samples = create_trajectories(num_samples, horizon, action_prob_pi)\n",
    "        # get \\sum_{\\tau} P(\\tau|\\theta) f_i^{\\tau}\n",
    "        second_term = get_second_term(state_samples, action_samples, probmat_u, probmat_pi, theta, gamma)\n",
    "        # get alpha\n",
    "        alpha = torch.sign(torch.where(theta == 0.0, 1.0, theta))\n",
    "        # compute gradient of g(\\theta)\n",
    "        nabla = f_exp - second_term - alpha * epsilon\n",
    "\n",
    "        # update theta (see above note)\n",
    "        theta += nabla * lr\n",
    "\n",
    "        # (output logs)\n",
    "        if verbose:\n",
    "            t_mean = torch.mean(torch.abs(nabla)).tolist()\n",
    "            t_max = torch.max(torch.abs(nabla)).tolist()\n",
    "            print(\"iter{}: nabla - average: {:1.4f}  max: {:2.4f}\".format(iter_num, t_mean, t_max), end=\"\\r\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92b87ec2-ae2c-4e00-bdfc-351c06294ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter1999: nabla - average: 0.0744  max: 2.8313\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters (theta)\n",
    "theta = torch.empty(STATE_SIZE).to(device)\n",
    "torch.nn.init.uniform_(theta, a=0.0, b=1.0)\n",
    "\n",
    "# optimize theta\n",
    "train(\n",
    "    theta=theta,\n",
    "    gamma=gamma,\n",
    "    num_samples=N,\n",
    "    horizon=H,\n",
    "    probmat_u=probmat_u,\n",
    "    probmat_pi=probmat_pi,\n",
    "    action_prob_pi=action_prob_pi,\n",
    "    epsilon=epsilon,\n",
    "    start_iter=0,\n",
    "    end_iter=2000,\n",
    "    lr=0.005,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5a7d5d-9af2-461a-af9e-08a1f6f8ea01",
   "metadata": {},
   "source": [
    "Now let's see the heatmap of the trained $\\theta$.<br>\n",
    "As you can see below, the states on the path toward the goal state are hot in this map.\n",
    "\n",
    "> Note : Strictly speaking, the reward in state $s_i$ is $\\theta^T \\cdot f(s_i)$.<br>\n",
    "> But in this example, the state's feature is just a one-hot vector, and I have then simply used $\\theta$ to check results.\n",
    "\n",
    "> Note : In the area of the following yellow spot (extremely hot), the expert doesn't also work well (hence the expert agent will walk around this area in order not to lose the reward), and then the visited frequencies in this area is becoming so large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac34a806-8286-4241-9d6b-3aaea5177002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "state_vals = torch.reshape(theta, (GRID_SIZE, GRID_SIZE))\n",
    "plt.imshow(state_vals.cpu().numpy(), cmap=\"hot\", interpolation=\"nearest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747e7d18-ab7f-4e05-a61a-6625e55f2790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
