{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f35fd98-8613-45a2-959b-e5da21d9d4d3",
   "metadata": {},
   "source": [
    "# Generative Adversarial Imitation Learning (GAIL)\n",
    "\n",
    "Generative Adversarial Imitation Learning (GAIL) is the method inspired by IRL (Inverse Reinforcement Learning) and GANs (Generative Adversarial Networks), in which discriminator network is used to distinguish true data from false data.\n",
    "\n",
    "Unlike previous IRL methods, GAIL constrains the behavior of the agent to be approximately optimal without explicitly recovering the reward's function, i.e, it scales an IRL approach by bypassing intermediate IRL steps (bypassing steps for learning a cost or reward function).<br>\n",
    "It directly extracts a policy from data, **as if it were obtained by reinforcement learning following inverse\n",
    "reinforcement learning**. (RL algorithms discussed in [here](https://github.com/tsmatz/reinforcement-learning-tutorials) can also be applied in GAIL.)\n",
    "\n",
    "Up until now, we assumed that the rewards $\\mathbf{w}$ is linear to the feature of trajectory, $\\verb|reward| = \\mathbf{w}^T \\cdot \\phi(\\tau)$.<br>\n",
    "GAIL, however, doesn't need this constraint, and can then imitate arbitrarily complex expert behaviors.\n",
    "\n",
    "GAIL is one of SOTA (state-of-the-art) imitation learning algorithm.\n",
    "\n",
    "*(back to [index](https://github.com/tsmatz/imitation-learning-tutorials/))*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bb5d64-6e29-413c-a95b-6b1678611cb0",
   "metadata": {},
   "source": [
    "## Overview of Generative Adversarial Imitation Learning (GAIL) method\n",
    "\n",
    "Let's briefly follow theoretical aspects behind GAIL algorithm along with the original paper [[Ho and Ermon, 2016](https://arxiv.org/pdf/1606.03476)].\n",
    "\n",
    "As we have discussed in [previous examples](./03_maxent_irl.ipynb), Maximum Entropy IRL (and succeeding Maximum Causal Entropy IRL) finds a cost function (see below note) to maximize the entropy $-H(\\pi)$ and expectation.<br>\n",
    "IRL operation in abstraction can then be written as :\n",
    "\n",
    "$\\displaystyle IRL(\\pi_E) = \\max_{c \\in \\mathcal{C}} \\left( \\min_{\\pi \\in \\Pi} \\left( -H(\\pi) + \\mathbb{E}_{\\pi}[c(s,a)] \\right) - \\mathbb{E}_{\\pi_E}[c(s,a)] \\right)$\n",
    "\n",
    "where $\\mathcal{C}$ is a set of cost functions.\n",
    "\n",
    "> Note : Unlike previous [Maximum Entropy IRL example](./03_maxent_irl.ipynb), the function $c()$ is not a reward function, but a cost function. The cost function is **minimized (not maximized)** for optimization.\n",
    "\n",
    "To prevent from [overfitting](https://tsmatz.wordpress.com/2017/09/13/overfitting-for-regression-and-deep-learning/) in machine learning problems, we introduce a regularizer, $\\psi : \\mathbb{R}^{\\mathcal{S} \\times \\mathcal{A}} \\to \\mathbb{R} \\cup \\{ \\infty \\}$, and apply in this equation. :\n",
    "\n",
    "$\\displaystyle IRL_{\\psi}(\\pi_E) = \\arg \\max_{c \\in \\mathbb{R}^{\\mathcal{S} \\times \\mathcal{A}}} \\left( -\\psi(c) + \\min_{\\pi \\in \\Pi} \\left( -H(\\pi) + \\mathbb{E}_{\\pi}[c(s,a)] \\right) - \\mathbb{E}_{\\pi_E}[c(s,a)] \\right) \\;\\;\\;\\;\\;\\; (1) $\n",
    "\n",
    "Now we introduce the occupancy measure which is often used in the theory of RL.\n",
    "\n",
    "First, we define the occupancy measure $\\rho_{\\pi}(s, a) : \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}$ for $\\pi$ as :\n",
    "\n",
    "$\\displaystyle \\rho_{\\pi}(s, a) \\coloneqq \\pi(a | s) \\sum_{t=0}^{\\infty} \\gamma^t P(s_t=s|\\pi) $\n",
    "\n",
    "> Note : $P(s_t=s|\\pi)$ is the probability of landing in state $s$.\n",
    "\n",
    "In abstraction, the occupancy measure is the probability of state's occurrence when navigating the environment with policy $\\pi$.\n",
    "\n",
    "When we denote a set of valid occupancy measures $\\{ \\rho_{\\pi} : \\pi \\in \\Pi \\}$ as $\\mathcal{D}$, it's known that there is a one-to-one correspondence between $\\Pi$ and $\\mathcal{D}$. (i.e, When some $\\rho$ is given, the corresponding $\\pi$ to satisfy $\\rho = \\rho_\\pi$ is uniquely determined.)\n",
    "\n",
    "Using occupancy measures, it's known that to find the optimal $\\pi$ under a cost function obtained in (1) is equivalent to :\n",
    "\n",
    "$\\displaystyle RL \\circ IRL_{\\psi}(\\pi_E) = \\arg \\min_{\\pi \\in \\Pi} \\left( -H(\\pi) + \\psi^* \\left( \\rho_{\\pi} - \\rho_{\\pi_E} \\right) \\right) \\;\\;\\;\\;\\;\\; (2)$\n",
    "\n",
    "where $\\psi^* : \\mathbb{R}^{\\mathcal{S} \\times \\mathcal{A}} \\to \\mathbb{R} \\cup \\{ \\infty \\}$ is the [convex conjugate](https://en.wikipedia.org/wiki/Convex_conjugate) of $\\psi$.\n",
    "\n",
    "Especially, when $\\psi$ is constant and $\\pi$ is then obtained by (2), it's known that $\\rho_{\\pi} = \\rho_{\\pi_E}$.\n",
    "\n",
    "> Note : See [original paper](https://arxiv.org/pdf/1606.03476) and its Appendix A.1 for this proof.\n",
    "\n",
    "This implicitly means that $\\psi$-regularized IRL is to seek a policy whose occupancy measure is close to the expert, as measured by the convex function $\\psi^*$. If $\\psi$ is constant, the solution of (2) simply matches occupancy measures with expert at all states and actions.<br>\n",
    "Here I don't go so far into details, but it's also known that the expectation matching with linear cost function (which are discussed in previous examples) is the special case of above (2). (See apprenticeship learning in [original paper](https://arxiv.org/pdf/1606.03476) for details.)\n",
    "\n",
    "Now let $\\psi_{GA}()$ be the following regularization function. :\n",
    "\n",
    "$\\displaystyle \\psi_{GA}(c) \\coloneqq \\mathbb{E}_{\\pi_E}[g(c(s,a))] $  when $c \\lt 0$\n",
    "\n",
    "$\\displaystyle \\psi_{GA}(c) \\coloneqq +\\infty $  otherwise\n",
    "\n",
    "where\n",
    "\n",
    "$\\displaystyle g(x) \\coloneqq -x-\\log(1-e^x) $  when $x \\lt 0$\n",
    "\n",
    "$\\displaystyle g(x) \\coloneqq +\\infty $  otherwise\n",
    "\n",
    "This regularization forces that the cost function $c$ is always negative, and penalitizes for small $c$ and large $c$ (close to zero), because the plot of function $g(x)$ is as follows. :\n",
    "\n",
    "<img src=\"./assets/regularization_plot.png\" alt=\"Regularization Plot\" width=\"300\"/>\n",
    "\n",
    "Unlike linear cost functions (or linear reward functions) seen in previous IRL examples, $\\psi_{GA}()$ allows for any cost function, as long as it is negative everywhere, and $RL \\circ IRL_{\\psi}(\\pi_E)$ can be fitted to more complex cost estimation.\n",
    "\n",
    "Under this assumption, it's known that it holds the following equation. (See [original paper](https://arxiv.org/pdf/1606.03476) for the proof.)\n",
    "\n",
    "$\\displaystyle \\psi_{GA}^* (\\rho_{\\pi} - \\rho_{\\pi_E}) = \\max_{D \\in (0,1)^{\\mathcal{S} \\times \\mathcal{A}}} \\left( \\mathbb{E}_{\\pi} [\\log(D(s, a))] + \\mathbb{E}_{\\pi_E} [\\log(1 - D(s, a))] \\right) \\;\\;\\;\\;\\;\\; (3) $\n",
    "\n",
    "where $D$ is discriminative classifier $D : \\mathcal{S} \\times \\mathcal{A} \\to (0,1)$.\n",
    "\n",
    "Now we assume that the lerner's policy $\\pi$ is parameterized by $\\theta$ (i.e, $\\pi = \\pi_{\\theta}$) and the discriminator network $D$ is parameterized by $w$ (i.e, $D = D_w$).\n",
    "\n",
    "By above (2) and (3), our algorithm (GAIL) is to alternately update $w$ and $\\theta$ as follows. :\n",
    "\n",
    "- Update $w$ to increase $\\mathbb{E}_{\\pi} [\\log(D(s, a))] + \\mathbb{E}_{\\pi_E} [\\log(1 - D(s, a))]$.\n",
    "- Update $\\theta$ to decrease $\\mathbb{E}_{\\pi} [\\log(D(s, a))] - \\lambda H(\\pi) $ using the updated discriminator $D(s,a)$, where $\\lambda \\geq 0$ is a controlling coefficient.\n",
    "\n",
    "In [original paper](https://arxiv.org/pdf/1606.03476), Adam gradient step is applied to update $w$, and TRPO (trust region policy optimization) algorithm is used to update $\\theta$ in order to avoid unstable policy updates.<br>\n",
    "As you'll see later, we'll however use PPO (proximal policy optimization) instead of TRPO in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2887022-1916-46a6-aa93-cf26c322c293",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e686e301-bc83-421a-aff1-6ce67283f4ab",
   "metadata": {},
   "source": [
    "### 1. Restore environment and load expert's data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c802b3dd-8ce5-415a-90b3-7ccbb028d4c4",
   "metadata": {},
   "source": [
    "Before we start, we need to install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e202432-fb4e-4065-b7e3-4e5a3b066d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5260fae-0aea-4077-a1b4-ee164ab296ad",
   "metadata": {},
   "source": [
    "Firstly, I restore GridWorld environment from JSON file. (See [this script](./00_generate_expert_trajectories.ipynb) for generating the same environment.)\n",
    "\n",
    "For details about this environment, see [Readme.md](https://github.com/tsmatz/imitation-learning-tutorials/blob/master/Readme.md).\n",
    "\n",
    "I note that I'll apply temporal difference (TD) learning (see [here](https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/03-actor-critic.ipynb)) in this example, and the difference between termination and truncation is important.<br>\n",
    "Thus, unlike previous examples, the truncation is not performed in this environment, and I'll handle the truncation during the training.\n",
    "\n",
    "> Note : All members are implemented as PyTorch tensor operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ce850d3-00a6-41af-9d3e-84ebbff0132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" All members are tensor operations. \"\"\"\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "GRID_SIZE = 50\n",
    "MAX_TIMESTEP = 200\n",
    "\n",
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    This environment is motivated by the following paper.\n",
    "    https://proceedings.mlr.press/v15/boularias11a/boularias11a.pdf\n",
    "\n",
    "    - It has 50 x 50 grids (cells).\n",
    "    - The agent has four actions for moving in one of the directions of the compass.\n",
    "    - If ```transition_prob``` = True, the actions succeed with probability 0.7,\n",
    "      a failure results in a uniform random transition to one of the adjacent states.\n",
    "    - A reward of 10 is given for reaching the goal state, located on the bottom-right corner.\n",
    "    - For the remaining states,\n",
    "      the reward function was randomly set to 0 with probability 2/3\n",
    "      and to −1 with probability 1/3.\n",
    "    - If the agent moves across the border, it's given the fail reward (i.e, reward=`-1`).\n",
    "    - The initial state is sampled from a uniform distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reward_map, valid_states, transition_prob=True):\n",
    "        \"\"\"\n",
    "        Initialize class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        reward_map : float[GRID_SIZE * GRID_SIZE]\n",
    "            Reward for each state.\n",
    "        valid_states : list(int[2])\n",
    "            List of states, in which the agent can reach to goal state without losing any rewards.\n",
    "            Each state is a 2d vector, [row, column].\n",
    "            When you call reset(), the initial state is picked up from these states.\n",
    "        transition_prob : bool\n",
    "            True if transition probability (above) is enabled.\n",
    "            False when we generate an expert agent without noise.\n",
    "        \"\"\"\n",
    "        self.reward_map = torch.tensor(reward_map).to(device)\n",
    "        self.valid_states = torch.tensor(valid_states).to(device)\n",
    "        self.transition_prob = transition_prob\n",
    "\n",
    "    def reset(self, batch_size):\n",
    "        \"\"\"\n",
    "        Randomly, get initial state (single state) from valid states.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : int\n",
    "            The number of returned states.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        state : int[batch_size]\n",
    "            Return the picked-up state id.\n",
    "        \"\"\"\n",
    "        # initialize step count\n",
    "        self.step_count = 0\n",
    "        # pick up sample of valid states\n",
    "        indices = torch.multinomial(torch.ones(len(self.valid_states)).to(device), batch_size, replacement=True)\n",
    "        state_2d = self.valid_states[indices]\n",
    "        # convert 2d index to 1d index\n",
    "        state_1d = state_2d[:,0] * GRID_SIZE + state_2d[:,1]\n",
    "        # return result\n",
    "        return state_1d\n",
    "\n",
    "    def step(self, actions, states):\n",
    "        \"\"\"\n",
    "        Take action, proceed step, and return the result.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        actions : int[batch_size]\n",
    "            Actions to take\n",
    "            (0=UP 1=DOWN 2=LEFT 3=RIGHT)\n",
    "        states : int[batch_size]\n",
    "            Current state id.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        new-states : int[batch_size]\n",
    "            New state id.\n",
    "        rewards : int[batch_size]\n",
    "            The obtained reward.\n",
    "        dones : bool[batch_size]\n",
    "            Flag to check whether it terminates.\n",
    "        \"\"\"\n",
    "        # get batch size\n",
    "        batch_size = actions.shape[0]\n",
    "        # if transition prob is enabled, apply transition\n",
    "        if self.transition_prob:\n",
    "            # the action succeeds with probability 0.7\n",
    "            prob = torch.ones(batch_size, 4).to(device)\n",
    "            mask = F.one_hot(actions, num_classes=4).bool()\n",
    "            prob = torch.where(mask, 7.0, prob)\n",
    "            selected_actions = torch.multinomial(prob, 1, replacement=True)\n",
    "            selected_actions = selected_actions.squeeze(dim=1)\n",
    "            action_onehot = F.one_hot(selected_actions, num_classes=4)\n",
    "        else:\n",
    "            action_onehot = F.one_hot(actions, num_classes=4)\n",
    "        # get 2d state\n",
    "        mod = torch.div(states, GRID_SIZE, rounding_mode=\"floor\")\n",
    "        reminder = torch.remainder(states, GRID_SIZE)\n",
    "        state_2d = torch.cat((mod.unsqueeze(dim=-1), reminder.unsqueeze(dim=-1)), dim=-1)\n",
    "        # move state\n",
    "        # (0=UP 1=DOWN 2=LEFT 3=RIGHT)\n",
    "        up_and_down = action_onehot[:,1] - action_onehot[:,0]\n",
    "        left_and_right = action_onehot[:,3] - action_onehot[:,2]\n",
    "        move = torch.cat((up_and_down.unsqueeze(dim=-1), left_and_right.unsqueeze(dim=-1)), dim=-1)\n",
    "        new_states = state_2d + move\n",
    "        # set reward\n",
    "        rewards = torch.zeros(batch_size).to(device)\n",
    "        rewards = torch.where(new_states[:,0] < 0, -1.0, rewards)\n",
    "        rewards = torch.where(new_states[:,0] >= GRID_SIZE, -1.0, rewards)\n",
    "        rewards = torch.where(new_states[:,1] < 0, -1.0, rewards)\n",
    "        rewards = torch.where(new_states[:,1] >= GRID_SIZE, -1.0, rewards)\n",
    "        # correct location\n",
    "        new_states = torch.clip(new_states, min=0, max=GRID_SIZE-1)\n",
    "        # if succeed, add reward of current state\n",
    "        states_1d = new_states[:,0] * GRID_SIZE + new_states[:,1]\n",
    "        rewards = torch.where(rewards>=0.0, rewards+self.reward_map[states_1d], rewards)\n",
    "        # return result\n",
    "        self.step_count += 1\n",
    "        # note : the truncation is not perfomed (see above)\n",
    "        return new_states[:,0] * GRID_SIZE + new_states[:,1], rewards, rewards==self.reward_map[GRID_SIZE * GRID_SIZE - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73d95082-f467-48f2-96a9-1150fe17e810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"gridworld.json\", \"r\") as f:\n",
    "    json_object = json.load(f)\n",
    "    env = GridWorld(**json_object, transition_prob=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09eeb91-78f4-42c3-9911-0ae54d357e0f",
   "metadata": {},
   "source": [
    "Load expert's data (demonstrations) which is saved in ```./expert_data``` folder in this repository.\n",
    "\n",
    "> Note : See [this script](./00_generate_expert_trajectories.ipynb) for generating expert dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3adf63df-208b-408b-ac5d-1365309016f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "dest_dir = \"./expert_data\"\n",
    "checkpoint_file = \"ckpt0.pkl\"\n",
    "\n",
    "# load expert data from pickle\n",
    "with open(f\"{dest_dir}/{checkpoint_file}\", \"rb\") as f:\n",
    "    exp_data = pickle.load(f)\n",
    "exp_states = torch.tensor(exp_data[\"states\"]).to(device)\n",
    "exp_actions = torch.tensor(exp_data[\"actions\"]).to(device)\n",
    "timestep_lens = exp_data[\"timestep_lens\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344eeff1-815d-4d0b-b616-a74c18216d6d",
   "metadata": {},
   "source": [
    "### 2. Create networks\n",
    "\n",
    "Now I define the following policy network $\\pi_{\\theta}$, value network, and discriminator network $D_w$.<br>\n",
    "In TRPO (also, PPO) implementation, not only the policy network, but the value network is also required.\n",
    "\n",
    "- Policy network : It receives one-hot state as input. It then returns the optimal action logits as output.\n",
    "- Value network : It receives one-hot state as input. It then returns a single value as output.\n",
    "- Discriminator network : It receives one-hot state and one-hot action as input. It then returns a value of range (0, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11ef1777-36be-48a0-b07a-b856ffa5fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "STATE_SIZE = GRID_SIZE*GRID_SIZE\n",
    "ACTION_SIZE = 4\n",
    "\n",
    "#\n",
    "# Define model\n",
    "#\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(STATE_SIZE, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, 4)\n",
    "\n",
    "    def forward(self, s):\n",
    "        outs = self.hidden(s)\n",
    "        outs = F.relu(outs)\n",
    "        logits = self.output(outs)\n",
    "        return logits\n",
    "\n",
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(STATE_SIZE, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, s):\n",
    "        outs = self.hidden(s)\n",
    "        outs = F.relu(outs)\n",
    "        value = self.output(outs)\n",
    "        return value\n",
    "\n",
    "class DiscriminatorNet(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(STATE_SIZE + ACTION_SIZE, hidden_dim)\n",
    "        self.hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.get_logits = nn.Linear(hidden_dim, 1)\n",
    "        self.get_sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, s):\n",
    "        outs = self.hidden1(s)\n",
    "        outs = F.relu(outs)\n",
    "        outs = self.hidden2(outs)\n",
    "        outs = F.relu(outs)\n",
    "        logits = self.get_logits(outs)\n",
    "        output = self.get_sigmoid(logits)\n",
    "        return output\n",
    "\n",
    "#\n",
    "# Generate model\n",
    "#\n",
    "policy_func = PolicyNet().to(device)\n",
    "value_func = ValueNet().to(device)\n",
    "discriminator = DiscriminatorNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b318a-2d41-402e-85a9-d12a22689eee",
   "metadata": {},
   "source": [
    "### 3. Create functions to get learner and expert data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38c67e1-2d0a-4cf7-9e62-a4c3f034b0d8",
   "metadata": {},
   "source": [
    "Now we create a function to get expert samples.<br>\n",
    "In this example, we get 6000 samples (6000 steps) at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cddcefb-8961-4646-bc51-cf127505a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import itertools\n",
    "\n",
    "num_samples = 6000\n",
    "\n",
    "expert_loader = DataLoader(\n",
    "    list(zip(exp_states, exp_actions)),\n",
    "    batch_size=num_samples,\n",
    "    shuffle=False,\n",
    ")\n",
    "expert_iter = iter(itertools.cycle(expert_loader))\n",
    "\n",
    "def get_data_by_expert(num_samples=num_samples):\n",
    "    data_length = 0\n",
    "    while data_length < num_samples:\n",
    "        states, actions = next(expert_iter)\n",
    "        data_length = len(states)\n",
    "    return states, actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3adda4-d147-4234-a80f-34791da215bb",
   "metadata": {},
   "source": [
    "Not only sampling by the expert, but sampling by policy $\\pi$ is also required, because we need the expectation $\\mathbb{E}_{\\pi}(\\cdot)$.\n",
    "\n",
    "In sampling by policy $\\pi$, we need values and advantages for applying TRPO (or PPO) algorithm.\n",
    "\n",
    "> Note : See [here](https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/03-actor-critic.ipynb) for theoretical aspects of values and advantages in RL.\n",
    "\n",
    "In this example, I'll use TD (temporal difference) and GAE (generalized advantage estimation) for getting values and advantages in TRPO (or PPO).\n",
    "\n",
    "Here I then briefly summarize TD and GAE.\n",
    "\n",
    "Suppose, $V(\\cdot)$ is the value network.<br>\n",
    "In TD (temporal difference), we get the value at time-step $t$ by $r_t + \\gamma V(s_{t+1})$, where $r_t$ is a real score at $t$. (I note that $r_t = \\log(D(s_t, a_t))$ in this example.)<br>\n",
    "We then get value's difference (between value and estimated value) $\\delta_t^V$ by :\n",
    "\n",
    "$\\displaystyle \\delta_t^V \\coloneqq r_t + \\gamma V(s_{t+1}) - V(s_t)$\n",
    "\n",
    "In [GAE (generalized advantage estimation)](https://arxiv.org/pdf/1506.02438), the advantage at $t$ is obtained by :\n",
    "\n",
    "$\\displaystyle \\hat{A}_t^{\\verb|GAE|} \\coloneqq \\sum_{l=0}^{\\infty} (\\gamma \\lambda_{\\verb|GAE|})^l \\delta_{t+l}^V$\n",
    "\n",
    "where $\\lambda_{\\verb|GAE|} \\in [0, 1]$ is a controlling parameter (between bias and variance) in GAE.\n",
    "\n",
    "When you set $\\lambda_{\\verb|GAE|}=1.0$, then $\\hat{A}_t^{\\verb|GAE|}$ is just a diffrence of values without generalization as follows. (See [original paper](https://arxiv.org/pdf/1506.02438) for details.)\n",
    "\n",
    "$\\displaystyle \\sum_{l=0}^{k - 1} \\gamma^l \\delta_{t+l}^V = -V(s_t) + r_t + \\gamma r_{t+1} + \\cdots + \\gamma^{k-1} r_{t+k-1} + \\gamma^k V(s_{t+k}) $\n",
    "\n",
    "It's worth noting that the rewards are never used in GAIL algorithm (because it's imitation learning !), but **here we use episode's reward (reward result) to evaluate how the agent is learned** in this example. (You won't use reward in practical GAIL algorithm, but here I use only for checking how well it's working.)\n",
    "\n",
    "> Note : As I have mentioned above, the truncation in environment is not performed in the environemnt. Instead, this function handles the truncation as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "793d5200-61dc-45de-b94c-119f4cae968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.995\n",
    "gae_lambda = 1.0\n",
    "\n",
    "def get_data_by_learner_policy(\n",
    "    env,\n",
    "    policy_net,\n",
    "    value_net,\n",
    "    discriminator_net,\n",
    "    gamma=gamma,\n",
    "    gae_lambda=gae_lambda,\n",
    "    num_samples=num_samples,\n",
    "    batch_size=128,\n",
    "):\n",
    "    \"\"\"\n",
    "    Collect samples with policy pi.\n",
    "    To speed up training, this function runs as a batch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : GridWorld\n",
    "        Environment class.\n",
    "    policy_net : torch.nn.Module\n",
    "        Policy network to pick up action.\n",
    "    value_net : torch.nn.Module\n",
    "        Value network used to get values and advantages.\n",
    "    discriminator_net : torch.nn.Module\n",
    "        Discriminator network used to get values and advantages\n",
    "    gamma : float\n",
    "        A discount value.\n",
    "    gae_lambda : float\n",
    "        A parameter controlling bias and variance in GAE. (See above)\n",
    "    num_samples : int\n",
    "        Number of samples to pick up.\n",
    "    batch_size : int\n",
    "        Batch size used to pick up samples.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    states : torch.tensor((num_samples), dtype=int)\n",
    "        Collected states.\n",
    "    actions : torch.tensor((num_samples), dtype=int)\n",
    "        Collected actions.\n",
    "    action_logits : torch.tensor((num_samples, ACTION_SIZE), dtype=float)\n",
    "        Logits used to pick up actions.\n",
    "    advantages : torch.tensor((num_samples), dtype=float)\n",
    "        Advantages which is used to optimize policy.\n",
    "        This advantage is obtained by GAE (generalized advantage estimation).\n",
    "        This tensor has graph to be optimized (i.e, can be used for optimization.)\n",
    "    discount : torch.tensor((num_samples), dtype=float)\n",
    "        Discount factor gamma^t.\n",
    "        Later this coefficient is used to get gamma-discounted causal entropy.\n",
    "    average_reward : torch.tensor(float)\n",
    "        The average of episode's reward in all executed episodes.\n",
    "        This reward is not used in GAIL algorithm,\n",
    "        but it's used for the evaluation of training in this example.\n",
    "    \"\"\"\n",
    "\n",
    "    ##########\n",
    "    # Operations are processed as a batch.\n",
    "    # All working tensor has dimension: (step_count, batch_size, ...)\n",
    "    ##########\n",
    "\n",
    "    # initialize results\n",
    "    states = torch.empty((0), dtype=int).to(device)\n",
    "    actions = torch.empty((0), dtype=int).to(device)\n",
    "    action_logits = torch.empty((0, ACTION_SIZE), dtype=float).to(device)\n",
    "    advantages = torch.empty((0), dtype=float).to(device)\n",
    "    discount = torch.empty((0), dtype=float).to(device)\n",
    "\n",
    "    # note : reward is not used in GAIL, but it's used to evaluate how well it's learned...\n",
    "    episode_rewards = torch.empty((0)).to(device)\n",
    "\n",
    "    s = torch.empty((0)).to(device)\n",
    "    while len(states) < num_samples:\n",
    "        #\n",
    "        # initialize episode\n",
    "        #\n",
    "        if s.nelement() == 0:\n",
    "            reward_total = torch.zeros(batch_size).to(device)\n",
    "            s = env.reset(batch_size)\n",
    "            states_ep = torch.empty((0, batch_size), dtype=int).to(device)\n",
    "            actions_ep = torch.empty((0, batch_size), dtype=int).to(device)\n",
    "            action_logits_ep = torch.empty((0, batch_size, ACTION_SIZE), dtype=float).to(device)\n",
    "\n",
    "        #\n",
    "        # step episode\n",
    "        #\n",
    "\n",
    "        # get state\n",
    "        states_ep = torch.cat((states_ep, s.unsqueeze(dim=0)), dim=0)\n",
    "        # get action with policy pi\n",
    "        s_onehot = F.one_hot(s, num_classes=STATE_SIZE)\n",
    "        logits = policy_net(s_onehot.float()).detach()\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        a = torch.multinomial(probs, num_samples=1).squeeze(dim=-1)\n",
    "        actions_ep = torch.cat((actions_ep, a.unsqueeze(dim=0)), dim=0)\n",
    "        action_logits_ep = torch.cat((action_logits_ep, logits.unsqueeze(dim=0)), dim=0)\n",
    "        # step to the next state\n",
    "        s, r, done = env.step(a, s)\n",
    "        # (note : reward is only used to evaluate)\n",
    "        reward_total += r\n",
    "\n",
    "        #\n",
    "        # finalize episode\n",
    "        #\n",
    "\n",
    "        # pick up indices to be done (not truncated) and add to reward's record\n",
    "        done_indices = done.nonzero().squeeze(dim=-1)\n",
    "        if env.step_count==MAX_TIMESTEP:\n",
    "            episode_rewards = torch.cat((episode_rewards, reward_total))\n",
    "        elif len(done_indices) > 0:\n",
    "            episode_rewards = torch.cat((episode_rewards, reward_total[done_indices]))\n",
    "        # pick up indices to be finalized (terminated or truncated)\n",
    "        trunc = torch.tensor((env.step_count==MAX_TIMESTEP) or (env.step_count >= num_samples - len(states))).to(device)\n",
    "        fin = torch.logical_or(done, trunc)\n",
    "        fin_indices = fin.nonzero().squeeze(dim=-1)\n",
    "        if len(fin_indices) > 0:\n",
    "            fin_len = len(fin_indices)\n",
    "            # pick up results to be finalized\n",
    "            states_fin = states_ep[:,fin_indices]\n",
    "            actions_fin = actions_ep[:,fin_indices]\n",
    "            action_logits_fin = action_logits_ep[:,fin_indices,:]\n",
    "            # get log(D(s,a))\n",
    "            states_onehot_fin = F.one_hot(states_fin, num_classes=STATE_SIZE).float()\n",
    "            actions_onehot_fin = F.one_hot(actions_fin, num_classes=ACTION_SIZE).float()\n",
    "            state_action_fin = torch.cat((states_onehot_fin, actions_onehot_fin), dim=-1)\n",
    "            d_log_fin = torch.log(discriminator_net(state_action_fin).detach().squeeze(dim=-1)) # detach() - gradient update in discriminator is not required\n",
    "            # get values and value loss (see above for TD)\n",
    "            values_current_fin = value_net(states_onehot_fin).squeeze(dim=-1)\n",
    "            # when it's truncated, set next value in last element. 0 otherwise.\n",
    "            if trunc:\n",
    "                state_last = s[fin_indices]\n",
    "                state_last_onehot = F.one_hot(state_last, num_classes=STATE_SIZE).float()\n",
    "                value_last = value_net(state_last_onehot).squeeze(dim=-1)\n",
    "                value_last = value_last.masked_fill(mask=done[fin_indices], value=0.0)\n",
    "            else:\n",
    "                value_last = torch.zeros(fin_len).to(device)\n",
    "            values_next_fin = torch.cat((values_current_fin[1:,:], value_last.unsqueeze(dim=0)), dim=0)\n",
    "            # get delta\n",
    "            delta_fin = d_log_fin + values_next_fin * gamma - values_current_fin\n",
    "            # get advantages (see above for GAE)\n",
    "            gae_params = torch.tensor([(gamma * gae_lambda)**i for i in range(env.step_count)]).to(device)\n",
    "            gae_params = gae_params.unsqueeze(dim=-1).expand(-1, fin_len)\n",
    "            advs_fin = [torch.sum(gae_params[:env.step_count - i,:] * delta_fin[i:,:], dim=0) for i in range(env.step_count)]\n",
    "            advs_fin = torch.stack(advs_fin)\n",
    "            # get gamma-discount\n",
    "            discount_fin = torch.tensor([gamma**i for i in range(env.step_count)]).to(device)\n",
    "            discount_fin = discount_fin.unsqueeze(dim=-1).expand(-1, fin_len)\n",
    "            # add to results\n",
    "            states = torch.cat((states, states_fin.transpose(0,1).flatten()))\n",
    "            actions = torch.cat((actions, actions_fin.transpose(0,1).flatten()))\n",
    "            action_logits = torch.cat((action_logits, action_logits_fin.transpose(0,1).flatten(start_dim=0,end_dim=1)))\n",
    "            advantages = torch.cat((advantages, advs_fin.transpose(0,1).flatten()))\n",
    "            discount = torch.cat((discount, discount_fin.transpose(0,1).flatten()))\n",
    "            # remove finalized items in batch\n",
    "            work_indices = (fin==False).nonzero().squeeze(dim=-1)\n",
    "            states_ep = states_ep[:,work_indices]\n",
    "            actions_ep = actions_ep[:,work_indices]\n",
    "            action_logits_ep = action_logits_ep[:,work_indices,:]\n",
    "            s = s[work_indices]\n",
    "            reward_total = reward_total[work_indices]\n",
    "\n",
    "    # truncate results\n",
    "    states = states[:num_samples]\n",
    "    actions = actions[:num_samples]\n",
    "    action_logits = action_logits[:num_samples,:]\n",
    "    advantages = advantages[:num_samples]\n",
    "    discount = discount[:num_samples]\n",
    "    # shuffle results\n",
    "    rnd_indices = torch.randperm(num_samples)\n",
    "    states = states[rnd_indices]\n",
    "    actions = actions[rnd_indices]\n",
    "    action_logits = action_logits[rnd_indices,:]\n",
    "    advantages = advantages[rnd_indices]\n",
    "    discount = discount[rnd_indices]\n",
    "\n",
    "    return states, actions, action_logits, advantages, discount, torch.mean(episode_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb827d02-242f-41a5-b6ef-bc68e636abf9",
   "metadata": {},
   "source": [
    "### 4. Create a function to get loss for updating $D_w$ (discriminator network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b1fe5-8d41-4c13-a7cc-5940bac98082",
   "metadata": {},
   "source": [
    "Now I build a function to get loss for updating $D_w$ by maximizing $\\mathbb{E}_{\\pi} [\\log(D(s, a))] + \\mathbb{E}_{\\pi_E} [\\log(1 - D(s, a))]$.<br>\n",
    "Maximizing above equation is equivalent to minimizing the following equation. Each term can then be obtained by binary cross entropy (BCE) loss.\n",
    "\n",
    "$\\displaystyle \\verb|discriminator_loss| \\coloneqq (-\\mathbb{E}_{\\pi} [\\log(D(s, a))]) + (-\\mathbb{E}_{\\pi_E} [\\log(1 - D(s, a))])$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4609b19-d169-4d0e-aec8-447b652e6260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator_loss(discriminator_net, exp_states, exp_actions, pi_states, pi_actions):\n",
    "    \"\"\"\n",
    "    Collect samples with policy pi.\n",
    "    To speed up training, this function runs as batch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    discriminator_net : torch.nn.Module\n",
    "        Discriminator network to be updated\n",
    "    exp_states : torch.tensor((num_samples), dtype=int)\n",
    "        States visited by expert policy.\n",
    "    exp_actions : torch.tensor((num_samples), dtype=int)\n",
    "        Corresponding actions to be taken by expert.\n",
    "    pi_states : torch.tensor((num_samples), dtype=int)\n",
    "        States visited by policy pi.\n",
    "    pi_actions : torch.tensor((num_samples), dtype=int)\n",
    "        Corresponding actions to be taken by policy pi.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    Mean of discriminator loss\n",
    "    \"\"\"\n",
    "\n",
    "    # get D(s,a)\n",
    "    states_onehot_pi = F.one_hot(pi_states, num_classes=STATE_SIZE).float()\n",
    "    actions_onehot_pi = F.one_hot(pi_actions, num_classes=ACTION_SIZE).float()\n",
    "    state_action_pi = torch.cat((states_onehot_pi, actions_onehot_pi), dim=-1)\n",
    "    d_pi = discriminator_net(state_action_pi).squeeze(dim=-1)\n",
    "\n",
    "    states_onehot_exp = F.one_hot(exp_states, num_classes=STATE_SIZE).float()\n",
    "    actions_onehot_exp = F.one_hot(exp_actions, num_classes=ACTION_SIZE).float()\n",
    "    state_action_exp = torch.cat((states_onehot_exp, actions_onehot_exp), dim=-1)\n",
    "    d_exp = discriminator_net(state_action_exp).squeeze(dim=-1)\n",
    "\n",
    "    # get mean of binary cross entropy (BCE) loss\n",
    "    mean_loss_pi = F.binary_cross_entropy(d_pi, torch.ones_like(d_pi).to(device))\n",
    "    mean_loss_exp = F.binary_cross_entropy(d_exp, torch.zeros_like(d_exp).to(device))\n",
    "\n",
    "    return mean_loss_pi + mean_loss_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005a908c-29d3-48e7-b054-67c6b69af572",
   "metadata": {},
   "source": [
    "### 5. Create a function to get loss for updating $\\pi_{\\theta}$ (policy network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fb05a6-1311-4d38-940f-9c47fa3bab56",
   "metadata": {},
   "source": [
    "Now we create a function to get loss for updating $\\theta$ to minimize $\\mathbb{E}_{\\pi} [\\log(D(s, a))] - \\lambda H(\\pi) $ (using the updated discriminator $D(s,a)$) with reinforcement learning algorithms.\n",
    "\n",
    "In [original paper](https://arxiv.org/pdf/1606.03476), TRPO (trust region policy optimization) is used to update policy parameters $\\theta$, but here **I instead use PPO** with entropy regularizer (see [here](https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/04-ppo.ipynb)) to update $\\theta$, because PPO is easier to implement.<br>\n",
    "I note that both TRPO and PPO are motivated by the same objective to avoid stepping so far (by using KL-divergence) in optimizing policy.\n",
    "\n",
    "> Note : In practical TRPO (to solve problems analytically), it needs approximation by Taylor expansion and Lagrangians.\n",
    "\n",
    "Now let's briefly summarize PPO algorithm.\n",
    "\n",
    "As I have discussed in [here](https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/04-ppo.ipynb), PPO algorithm optimizes $\\theta$ to minimize advantage loss, KL (KL-divergence) loss, value loss, and also entropy loss $- \\lambda H(\\pi)$. (Here $\\lambda$ is a weight's coefficient for entropy loss.)\n",
    "\n",
    "As I have discussed in [here](https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/04-ppo.ipynb), the advantage loss is obtained by the following equation (see below note) :\n",
    "\n",
    "$\\displaystyle \\mathbb{E}_{\\pi} \\left[ \\frac{P(a | \\pi_\\theta (s))}{P(a | \\pi_{\\theta_{old}} (s))} A \\right]$\n",
    "\n",
    "where $A$ is the advantage obtained by the value $\\log(D(s, a))$.\n",
    "\n",
    "> Note : In regular RL, $\\mathbb{E}(R)$ (where $R$ is reward) is maximized in optimization, but here $\\mathbb{E}(\\log(D(s,a)))$ should be minimized, because $\\log(D(s,a))$ is a loss score (which value is always negative) and $\\log(D(s,a))$ is getting lower (i.e, $D(s,a)$ is closer to 0) when it’s optimized.<br>\n",
    "> Then advantages in PPO should also be minimized (not maximized) in policy optimization in GAIL algorithm.\n",
    "\n",
    "In GAIL, the entropy $ H $ is $\\gamma$-discounted causal entropy as follows. (See [Maximum Causal Entropy IRL](https://arxiv.org/pdf/2203.11409).) :\n",
    "\n",
    "$\\displaystyle H(\\pi) \\coloneqq \\mathbb{E}_{\\pi} \\left[ -\\sum_t \\gamma^t \\log \\pi_t(a_t|s_t) \\right] $\n",
    "\n",
    "For value loss, I'll simply use the mean square loss (MSE) of advantages in this example. (See above description for GAE with $\\lambda_{\\verb|GAE|}=1.0$.)<br>\n",
    "Unlike advantages itself, value loss is always positive.\n",
    "\n",
    "> Note : Log probability is equivalent to the negative value of cross-entropy error in categorical distribution. I have then used ```-torch.nn.functional.cross_entropy()``` to get log probability.\n",
    "\n",
    "> Note : In PPO, advantage can also be normalized as $(A-\\mu) / \\sigma$, where $\\mu$ is a mean and $\\sigma$ is a standard deviation of advantages.<br>\n",
    "> In this training, I haven't used normalized advantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "216ac564-bbed-4277-ae45-08de2b3cedd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy_loss(policy_net, states, actions, logits, advantages, discount):\n",
    "    logits_old = logits\n",
    "\n",
    "    # get logits to be used for optimization\n",
    "    s_onehot = F.one_hot(states, num_classes=STATE_SIZE)\n",
    "    logits_new = policy_net(s_onehot.float())\n",
    "\n",
    "    # get advantage loss (see above)\n",
    "    logprb_old = -F.cross_entropy(logits_old, actions, reduction=\"none\") # get log probability (see above note)\n",
    "    logprb_new = -F.cross_entropy(logits_new, actions, reduction=\"none\") # get log probability (see above note)\n",
    "    prb_ratio = torch.exp(logprb_new - logprb_old) # P(a|pi_new(s)) / P(a|pi_old(s))\n",
    "    advantage_loss = prb_ratio * advantages\n",
    "\n",
    "    # get value loss (see above)\n",
    "    value_loss = torch.mean(advantages**2)\n",
    "\n",
    "    # get KL loss\n",
    "    # (see https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/04-ppo.ipynb)\n",
    "    l_old = logits_old - torch.amax(logits_old, dim=1, keepdim=True) # reduce quantity\n",
    "    l_new = logits_new - torch.amax(logits_new, dim=1, keepdim=True) # reduce quantity\n",
    "    e_old = torch.exp(l_old)\n",
    "    e_new = torch.exp(l_new)\n",
    "    e_sum_old = torch.sum(e_old, dim=1, keepdim=True)\n",
    "    e_sum_new = torch.sum(e_new, dim=1, keepdim=True)\n",
    "    p_old = e_old / e_sum_old\n",
    "    kl_loss = torch.sum(\n",
    "        p_old * (l_old - torch.log(e_sum_old) - l_new + torch.log(e_sum_new)),\n",
    "        dim=1,\n",
    "        keepdim=True)\n",
    "\n",
    "    # get gamma-discounted causal entropy loss (see above)\n",
    "    entropy_loss = -discount * logprb_new\n",
    "\n",
    "    return advantage_loss, value_loss, kl_loss, entropy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c692bb-5bed-45df-befd-a061e825cbec",
   "metadata": {},
   "source": [
    "### 6. Put it all together (Train and optimize parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9974426d-d429-448c-b7bc-94cea050fdfb",
   "metadata": {},
   "source": [
    "Now we alternately update $w$ (discriminator network) and $\\theta$ (policy network).\n",
    "\n",
    "In this game, the maximum episode's reward without losing any rewards is ```10.0```, and I have then trained the agent until the estimated reward is over ```5.0```.\n",
    "\n",
    "> Note : The reward is not used in optimization, and it's used only for evaluation. (The reward cannot be used in practices.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8809b073-1ebb-48fd-937c-bdc4d3037bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter0 - reward mean -66.4531\n",
      "iter200 - reward mean -66.7412\n",
      "iter400 - reward mean -22.8123\n",
      "iter600 - reward mean -4.40874\n",
      "iter800 - reward mean -0.3613\n",
      "iter1000 - reward mean 1.0615\n",
      "iter1200 - reward mean 1.9463\n",
      "iter1400 - reward mean 2.6536\n",
      "iter1600 - reward mean 2.8142\n",
      "iter1800 - reward mean 2.7464\n",
      "iter2000 - reward mean 2.9924\n",
      "iter2200 - reward mean 3.3597\n",
      "iter2400 - reward mean 3.7492\n",
      "iter2600 - reward mean 3.8088\n",
      "iter2800 - reward mean 4.2099\n",
      "iter3000 - reward mean 4.6081\n",
      "iter3103 - reward mean 4.8151\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vf_coeff = 0.01\n",
    "kl_coeff = 1.0\n",
    "_lambda = 0.005\n",
    "\n",
    "reward_records = []\n",
    "\n",
    "opt_d = torch.optim.AdamW(discriminator.parameters(), lr=0.001)\n",
    "opt_pi = torch.optim.AdamW(list(policy_func.parameters()) + list(value_func.parameters()), lr=0.001)\n",
    "\n",
    "for iter_num in range(10000):\n",
    "    # get expert data\n",
    "    states_ex, actions_ex = get_data_by_expert()\n",
    "\n",
    "    # get data by policy pi\n",
    "    states, actions, logits, advantages, discount, reward_mean = get_data_by_learner_policy(\n",
    "        env=env,\n",
    "        policy_net=policy_func,\n",
    "        value_net=value_func,\n",
    "        discriminator_net=discriminator,\n",
    "    )\n",
    "    reward_records.append(reward_mean.item())\n",
    "\n",
    "    # update discriminator\n",
    "    d_loss = get_discriminator_loss(\n",
    "        discriminator,\n",
    "        states_ex,\n",
    "        actions_ex,\n",
    "        states,\n",
    "        actions,\n",
    "    )\n",
    "    opt_d.zero_grad()\n",
    "    d_loss.backward()\n",
    "    opt_d.step()\n",
    "\n",
    "    # update policy\n",
    "    adv_loss, val_loss, kl_loss, ent_loss = get_policy_loss(\n",
    "        policy_func,\n",
    "        states,\n",
    "        actions,\n",
    "        logits,\n",
    "        advantages,\n",
    "        discount,\n",
    "    )\n",
    "    pi_loss = adv_loss + val_loss * vf_coeff + kl_loss * kl_coeff + ent_loss * _lambda\n",
    "    opt_pi.zero_grad()\n",
    "    pi_loss.sum().backward()\n",
    "    opt_pi.step()\n",
    "\n",
    "    # output log\n",
    "    if iter_num % 200 == 0:\n",
    "        line_end = \"\\n\"\n",
    "        print_reward = np.average(reward_records[-200:])\n",
    "    else:\n",
    "        line_end = \"\\r\"\n",
    "        print_reward = reward_records[-1]\n",
    "    print(\"iter{} - reward mean {:2.4f}\".format(iter_num, print_reward), end=line_end)\n",
    "\n",
    "    # stop if reward mean reaches to threshold\n",
    "    if np.average(reward_records[-20:]) > 5.0:\n",
    "        break\n",
    "\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034797fc-9ea1-4ba2-90a1-0f1769823067",
   "metadata": {},
   "source": [
    "I plot results of evaluated episode reward till 1000 iterations.<br>\n",
    "As you can see below, the agent is well-trained and optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb58822e-3610-4ef8-b7bc-aa1b07a6ef3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7c3b58623220>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_records = reward_records[:1000]\n",
    "plt.plot(plot_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c3684f-ae52-4602-a3d3-fb47f5708a60",
   "metadata": {},
   "source": [
    "The following is the plotting of all evaluated results in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c14b1267-334c-441f-8f9e-55164c1bf6e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7c3b5851f4f0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(reward_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ae3cf9-1a82-46f8-8632-b1746d2f9022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
